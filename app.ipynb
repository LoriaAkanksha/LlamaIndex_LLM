{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (0.10.50)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.2.7)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core==0.10.50 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.10.50)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.10)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.23)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.25)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.50->llama_index) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (0.27.0)\n",
      "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (0.0.6)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (1.35.7)\n",
      "Requirement already satisfied: pandas in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (8.4.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-core==0.10.50->llama_index) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index) (0.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.50->llama_index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.50->llama_index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.50->llama_index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.50->llama_index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.50->llama_index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.50->llama_index) (2.7.4)\n",
      "Requirement already satisfied: anyio in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from httpx->llama-index-core==0.10.50->llama_index) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from httpx->llama-index-core==0.10.50->llama_index) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from httpx->llama-index-core==0.10.50->llama_index) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from httpx->llama-index-core==0.10.50->llama_index) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from httpx->llama-index-core==0.10.50->llama_index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.50->llama_index) (0.14.0)\n",
      "Requirement already satisfied: click in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.50->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.50->llama_index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.50->llama_index) (2024.5.15)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core==0.10.50->llama_index) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core==0.10.50->llama_index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core==0.10.50->llama_index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.50->llama_index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.50->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from dataclasses-json->llama-index-core==0.10.50->llama_index) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from pandas->llama-index-core==0.10.50->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from pandas->llama-index-core==0.10.50->llama_index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from pandas->llama-index-core==0.10.50->llama_index) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.50->llama_index) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.50->llama_index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.50->llama_index) (2.18.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/akanksha/anaconda3/envs/llamaindex/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.50->llama_index) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader\n",
    "documents=SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='92607a88-a806-463f-b2b8-5e46fb4962d6', embedding=None, metadata={'page_label': '1', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c5f3c575-5946-4b98-aa33-2007debb8b55', embedding=None, metadata={'page_label': '2', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Encyclopedia of \\nArtificial Intelligence\\nJuan Ramón Rabuñal Dopico\\nUniversity of A Coruña, Spain\\nJulián Dorado de la Calle\\nUniversity of A Coruña, Spain\\nAlejandro Pazos Sierra\\nUniversity of A Coruña, Spain\\nHershey • New YorkInformatIon ScI\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5487468c-0c28-4f1b-b3ee-0ed339f200a5', embedding=None, metadata={'page_label': '3', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Director of Editorial Content: Kristin Klinger\\nManaging Development Editor: Kristin Roth\\nDevelopment Editorial Assistant: Julia Mosemann, Rebecca Beistline\\nSenior Managing Editor:  Jennifer Neidig\\nManaging Editor:  Jamie Snavely\\nAssistant Managing Editor: Carole Coulson\\nTypesetter:   Jennifer Neidig, Amanda Appicello, Cindy Consonery\\nCover Design:  Lisa Tosheff\\nPrinted at:   Yurchak Printing Inc.\\nPublished in the United States of America by \\nInformation Science Reference (an imprint of IGI Global)\\n701 E. Chocolate Avenue, Suite 200Hershey PA 17033Tel: 717-533-8845Fax:  717-533-8661E-mail: cust@igi-global.comWeb site: http://www.igi-global.com/reference\\nand in the United Kingdom by\\nInformation Science Reference (an imprint of IGI Global)3 Henrietta StreetCovent GardenLondon WC2E 8LUTel: 44 20 7240 0856Fax:  44 20 7379 0609Web site: http://www.eurospanbookstore.com\\nCopyright © 2009 by IGI Global.  All rights reserved. No part of this publication may be reproduced, stored or distributed in any form or by any means, electronic or mechanical, including photocopying, without written permission from the publisher.\\nProduct or company names used in this set are for identification purposes only. Inclusion of the names of the products or companies does not indicate \\na claim of ownership by IGI Global of the trademark or registered trademark.\\nLibrary of Congress Cataloging-in-Publication DataEncyclopedia of artificial intelligence / Juan Ramon Rabunal Dopico, Julian Dorado de la Calle, and Alejandro Pazos Sierra, editors.\\n       p. cm.\\n  Includes bibliographical references and index.  Summary: \"This book is a comprehensive and in-depth reference to the most recent developments in the field covering theoretical developments, tech -\\nniques, technologies, among others\"--Provided by publisher.  ISBN 978-1-59904-849-9 (hardcover) -- ISBN 978-1-59904-850-5 (ebook) 1.  Artificial intelligence--Encyclopedias.  I. Rabunal, Juan Ramon, 1973- II. Dorado, Julian, 1970- III. Pazos Sierra, Alejandro.   Q334.2.E63 2008  006.303--dc22                                                            2008027245\\nBritish Cataloguing in Publication Data\\nA Cataloguing in Publication record for this book is available from the British Library.\\nAll work contributed to this encyclopedia set is new, previously-unpublished material. The views expressed in this encyclopedia set are those of the \\nauthors, but not necessarily of the publisher.\\nIf a library purchased a print copy of this publication, please go to http //www.igi-global.com/agreement for information on activating the library\\'s complimentary electronic access to this publication.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='546cc489-541b-4979-b742-1986f0c4c190', embedding=None, metadata={'page_label': '4', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Editorial Advisory Board\\nJuan Ríos Carrión\\nPolytechnical University of Madrid, Spain\\nAnselmo del Moral\\nUniversity of Deusto, Spain\\nDaniel Manrique Gamo\\nPolytechnical University of Madrid, Spain\\nJuan Pazos Sierra\\nPolytechnical University of Madrid, Spain\\nJose Crespo del Arco\\nPolytechnical University of Madrid, Spain\\nNorberto Ezquerra\\nGeorgia Institute of Technology, USA\\nLluís Jofre\\nPolytechnical University of Catalunya, SpainPeter SmithUniversity of Sunderland, UK\\nPaul M. Chapman\\nUniversity of Hull, UK\\nAna Belén Porto Pazos\\nUniversity of A Coruña, Spain\\nJavier Pereira\\nUniversity of A Coruña, Spain\\nStefano Cagnoni\\nUniversità degli Studi de Parma, Italy\\nJose María Barreiro Sorrivas\\nPolytechnical University of Madrid, Spain', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='949ad2e4-f76d-439d-a729-9d12dff50b37', embedding=None, metadata={'page_label': '5', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='List of Contributors\\nAdorni, Giovanni / Università degli Studi di Genova, Italy  ..................................................................840, 848\\nAkkaladevi, Somasheker / Virginia State University, USA  .........................................................940, 945, 1330\\nAl-Ahmadi, Mohammad Saad / King Fahd University of Petroleum and Minerals (KFUPM), \\n     Saudi Arabia .............................................................................................................................................1323\\nAliaga, Ramón J. / Universidad Politécnica de Valencia, Spain .................................................................1576\\nAlías, Francesc / Universitat Ramon Llull, Spain  ..................................................................................541, 788\\nAlonso-Betanzos, Amparo / University of A Coruña, Spain ..........................................................................632\\nAlonso Hernández, Jesús Bernardino / University of Las Palmas de Gran Canaria, Spain ...........1266, 1439\\nAlonso-Weber, Juan Manuel / Universidad Carlos III de Madrid, Spain ....................................................554\\nAlsina Pagès, Rosa Maria / Universitat Ramon Llull, Spain  .........................................................................719\\nAlvarellos González, Alberto / University of A Coruña, Spain .....................................................................167\\nAmarger, Véronique / University of Paris, France  .......................................................................................131\\nAmari, Shun-ichi / Brain Science Institute, Japan  .........................................................................................318\\nAmbrósio, Paulo Eduardo / Santa Cruz State University, Brazil..................................................................157\\nAnagnostou, Miltiades / National Technical University of Athens, Greece .......................................1429, 1524\\nAndrade, Javier / University of A Coruña, Spain ..........................................................................................975\\nAndrade, José Manuel / University of A Coruña, Spain ...............................................................................581\\nAng, Kai Keng / Institute for Infocomm Research, Singapore ..................................................................... 1396\\nAng Jr., Marcelo H. / National University of Singapore, Singapore  ................................................. 1072, 1080\\nAngulo, Cecilio / Technical University of Catalonia, Spain  ...............................................................1095, 1518\\nAnselma, Luca / Università di Torino, Italy  ...................................................................................................396\\nArcay, Bernardino / University of A Coruña, Spain ......................................................................................710\\nAres, Juan / University of A Coruña, Spain ....................................................................................................982\\nArmstrong, Alice J. / The George Washington University, USA  ...................................................................... 65\\nArquero, Águeda / Technical University of Madrid, Spain............................................................................781Aunet, Snorre / University of Oslo, Norway & Centers for Neural Inspired Nano Architectures,      Norway ............................................................................................................................................1474, 1555\\nAzzini, Antonia / University of Milan, Italy  ................................................................................................... 575\\nBadidi, Elarbi / United Arab Emirates University, UAE ..................................................................................31\\nBagchi, Kallol / University of Texas at El Paso, USA.......................................................................................51\\nBajo, Javier / Universidad Pontificia de Salamanca, Spain  .........................................................................1327\\nBarajas, Sandra E. / Instituto Nacional de Astrofísica Óptica y Electrónica, Mexico  ..................................867\\nBarron, Lucia / Instituto Tecnologico de Culiacan, Mexico  ..........................................................................860\\nBarták, Roman / Charles University in Prague, Czech Republic  ..................................................................404\\nBarton, Alan J. / National Research Council Canada, Canada ......................................................... 1205, 1589\\nBecerra, J. A. / University of A Coruña, Spain ...............................................................................................603\\nBedia, Manuel G. / University of Zaraogoza, Spain ......................................................................................256', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1b8940a-43b9-4213-a235-5b6b0877d434', embedding=None, metadata={'page_label': '6', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Beiu, Valeriu / United Arab Emirates University, UAE ..................................................................................471\\nBel Enguix, Gemma / Rovira i Virgili University, Spain .............................................................................. 1173\\nBelanche Muñoz, Lluís A.  / Universitat Politècnica de Catalunya, Spain  ...............................639, 1004, 1012\\nBerge, Hans Kristian Otnes / University of Oslo, Norway .........................................................................1485\\nBernier, Joel / SAGEM REOSC, France  .........................................................................................................131\\nBerrones, Arturo / Universidad Autónoma de Nuevo León, Mexico  ...........................................................1462\\nBershtein, Leonid S. / Taganrog Technological Institute of Southern Federal University, Russia................704\\nBessalah, Hamid / Center de Développement des Technologies Avancées (CDTA), Algérie  .........................831\\nBeynon, Malcolm J. / Cardiff University, UK ........................................................................................443, 696\\nBhatnagar, Vasudha / University of Delhi, India  .....................................................................................76, 172\\nBlanco, Ángela / Universidad Pontificia de Salamanca, Spain  ...................................................................... 561\\nBlanco, Francisco J. / Juan Canalejo Hospital, Spain  .................................................................................1583\\nBlasco, X. / Polytechnic University of Valencia, Spain  .................................................................................1296\\nBoonthum, Chutima / Hampton University, USA  ........................................................................................1253\\nBouridene, Ahmed. / Queens University of Belfast, Ireland  ..........................................................................831\\nBoyer-Xambeu, Marie-Thérèse / Université de Paris VII –  LED, France  ..................................................996\\nBozhenyuk, Alexander V . / Taganrog Technological Institute of Southern Federal University, Russia  .......704\\nBrest, Janez / University of Maribor, Slovenia  ...............................................................................................488\\nBueno, Raúl Vicen / University of Alcalá, Spain  ...................................................................................933, 956\\nBueno García, Gloria / University of Castilla – La Mancha, Spain  ...................................................... 367, 547\\nBuruncuk, Kadri / Near East University, Turkey ........................................................................................1596\\nCadenas, José M. / Universidad de Murcia, Spain  ........................................................................................480\\nCagnoni, Stefano / Università degli Studi di Parma, Italy  ........................................................... 840, 848, 1303\\nÇakıcı, Ruket / ICCS School of Informatics, University of Edinburgh, UK  ..................................................449\\nCanto, Rosalba Cuapa / Benemérita Universidad Autónoma de Puebla, Mexico  ............................1370, 1426\\nCarballo, Rodrigo / University of Santiago de Conpatela, Spain  ................................................................1603\\nCarbonero, M. / INSA – ETEA, Spain  .......................................................................................................... 1136\\nCardot, Hubert / University François-Rabelais of Tours, France .................................................................520\\nCastillo, Luis F. / National University, Colombia .......................................................................................... 256\\nCastro Ponte, Alberte / University of Santiago de Compostela, Spain..................................................144, 759\\nCastro, Alfonso / University of A Coruña, Spain ............................................................................................710\\nCastro-Bleda, María José / Universidad Politécnica de Valencia, Spain .....................................................231\\nCepero, M. / University of Granada, Spain ....................................................................................................910\\nChapman, Paul M. / University of Hull, UK..................................................................................................536Charrier, Christophe / University of Caen Basse-Normandie, France  .........................................................520\\nChen, Qiyang / Montclair State University, USA  .........................................................................418, 963, 1036\\nChen, Guanrong / City University of Hong Kong, Hong Kong, China  .................................................688, 734\\nChen, Sherry Y. / Brunel University, UK .......................................................................................................437\\nChikhi, Nassim / Center de Développement des Technologies Avancées (CDTA), Algérie  ...........................831\\nChiong, Raymond / Swinburne University of Technology, Sarawak Campus, Malaysia  ............................ 1562\\nChrysostomou, Kyriacos / Brunel University, UK ........................................................................................437\\nColomo, Ricardo / Universidad Carlos III de Madrid, Spain ...................................................................... 1064\\nCorchado, Juan M. / University of Salamanca, Spain  .........................................................................256, 1316\\nCoupland, Sarah / Royal Liverpool University Hospital, UK  ....................................................................... 390\\nCrespo, Jose / Universidad Politécnica de Madrid, Spain  ........................................................................... 1102\\nCruz-Corona, Carlos / Universidad de Granada, Spain ...............................................................................480\\nCuéllar, M. P. / Universidad de Granada, Spain .......................................................................................... 1152\\nCulhane, Aedín C. / Harvard School of Public Health, USA  ...........................................................................65', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d09b7861-db27-4aa4-8a14-ff12eb1da9bf', embedding=None, metadata={'page_label': '7', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Curra, Alberto / University of A Coruña, Spain ............................................................................................ 110\\nDamato, Bertil / Royal Liverpool University Hospital, UK  ...........................................................................390\\nDanciu, Daniela / University of Craiova, Romania  ......................................................................................1212\\nDanielson, Mats / Stockholm University, Sweden & Royal Institute of Technology, Sweden  .........................431\\nDas, Sanjoy / Kansas State University, USA  ....................................................................................... 1145, 1191\\nDavis, Darryl N. / University of Hull, UK  ......................................................................................................536\\nde la Mata Moya, David / University of Alcalá, Spain  ..................................................................................933\\nde la Rosa Turbides, Tomás / Universidad Carlos III de Madrid, Spain .................................................... 1024\\nDeleplace, Ghislain / Université de Paris VIII – LED, France  ......................................................................996\\nDelgado, M. / Universidad de Granada, Spain ............................................................................................. 1152\\nDelgado, Soledad / Technical University of Madrid, Spain  ............................................................................781\\nDel-Moral-Hernandez, Emilio / University of São Paulo, Brazil  .................................................................275\\nDeng, Pi-Sheng / California State University at Stanislaus, USA  ........................................................748, 1504\\nDéniz Suárez, Oscar / University of Las Palmas de Gran Canaria, Spain ....................................................367\\nDhurandher, Sanjay Kumar / University of Delhi, India  ................................................................... 589, 1530\\ndi Pierro, Francesco / University of Exeter, UK ...........................................................................................1042\\nDíaz Martín, José Fernando / University of Deusto, Spain  .......................................................................... 344\\nDíaz Pernas, F. J.  / University of Valladolid, Spain .......................................................................... 1490, 1497\\nDíez Higuera, J. F.  / University of Valladolid, Spain ........................................................................1490, 1497\\nDiuk, Carlos / Rutgers University, USA  .........................................................................................................825\\nDjebbari, Amira / National Research Council Canada, Canada .................................................................... 65\\nDorado de la Calle, Julián / University of A Coruña, Spain ...............................................................377, 1273\\nDornaika, Fadi / Institut Géographique National, France  ............................................................................ 625\\nDouglas, Angela / Liverpool Women’ s Hospital, UK ......................................................................................390\\nDuro, R. J. / University of A Coruña, Spain ...................................................................................................603\\nEdelkamp, Stefan / University of Dortmund, Germany .......................................................................501, 1549\\nEin-Dor, Phillip / Tel-Aviv University, Israel  .......................................................................................... 327, 334\\nEkenberg, Love / Stockholm University, Sweden & Royal Institute of Technology, Sweden  .........................431\\nEleuteri, Antonio / Royal Liverpool University Hospital, UK  .......................................................................390\\nEncheva, Sylvia / Haugesund University College, Norway  .........................................................................1610\\nErdogmus, Deniz / Northeastern University, USA  .........................................................................................902\\nEsmahi, Larbi / Athabasca University, Canada ...............................................................................................31\\nEspaña-Boquera, Salvador / Universidad Politécnica de Valencia, Spain ...................................................231\\nEzquerra, Norberto / Georgia Institute of Technology, USA  .......................................................................1290\\nFan, Liwei / National University of Singapore, Singapore  .............................................................................879\\nFarah, Ahcene / Ajman University, UAE ........................................................................................................831\\nFaundez-Zanuy, Marcos / Escola Universitària Politècnica de Mataró, Spain  ........................................... 262\\nFernández, J. Álvaro / University of Extremadura, Badajoz, Spain ........................................................45, 218\\nFernandez-Blanco, Enrique / University of A Coruña, Spain .....................................................377, 744, 1583\\nFerrer, Miguel A. / University of Las Palmas de Gran Canaria, Spain ............................................... 270, 1232\\nFigueiredo, Karla / UERJ, Brazil ........................................................................................................... 808, 817\\nFlauzino, Rogerio A. / University of São Paulo, Brazil  ............................................................................... 1121\\nFlores, Dionicio Zacarías / Benemérita Universidad Autónoma de Puebla, Mexico  ........................ 1370, 1426\\nFlores, Fernando Zacarías / Benemérita Universidad Autónoma de Puebla, México  ......................1370, 1426\\nFlores-Badillo, Marina / CINVESTAV Unidad Guadalajara, Mexico  ......................................................... 1615\\nFlórez-Revuelta, Francisco / University of Alicante, Spain  ........................................................................ 1363\\nFontenla-Romero, Oscar / University of A Coruña, Spain ............................................................................667\\nFormiga, Lluís / Universitat Ramon Llull, Spain  ...........................................................................................788\\nFornarelli, Girolamo / Politecnico di Bari, Italy  ................................................................................... 206, 211', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60d6788f-4b3a-4bcf-b9d0-24fddd073b46', embedding=None, metadata={'page_label': '8', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Fuster-Garcia, E. / Polytechnic University of Valencia, Spain  ....................................................................1296\\nGadea, Rafael / Universidad Politécnica de Valencia, Spain ....................................................................... 1576\\nGaranina, Natalia / Russian Academy of Science, Institute of Informatics Systems, Russia  .......................1089\\nGarcía, Ángel / Universidad Carlos III de Madrid, Spain ...........................................................................1064\\nGarcía, Rafael / University of A Coruña, Spain .............................................................................................982\\nGarcía González, Antonio / University of Alcalá, Spain  ...............................................................................956\\nGarcía-Chamizo, Juan Manuel / University of Alicante, Spain  .................................................................1363\\nGarcía-Córdova, Francisco / Polytechnic University of Cartagena (UPCT), Spain  .................................. 1197\\nGarcia-Raffi, L. M. / Polytechnic University of Valencia, Spain  ................................................................. 1296\\nGarcía-Rodríguez, José / University of Alicante, Spain  ..............................................................................1363\\nGarrido, Mª Carmen / Universidad de Murcia, Spain  ..................................................................................480\\nGarro, Alfredo / University of Calabria, Italy  .............................................................................................1018\\nGaubert, Patrice / Université de Paris 12 – ERUDITE, France  ...................................................................996\\nGavrilova, M. L. / University of Calgary, Canada .............................................................................................9\\nGeem, Zong Woo / Johns Hopkins University, USA  ...................................................................................... 803\\nGelbard, Roy / Bar-Ilan University, Israel .....................................................................................................796\\nGeorge, E. Olusegun / University of Memphis, USA  ............................................................................. 304, 312\\nGerek, Ömer Nezih / Anadolu University Eskisehir, Turkey ........................................................................ 1433\\nGestal, Marcos / University of A Coruña, Spain ....................................................................................581, 647\\nGiaquinto, Antonio / Politecnico di Bari, Italy  ...................................................................................... 206, 211\\nGil Pita, Roberto / University of Alcalá, Spain  ...................................................................................... 933, 956\\nGillard, Lucien / CNRS – LED, France .........................................................................................................996\\nGiret, Jean-Francois / CEREQ, France  .......................................................................................................1029\\nGómez, Gabriel / University of Zurich, Switzerland  ......................................................................................464\\nGómez, Juan M. / Universidad Carlos III de Madrid, Spain ....................................................................... 1064\\nGómez-Carracedo, Mari Paz / University of A Coruña, Spain .....................................................................647\\nGonzález-Fonteboa, Belén / University of A Coruña, Spain .........................................................................526\\nGonzález, Evelio J. / University of La Laguna, Spain .................................................................................... 917\\nGonzález, Roberto / University of Castilla – La Mancha, Spain  ...................................................................547\\nGonzalez-Abril, Luis / Technical University of Catalonia, Spain  ...............................................................1518\\nGonzález Bedia-Fonteboa, Manuel / University of Zaragoza, Spain ...........................................................256\\nGonzález-Castolo, Juan Carlos / CINVESTAV Unidad Guadalajara, Mexico  ............................................. 677\\nGonzález de la Rosa, Juan J.  / Universities of Cádiz-Córdoba, Spain ......................................................1226\\nGonzález Ortega, D. / University of Valladolid, Spain ...................................................................... 1490, 1497\\nGonzalo, Consuelo / Technical University of Madrid, Spain  .........................................................................781\\nGraesser, Art / The University of Memphis, USA  ......................................................................................... 1179\\nGrošek, Otokar / Slovak University of Technology, Slovakia ................................................................179, 186\\nGuerin-Dugue, Anne / GIPSA-lab, France  ..................................................................................................1244\\nGuerrero-González, Antonio / Polytechnic University of Cartagena (UPCT), Spain  ................................ 1197\\nGuijarro-Berdiñas, Bertha / University of A Coruña, Spain ........................................................................667\\nGuillen, A. / University of Granada, Spain .....................................................................................................910\\nGupta, Anamika / University of Delhi, India  ...................................................................................................76\\nGutiérrez, P.A. / University of Córdoba, Spain ............................................................................................ 1136\\nGutiérrez Sánchez, Germán / Universidad Carlos III de Madrid, Spain ..................................................... 554\\nHalang, Wolfgang A. / Fernuniversitaet in Hagen, Germany  ......................................................................1049\\nHammer, Barbara / Technical University of Clausthal, Germany  ..............................................................1337\\nHee, Lee Gim / DSO National Laboratories, Singapore .................................................................... 1072, 1080\\nHerrador, Manuel F. / University of A Coruña, Spain ................................................................................... 118', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6db6ec0-ad9b-4eb6-af2b-84e06bf3d098', embedding=None, metadata={'page_label': '9', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Herrera, Carlos / Intelligent Systems Research Centre, University of Ulster, North Ireland ............................1376\\nHerrera, L. J. / University of Granada, Spain................................................................................................910\\nHerrero, J. M. / Polytechnic University of Valencia, Spain  .........................................................................1296\\nHervás, C. / University of Córdoba, Spain ................................................................................................... 1136\\nHocaoğlu, Fatih Onur / Anadolu University Eskisehir, Turkey ...................................................................1433\\nHong, Wei-Chiang / Oriental Institute of Technology, Taiwan  ...................................................................... 410\\nHopgood, Adrian A. / De Montfort University, UK .......................................................................................989\\nHo-Phuoc, Tien / GIPSA-lab, France  ........................................................................................................... 1244\\nHuang, Xiaoyu / University of Shanghai for Science & Technology, China ....................................................51\\nHuber, Franz / California Institute of Technology, USA  ..............................................................................1351\\nIbáñez, Óscar / University of A Coruña, Spain ......................................................................................383, 759\\nIbrahim, Walid / United Arab Emirates University, UAE .............................................................................. 471\\nIftekharuddin, Khan M. / University of Memphis, USA  ....................................................................... 304, 312\\nIngber, Lester / Lester Ingber Research, USA  .................................................................................................. 58\\nIglesias, Gergorio / University of Santiago de Compostela, Spain  ..............................................................1603\\nIonescu, Laurenţiu / University of Pitesti, Romania  ......................................................................................609\\nIp, Horace H. S. / City University of Hong Kong, Hong Kong  ..........................................................................1\\nIriondo, Ignasi / Universitat Ramon Llull, Spain  ...........................................................................................541\\nIslam, Atiq / University of Memphis, USA  ..............................................................................................304, 312\\nIzeboudjen, Nouma / Center de Développement des Technologies Avancées (CDTA), Algérie  ....................831\\nJabbar, Shahid / University of Dortmund, Germany .....................................................................................501\\nJabr, Samir / Near East University, Turkey ..................................................................................................1596\\nJanković-Romano, Mario / University of Belgrade, Serbia  .......................................................................... 950\\nJarabo Amores, María Pilar / University of Alcalá, Spain  ........................................................................... 933\\nJaspe, Alberto / University of A Coruña, Spain ..............................................................................................873\\nJiang, Jun / City University of Hong Kong, Hong Kong  ....................................................................................1\\nJiménez Celorrio, Sergio / Universidad Carlos III de Madrid, Spain .........................................................1024\\nJiménez López, M. Dolores / Rovira i Virgili University, Spain .................................................................. 1173\\nJoo, Young Hoon / Kunsan National University, Korea .........................................................................688, 734\\nKaburlasos, Vassilis G. / Technological Educational Institution of Kavala, Greece ...................................1238\\nKačič, Zdravko / University of Maribor, Slovenia  .......................................................................................1467\\nKärnä, Tuomas / Helsinki University of Technology, Finland .......................................................................661\\nKatangur, Ajay K. / Texas A&M University – Corpus Christi, USA  ...........................................................1330\\nKhashman, Adnan / Near East University, Turkey ......................................................................................1596\\nKhu, Soon-Thiam / University of Exeter, UK ..............................................................................................1042\\nKleinschmidt, João H. / State University of Campinas, Brazil  ......................................................................755\\nKlimanek, David / Czech Technical University in Prague, Czech Republic  .................................................. 567\\nKochhar, Sarabjeet / University of Delhi, India  ............................................................................................172\\nKovács, Szilveszter / University of Miskolc, Hungary  ...................................................................................728\\nKovács, László / University of Miskolc, Hungary  ................................................................................ 654, 1130\\nKrčadinac, Uroš / University of Belgrade, Serbia  .........................................................................................950\\nKroc, Jiří / Section Computational Science, The University of Amsterdam, The Netherlands.......................353\\nKumar, Naveen / University of Delhi, India  .....................................................................................................76\\nKurban, Mehmet / Anadolu University Eskisehir, Turkey ........................................................................... 1433\\nLama, Manuel / University of Santiago de Compostela, Spain  ........................................................... 138, 1278\\nLaw, Ngai-Fong / The Hong Kong Polytechnic University, Hong Kong ........................................................289\\nLazarova-Molnar, Sanja / United Arab Emirates University, UAE .............................................................. 471\\nLebrun, Gilles / University of Caen Basse-Normandie, France  ....................................................................520\\nLedezma Espino, Agapito / Universidad Carlos III de Madrid, Spain ......................................................... 554', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfc10aa6-de34-4c76-a2a9-0915f7d0bba0', embedding=None, metadata={'page_label': '10', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Lee, Man Wai / Brunel University, UK...........................................................................................................437\\nLendasse, Amaury / Helsinki University of Technology, Finland ..................................................................661\\nLeung, C. W. / The Hong Kong Polytechnic University, Hong Kong ........................................................... 1568\\nLevinstein, Irwin B. / Old Dominion University, USA  .................................................................................1253\\nLevy, Simon D. / Washington and Lee University, USA  .................................................................................514\\nLezoray, Olivier / University of Caen Basse-Normandie, France  .................................................................520\\nLiang, Faming / Texas A&M University, USA  ..............................................................................................1482\\nLiew, Alan Wee-Chung / Griffith University, Australia ................................................................................. 289\\nLisboa, Paulo J.G. / Liverpool John Moores University, UK ...........................................................................71\\nLittman, Michael / Rutgers University, USA..................................................................................................825\\nLiu, Xiaohui / Brunel University, UK .............................................................................................................437\\nLopes, Heitor Silvério / Federal University of Technology, Brazil ................................................................596\\nLópez, M. Gloria / University of A Coruña, Spain ......................................................................................... 110\\nLópez-Mellado, Ernesto / CINVESTAV Unidad Guadalajara, Mexico  ...............................................677, 1615\\nLópez-Rodríguez, Domingo / University of Málaga, Spain  ........................................................................ 1112\\nLosada Rodriguez, Miguel Ángel / University of Granada, Spain ...............................................................144\\nLoula, Angelo / State University of Feira de Santana, Brazil & State University of Campinas (UNICAMP), \\n     Brazil .........................................................................................................................................................1543\\nLoureiro, Javier Pereira / University of A Coruña, Spain .................................................................1283, 1290\\nLukomski, Robert / Wroclaw University of Technology, Poland .................................................................1356\\nLungarella, Max / University of Zurich, Switzerland  ....................................................................................464\\nLuo, Xin / The University of New Mexico, USA  ...........................................................................940, 945, 1330\\nMadani, Kurosh / University of Paris, France  ..............................................................................................131\\nMadureira, Ana Marie / Polytechnic Institute of Porto, Portugal  ................................................................853\\nMagliano, Joseph P. / Northern Illinois University, USA  .............................................................................1253\\nMagoulas, George D. / University of London, UK ....................................................................................... 1411\\nMagro, Diego / Università di Torino, Italy  ..................................................................................................... 396\\nMaitra, Anutosh / Dhirubhai Ambani Institute of Information and Communication Technology, India .......494\\nMandl, Thomas / University of Hildesheim, Germany...................................................................................151Manrique, Daniel / Inteligencia Artificial, Facultad de Informatica, UPM, Spain  .......................................767\\nMarichal, G. Nicolás / University of La Laguna, Spain ................................................................................. 917\\nMarín-García, Fulgencio / Polytechnic University of Cartagena (UPCT), Spain  ...................................... 1197\\nMartínez, Antonio / University of Castilla – La Mancha, Spain  ...................................................................547\\nMartínez, Elisa / Universitat Ramon Llull, Spain  ..........................................................................................541\\nMartínez, Estíbaliz / Technical University of Madrid, Spain  ........................................................................781\\nMartínez, Jorge D. / Universidad Politécnica de Valencia, Spain ...............................................................1576\\nMartínez, Mª Isabel / University of A Coruña, Spain .................................................................................... 118\\nMartínez-Abella, Fernando / University of A Coruña, Spain .......................................................................526\\nMartínez Carballo, Manuel / University of A Coruña, Spain .......................................................................532\\nMartínez-Estudillo, F.J. / INSA – ETEA, Spain  ........................................................................................... 1136\\nMartínez-Feijóo, Diego / University of A Coruña, Spain ............................................................................1583\\nMartínez Romero, Marcos / University of A Coruña, Spain .............................................................1283, 1290\\nMartínez-Zarzuela, M. / University of Valladolid, Spain .................................................................. 1490, 1497\\nMartín-Guerrero, José D. / University of Valencia, Spain .............................................................................. 71\\nMartín-Merino, Manuel / Universidad Pontificia de Salamanca, Spain  ...................................................... 561\\nMateo, Fernando / Universidad Politécnica de Valencia, Spain ................................................................. 1576\\nMateo Segura, Clàudia / Universitat Ramon Llull, Spain  .............................................................................719\\nMato, Virginia / University of A Coruña, Spain ............................................................................................. 110\\nMaučec, Mirjam Sepesy / University of Maribor, Slovenia  .........................................................................1467', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cc85f09-ebc7-4910-8766-f655f19b54fc', embedding=None, metadata={'page_label': '11', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Mazare, Alin / University of Pitesti, Romania  ................................................................................................609\\nMcCarthy, Philip / The University of Memphis, USA  .................................................................................. 1179\\nMcGinnity, Thomas M. / Intelligent Systems Research Centre, University of Ulster, North Ireland ............... 1376\\nMcNamara, Danielle S. / The University of Memphis, USA  ........................................................................ 1253\\nMeged, Avichai / Bar-Ilan University, Israel ..................................................................................................796\\nMéndez Salgueiro, José Ramón / University of A Coruña, Spain .................................................................532\\nMeng, Hai-Dong / Inner Mongolia University of Science and Technology, China ........................................ 297\\nMérida-Casermeiro, Enrique / University of Málaga, Spain  ..................................................................... 1112\\nMesejo, Pablo / University of A Coruña, Spain ............................................................................................1583\\nMichalewicz, Zbigniew / The University of Adelaide, Australia .....................................................................16\\nMiguélez Rico, Mónica / University of A Coruña, Spain .............................................................236, 241, 1273\\nMillis, Keith K. / The University of Memphis, USA  .....................................................................................1253\\nMisra, Sudip / Yale University, USA  .....................................................................................................589, 1530\\nMohammadian, M. / University of Canberra, Australia .....................................................................456, 1510\\nMonzó, José Mª / Universidad Politécnica de Valencia, Spain ....................................................................1576\\nMorales Moreno, Aythami / University of Las Palmas de Gran Canaria, Spain ....................................... 1259\\nMordonini, Monica / Università degli Studi di Parma, Italy  ....................................................... 840, 848, 1303\\nMoreno-Muñoz, A. / Universities of Cádiz-Córdoba, Spain ........................................................................1226\\nMuñoz, Enrique / Universidad de Murcia, Spain  ..........................................................................................480\\nMuñoz, Luis Miguel Guzmán / Benemérita Universidad Autónoma de Puebla, Mexico  .................1370, 1426\\nMussi, Luca  / Università degli Studi di Perugia, Italy  ..........................................................................840, 848\\nMutihac, Radu / University of Bucharest, Romania ......................................................................22, 223, 1056\\nNarula, Prayag / University of Delhi, India  ......................................................................................... 589, 1530\\nNeto, João José / Universidade de São Paulo, Brazil  ......................................................................................37\\nNitta, Tohru / AIST, Japan ..............................................................................................................................361\\nNóvoa, Francisco J. / University of A Coruña, Spain .................................................................................... 110\\nOja, Erkki / Helsinki University of Technology, Finland .............................................................................1343\\nOlteanu, Madalina / Université de Paris I – CES SAMOS, France  ..............................................................996\\nOrtiz-de-Lazcano-Lobato, Juan M. / University of Málaga, Spain  ........................................................... 1112\\nPacheco, Marco / PUC-Rio, Brazil.........................................................................................................808, 817\\nPanigrahi, Bijaya K. / Indian Institute of Technology, India ....................................................................... 1145\\nPapaioannou, Ioannis / National Technical University of Athens, Greece ........................................ 1418, 1524\\nPazos Montañés, Félix / University of A Coruña, Spain ................................................................................167\\nPazos Sierra, Alejandro / University of A Coruña, Spain ............................................................................1283\\nPedreira, Nieves / University of A Coruña, Spain ..........................................................................................532\\nPegalajar, M. C. / University of Granada, Spain ......................................................................................... 1152\\nPelta, David A. / Universidad de Granada, Spain ..........................................................................................480\\nPeña, Dexmont / Universidad Autónoma de Nuevo León, Mexico...............................................................1462Peng, Chun-Cheng / University of London, UK .......................................................................................... 1411\\nPérez, Juan L. / University of A Coruña, Spain ..................................................................................... 118, 526\\nPérez, Óscar / Universidad Autónoma de Madrid, Spain  ...............................................................................282\\nPérez-Sánchez, Beatriz / University of A Coruña, Spain ...............................................................................667\\nPeriscal, David / University of A Coruña, Spain ............................................................................................618\\nPerl, Juergen / University of Mainz, Germany .............................................................................................1212\\nPeters, Georg / Munich University of Applied Sciences, Germany  ................................................................774\\nPiana, Michele / Universita’ di Verona, Italy  ..................................................................................................372\\nPlanet, Santiago / Universitat Ramon Llull, Spain  .........................................................................................541\\nPoggi, Agostino / Università di Parma, Italy  ................................................................................................1404\\nPoh, Kim Leng / National University of Singapore, Singapore  .....................................................................879', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f353b17b-2545-42f8-95f2-c5b5fa1fa3f0', embedding=None, metadata={'page_label': '12', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Porto Pazos, Ana Belén / University of A Coruña, Spain ..............................................................................167\\nPrincipe, Jose C. / University of Florida, USA...............................................................................................902\\nPutonet, Carlos G. / University of Granada, Spain ..................................................................................... 1226\\nQuackenbush, John / Harvard School of Public Health, USA  ........................................................................ 65\\nQueiroz, João / State University of Campinas (UNICAMP), & Federal University of Bahia, Brazil  .........1543\\nQuek, Chai / Nanyang Technological University, Singapore .......................................................................1396\\nRabuñal Dopico, Juan Ramón / University of A Coruña, Spain ..........................................................125, 383\\nRaducanu, Bogdan / Computer Vision Center, Spain  ....................................................................................625\\nRamos, Carlos / Polytechnic of Porto, Portugal  ..............................................................................................92\\nRashid, Shaista / University of Bradford, UK ................................................................................................337\\nRăsvan, Vladimir / University of Craiova, Romania  ...................................................................................1212\\nReyes-Galaviz, Orion Fausto / Universidad Autónoma de Tlaxcala, Mexico.......................................860, 867\\nReyes-García, Carlos Alberto / Instituto Nacional de Astrofísica Óptica y Electrónica, Mexico  ........860, 867\\nRiaño Sierra, Jesús M.  / University of Deusto, Spain  ..................................................................................344\\nRigas, Dimitris / University of Bradford, UK .................................................................................................337\\nRíos, Juan / Inteligencia Artificial, Facultad de Informatica, UPM, Spain  ...................................................767\\nRivero, Daniel / University of A Coruña, Spain .....................................................................................125, 618\\nRodrigues, Ernesto / Federal University of Technology, Brazil ....................................................................596\\nRodriquez, Gregorio Iglesias / University of Santiago de Compostela, Spain  ...................................144, 1614\\nRodríguez, M. Antón / University of Valladolid, Spain ..................................................................... 1490, 1497\\nRodríguez, Patricia Henríquez / University of Las Palmas de Gran Canaria, Spain ......................1266, 1439\\nRodríguez, Santiago / University of A Coruña, Spain ...................................................................................975\\nRodríguez, Sara / Universidad de Salamanca, Spain  ..................................................................................1316\\nRodríguez-Patón, Alfonso / Inteligencia Artificial, Facultad de Informatica, UPM, Spain  .........................767\\nRojas, F. / University of Granada, Spain ........................................................................................................910\\nRojas, F. J. / University of Granada, Spain ....................................................................................................910\\nRojas, I. / University of Granada, Spain .........................................................................................................910\\nRokach, Lior / Ben Gurion University, Israel ................................................................................................884\\nRomero, Carlos F. / University of Las Palmas de Gran Canaria, Spain ..................................................... 1447\\nRomero, Enrique / Technical University of Catalonia, Spain  .....................................................................1205\\nRomero-García, V . / Polytechnic University of Valencia, Spain  ..................................................................1296\\nRosa Zurera, Manuel / University of Alcalá, Spain  ..............................................................................933, 956\\nRoussaki, Ioanna / National Technical University of Athens, Greece ...............................................1418, 1524\\nRousset, Patrick / CEREQ, France  ..............................................................................................................1029\\nRoy, Shourya / IBM Research, India Research Lab, India .......................................................................99, 105\\nRuano, Marcos / Universidad Carlos III de Madrid, Spain .........................................................................1064\\nRus, Vasile / The University of Memphis, USA  ............................................................................................. 1179\\nRusiecki, Andrzej / Wroclaw University of Technology, Poland..................................................................1389Russomanno, David J. / University of Memphis, USA  ..........................................................................304, 312\\nSadri, Fariba / Imperial College London, UK..................................................................................................85Salazar, Addisson / iTEAM, Polytechnic University of Valencia, Spain  ................................................192, 199\\nSanchez, Rodrigo Carballo / University of Santiago de Compostela, Spain  ...................................... 144, 1614\\nSánchez, Eduardo / University of Santiago de Compostela, Spain  .....................................................138, 1278\\nSánchez, Ricardo / Universidad Autónoma de Nuevo León, Mexico  ........................................................... 1462\\nSánchez-Maroño, Noelia / University of A Coruña, Spain ............................................................................632\\nSánchez-Montañés, Manuel / Universidad Autónoma de Madrid, Spain  .............................................282, 561\\nSánchez-Pérez, J. V . / Polytechnic University of Valencia, Spain  ................................................................1296\\nSanchis, J. / Polytechnic University of Valencia, Spain  ................................................................................1296\\nSanchis de Miguel, Araceli / Universidad Carlos III de Madrid, Spain ........................................................554', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ba66c852-ff34-4cad-bd66-de28a02b4b66', embedding=None, metadata={'page_label': '13', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sarathy, Rathindra / Oklahoma State University, USA  ...............................................................................1323\\nSavić, Dragan A. / University of Exeter, UK ................................................................................................1042\\nSchleif, Frank-M. / University of Leipzig, Germany  .................................................................................... 1337\\nSeoane, Antonio / University of A Coruña, Spain ..........................................................................................873\\nSeoane, María / University of A Coruña, Spain .....................................................................................975, 982\\nSeoane Fernández, José Antonio / University of A Coruña, Spain .....................................236, 241, 744, 1273\\nSerantes, J. Andrés / University of A Coruña, Spain .....................................................................................744\\nŞerban, Gheorghe / University of Pitesti, Romania  .......................................................................................609\\nSergiadis, George D. / Aristotle University of Thessaloniki, Greece  .............................................................967\\nSerrano, Arturo / iTEAM, Polytechnic University of Valencia, Spain  ...................................................192, 199\\nSerrano-López, Antonio J. / University of Valencia, Spain ............................................................................. 71\\nSesmero Lorente, M. Paz / Universidad Carlos III de Madrid, Spain .......................................................... 554\\nShambaugh, Neal / West Virginia University, USA  ......................................................................................1310\\nSharkey, Amanda J.C. / University of Sheffield, UK  ...........................................................................161, 1537\\nShilov, Nikolay V . / Russian Academy of Science, Institute of Informatics Systems, Russia  ........................ 1089\\nSieber, Tanja / University of Miskolc, Hungary  ........................................................................................... 1130\\nSilaghi, Marius C. / Florida Insitute of Technology, USA  ..............................................................................507\\nSilva, Ivan N. / University of São Paulo, Brazil  ........................................................................................... 1121\\nSloot, Peter M.A. / Section Computational Science, The University of Amsterdam, The Netherlands ..........353\\nSocoró Carrié, Joan-Claudi / Universitat Ramon Llull, Spain  .............................................................541, 719\\nSofron, Emil / University of Pitesti, Romania  ................................................................................................609\\nSong, Yu-Chen / Inner Mongolia University of Science and Technology, China ...........................................297\\nSorathia, Vikram / Dhirubhai Ambani Institute of Information and Communication Technology, India .....494\\nSoria-Olivas, Emilio / University of Valencia, Spain ....................................................................................... 71\\nSossa, Humberto / Center for Computing Research, IPN, Mexico  ................................................................248\\nSouza, Flavio / UERJ, Brazil ..................................................................................................................808, 817\\nStanković, Milan / University of Belgrade, Serbia  ........................................................................................ 950\\nStathis, Kostas / Royal Holloway, University of London, UK ..........................................................................85\\nSuárez, Sonia / University of A Coruña, Spain .......................................................................................975, 982\\nSubramaniam, L. Venkata / IBM Research, India Research Lab, India ................................................. 99, 105\\nSulc, Bohumil / Czech Technical University in Prague, Czech Republic  ....................................................... 567\\nSzenher, Matthew / University of Edinburgh, UK ....................................................................................... 1185\\nTaktak, Azzam / Royal Liverpool University Hospital, UK  ...........................................................................390\\nTang, Zaiyong / Salem State College, USA  ......................................................................................................51\\nTapia, Dante I. / Universidad de Salamanca, Spain  ..................................................................................... 1316\\nTaveira Pinto, Francisco / University of Santiago de Compostela, Spain  ................................................... 1603\\nTejera Santana, Aday / University of Las Palmas de Gran Canaria, Spain .................................................270\\nTéllez, Ricardo / Technical University of Catalonia, Spain  .........................................................................1095\\nTettamanzi, Andrea G. B. / University of Milan, Italy  ..................................................................................575\\nTikk, Domonkos / Budapest University of Technology and Economics, Hungary ........................................ 654\\nTlelo-Cuautle, Esteban / Instituto Nacional de Astrofísica Óptica y Electrónica, Mexico  ...........................867\\nTomaiuolo, Michele / Università di Parma, Italy  .........................................................................................1404\\nTorijano Gordo, Elena / University of Alcalá, Spain.....................................................................................956\\nTorres, Manuel / University of Castilla – La Mancha, Spain  ........................................................................547\\nTravieso González, Carlos M. / University of Las Palmas de Gran Canaria, Spain ........................ 1259, 1447\\nTumin, Sharil / University of Bergen, Norway .............................................................................................1610\\nTurgay, Safiye / Abant İzzet Baysal University, Turkey  ..................................................................................924\\nValdés, Julio J. / National Research Council Canada, Canada .........................................................1205, 1589\\nValenzuela, O. / University of Granada, Spain .............................................................................................. 910', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ed3cffa-3e84-4619-b43a-4d7a11e4eb1c', embedding=None, metadata={'page_label': '14', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vargas, J. Francisco / University of Las Palmas de Gran Canaria, Spain & Universidad de Antioquia, \\n      Colombia ..................................................................................................................................................1232\\nVazquez, Roberto A. / Center for Computing Research, IPN, Mexico  ..........................................................248\\nVázquez Naya, José Manuel / University of A Coruña, Spain ..........................................................1283, 1290\\nVellasco, Marley / PUC-Rio, Brazil  ....................................................................................................... 808, 817\\nVerdegay, José L. / Universidad de Granada, Spain .....................................................................................480\\nVillmann, Thomas / University of Leipzig, Germany  ..................................................................................1337\\nVlachos, Ioannis K. / Aristotle University of Thessaloniki, Greece  ...............................................................967\\nVoiry, Matthieu / University of Paris, France & SAGEM REOSC, France  ..................................................131\\nWang, John / Montclair State University, USA  ............................................................418, 424, 974, 963, 1036\\nWilkosz, Kazimierz / Wroclaw University of Technology, Poland ..............................................................1356\\nWilliamson, Kristian / Statistics Canada, Canada  ..........................................................................................31\\nWong, T. T. / The Hong Kong Polytechnic University, Hong Kong ............................................................. 1568\\nXu, Lei / Chinese University of Hong Kong, Hong Kong & Peking University, China ................318, 892, 1343\\nYaman, Fahrettin / Abant İzzet Baysal University, Turkey  ............................................................................924\\nYan, Hong / City University of Hong Kong, Hong Kong & University of Sydney, Australia  ......................... 289\\nYan, Yan / Tsinghua University, Beijing, China ............................................................................................ 1455\\nYao, James / Montclair State University, USA  ........................................................................................418, 424\\nYokoo, Makoto / Kyushu University, Japan ................................................................................................... 507\\nYousuf, Muhammad Ali / Tecnologico de Monterrey – Santa Fe Campus, México....................................1383Zajac, Pavol / Slovak University of Technology, Slovakia ......................................................................179, 186\\nZamora-Martínez, Francisco / Universidad Politécnica de Valencia, Spain ............................................... 231\\nZarri, Gian Piero / LaLIC, University Paris 4-Sorbonne, France ..................................................... 1159, 1167\\nZatarain, Ramon / Instituto Tecnologico de Culiacan, Mexico  .....................................................................860\\nZhang, Yu-Jin / Tsinghua University, Beijing, China ................................................................................... 1455\\nZhao, Yi / Fernuniversitaet in Hagen, Germany...........................................................................................1079Ziemke, Tom / University of Skovde, Sweden  ............................................................................................... 1376', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='813080f3-51af-4e61-b1c0-c2e0c18af74c', embedding=None, metadata={'page_label': '15', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents\\nby Volume\\nVolume I\\nActive Learning with SVM / Jun Jiang, City University of Hong Kong, Hong Kong; and \\nHorace H. S. Ip, City University of Hong Kong, Hong Kong  .............................................................................. 1\\nAdaptive Algorithms for Intelligent Geometric Computing / M. L. Gavrilova, University of Calgary, \\nCanada ................................................................................................................................................................. 9\\nAdaptive Business Intelligence / Zbigniew Michalewicz, The University of Adelaide, Australia ..................... 16\\nAdaptive Neural Algorithms for PCA and ICA / Radu Mutihac, University of Bucharest, Romania ............... 22\\nAdaptive Neuro-Fuzzy Systems / Larbi Esmahi, Athabasca University, Canada; Kristian Williamson, \\nStatistics Canada, Canada; and Elarbi Badidi, United Arab Emirates University, UAE ................................. 31\\nAdaptive Technology and Its Applications / João José Neto, Universidade de São Paulo, Brazil  ................... 37\\nAdvanced Cellular Neural Networks Image Processing / J. Álvaro Fernández, University of \\nExtremadura, Badajoz, Spain  ............................................................................................................................ 45\\nAgent-Based Intelligent System Modeling / Zaiyong Tang, Salem State College, USA; Xiaoyu Huang, \\nUniversity of Shanghai for Science & Technology, China; and Kallol Bagchi, University of Texas \\nat El Paso, USA  .................................................................................................................................................. 51\\nAI and Ideas by Statistical Mechanics / Lester Ingber, Lester Ingber Research, USA  ...................................... 58\\nAI Methods for Analyzing Microarray Data / Amira Djebbari, National Research Council Canada, \\nCanada; Aedín C. Culhane, Harvard School of Public Health, USA; Alice J. Armstrong, The George Washington University, USA; and John Quackenbush, Harvard School of Public Health, USA .................................................................................................................................................................... 65\\nAI Walk from Pharmacokinetics to Marketing, An / José D. Martín-Guerrero, University of Valencia, \\nSpain; Emilio Soria-Olivas, University of Valencia, Spain; Paulo J.G. Lisboa, Liverpool John Moores University, UK; and Antonio J. Serrano-López, University of Valencia, Spain ................................................ 71\\nAlgorithms for Association Rule Mining / Vasudha Bhatnagar, University of Delhi, India; Anamika Gupta, University of Delhi, India; and Naveen Kumar, University of Delhi, India  ........................... 76', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfb6259b-bb5f-4a6a-ae27-518aa6540513', embedding=None, metadata={'page_label': '16', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Ambient Intelligence / Fariba Sadri, Imperial College London, UK; and Kostas Stathis, \\nRoyal Holloway, University of London, UK ...................................................................................................... 85\\nAmbient Intelligence Environments / Carlos Ramos, Polytechnic of Porto, Portugal  ..................................... 92\\nAnalytics for Noisy Unstructured Text Data I / Shourya Roy, IBM Research, India Research Lab, India; \\nand L. Venkata Subramaniam, IBM Research, India Research Lab, India........................................................ 99\\nAnalytics for Noisy Unstructured Text Data II / L. Venkata Subramaniam, IBM Research, India Research Lab, India; and Shourya Roy, IBM Research, India Research Lab, India....................................................... 105\\nAngiographic Images Segmentation Techniques / Francisco J. Nóvoa, University of A Coruña, Spain; Alberto Curra, University of A Coruña, Spain; M. Gloria López, University of A Coruña, Spain; and Virginia Mato, University of A Coruña, Spain  ................................................................................................. 110\\nANN Application in the Field of Structural Concrete / Juan L. Pérez, University of A Coruña, Spain; \\nMª Isabel Martínez, University of A Coruña, Spain; and Manuel F . Herrador, University of A Coruña, Spain .................................................................................................................................................. 118\\nANN Development with EC Tools: An Overview / Daniel Rivero, University of A Coruña, Spain; and Juan Ramón Rabuñal Dopico, University of A Coruña, Spain  ................................................................. 125\\nANN-Based Defects’ Diagnosis of Industrial Optical Devices / Matthieu Voiry, University of Paris, \\nFrance & SAGEM REOSC, France; Véronique Amarger, University of Paris, France; Joel Bernier, SAGEM REOSC, France; and Kurosh Madani, University of Paris, France  ................................................. 131\\nArtificial Intelligence and Education / Eduardo Sánchez, University of Santiago de Compostela, Spain; \\nand Manuel Lama, University of Santiago de Compostela, Spain  .................................................................. 138\\nArtificial Intelligence and Rubble-Mound Breakwater Stability / Gregorio Iglesias Rodriquez, University of \\nSantiago de Compostela, Spain; Alberte Castro Ponte, University of Santiago de Compostela, Spain; Rodrigo Carballo Sanchez, University of Santiago de Compostela, Spain; and Miguel Ángel Losada Rodriguez, University of Granada, Spain  ........................................................................................................ 144\\nArtificial Intelligence for Information Retrieval / Thomas Mandl, University of Hildesheim, Germany ........ 151\\nArtificial Intelligence in Computer-Aided Diagnosis / Paulo Eduardo Ambrósio, Santa Cruz State University, Brazil ............................................................................................................................................. 157\\nArtificial Neural Networks and Cognitive Modelling / Amanda J.C. Sharkey, University of Sheffield, UK  ...161\\nArtificial NeuroGlial Networks / Ana Belén Porto Pazos, University of A Coruña, Spain; Alberto Alvarellos \\nGonzález, University of A Coruña, Spain; and Félix Montañés Pazos, University of A Coruña, Spain ......... 167\\nAssociation Rule Mining / Vasudha Bhatnagar, University of Delhi, India; and Sarabjeet Kochhar, \\nUniversity of Delhi, India  ................................................................................................................................ 172\\nAutomated Cryptanalysis / Otokar Grošek, Slovak University of Technology, Slovakia; and \\nPavol Zajac, Slovak University of Technology, Slovakia  ................................................................................. 179', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a042eee-7577-48a0-9cf5-b5b90ab9b2ec', embedding=None, metadata={'page_label': '17', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Automated Cryptanalysis of Classical Ciphers / Otokar Grošek, Slovak University of Technology, \\nSlovakia; and Pavol Zajac, Slovak University of Technology, Slovakia .......................................................... 186\\nAutomatic Classification of Impact-Echo Spectra I / Addisson Salazar, iTEAM, Polytechnic \\nUniversity of Valencia, Spain; and Arturo Serrano, iTEAM, Polytechnic University of Valencia, Spain  ......192\\nAutomatic Classification of Impact-Echo Spectra II / Addisson Salazar, iTEAM, Polytechnic \\nUniversity of Valencia, Spain; and Arturo Serrano, iTEAM, Polytechnic University of Valencia, Spain  .......199\\nA VI of Surface Flaws on Manufactures I / Girolamo Fornarelli, Politecnico di Bari, Italy; \\nand Antonio Giaquinto, Politecnico di Bari, Italy  ........................................................................................... 206\\nA VI of Surface Flaws on Manufactures II / Girolamo Fornarelli, Politecnico di Bari, Italy; and Antonio Giaquinto, Politecnico di Bari, Italy  ........................................................................................... 211\\nBasic Cellular Neural Networks Image Processing / J. Álvaro Fernández, \\nUniversity of Extremadura, Badajoz, Spain ..................................................................................................... 218\\nBayesian Neural Networks for Image Restoration / Radu Mutihac, University of Bucharest, \\nRomania ........................................................................................................................................................... 223\\nBehaviour-Based Clustering of Neural Networks / María José Castro-Bleda, Universidad Politécnica de Valencia, Spain; Salvador España-Boquera, Universidad Politécnica de Valencia, Spain; and Francisco Zamora-Martínez, Universidad Politécnica de Valencia, Spain ................................................................................................................................................................ 231\\nBio-Inspired Algorithms in Bioinformatics I / José Antonio Seoane Fernández, University of \\nA Coruña, Spain; and Mónica Miguélez Rico, University of A Coruña, Spain ............................................... 236\\nBio-Inspired Algorithms in Bioinformatics II / José Antonio Seoane Fernández, University \\nof A Coruña, Spain; and Mónica Miguélez Rico, University of A Coruña, Spain  ........................................... 241\\nBioinspired Associative Memories / Roberto A. Vazquez, Center for Computing Research, IPN, \\nMexico; and Humberto Sossa, Center for Computing Research, IPN, Mexico  ............................................... 248\\nBio-Inspired Dynamical Tools for Analyzing Cognition / Manuel G. Bedia, University of \\nZaragoza, Spain; Juan M. Corchado, University of Salamanca, Spain; and Luis F . Castillo, National University, Colombia ........................................................................................................................ 256\\nBiometric Security Technology / Marcos Faundez-Zanuy, Escola Universitària Politècnica de Mataró, Spain .................................................................................................................................................. 262\\nBlind Source Separation by ICA / Miguel A. Ferrer, University of Las Palmas de Gran Canaria, Spain; and Aday Tejera Santana, University of Las Palmas de Gran Canaria, Spain  ............................................... 270\\nChaotic Neural Networks / Emilio Del-Moral-Hernandez, University of São Paulo, Brazil  .......................... 275\\nClass Prediction in Test Sets with Shifted Distributions / Óscar Pérez, Universidad Autónoma \\nde Madrid, Spain; and Manuel Sánchez-Montañés, Universidad Autónoma de Madrid, Spain  ..................... 282', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd3ca81f-55fb-4d89-a1ad-e22036a1bbb4', embedding=None, metadata={'page_label': '18', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Cluster Analysis of Gene Expression Data / Alan Wee-Chung Liew, Griffith University, Australia; \\nNgai-Fong Law, The Hong Kong Polytechnic University, Hong Kong; and Hong Yan, City University of Hong Kong, Hong Kong & University of Sydney, Australia........................................................................ 289\\nClustering Algorithm for Arbitrary Data Sets / Yu-Chen Song, Inner Mongolia University of Science and Technology, China; and Hai-Dong Meng, Inner Mongolia University of Science and Technology, China ................................................................................................................................................................ 297\\nCNS Tumor Prediction Using Gene Expression Data Part I / Atiq Islam, University of Memphis, USA; \\nKhan M. Iftekharuddin, University of Memphis, USA; E. Olusegun George, University of Memphis, USA; and David J. Russomanno, University of Memphis, USA  ................................................................................ 304\\nCNS Tumor Prediction Using Gene Expression Data Part II / Atiq Islam, University of Memphis, USA; \\nKhan M. Iftekharuddin, University of Memphis, USA; E. Olusegun George, University of Memphis, USA; and David J. Russomanno, University of Memphis, USA  ................................................................................ 312\\nCombining Classifiers and Learning Mixture-of-Experts / Lei Xu, Chinese University of Hong Kong, \\nHong Kong & Peking University, China; and Shun-ichi Amari, Brain Science Institute, Japan  .................... 318\\nCommonsense Knowledge Representation I / Phillip Ein-Dor, Tel-Aviv University, Israel  ............................ 327\\nCommonsense Knowledge Representation II / Phillip Ein-Dor, Tel-Aviv University, Israel  .......................... 334\\nComparative Study on E-Note-Taking, A / Shaista Rashid, University of Bradford, UK; \\nand Dimitris Rigas, University of Bradford, UK ............................................................................................. 337\\nComparison of Cooling Schedules for Simulated Annealing, A / José Fernando Díaz Martín, University of Deusto, Spain; and Jesús M. Riaño Sierra, University of Deusto, Spain  .................................. 344\\nComplex Systems Modeling by Cellular Automata / Jiří Kroc, Section Computational Science, \\nThe University of Amsterdam, The Netherlands; and Peter M.A. Sloot, Section Computational Science, The University of Amsterdam, The Netherlands .............................................................................................. 353\\nComplex-Valued Neural Networks / Tohru Nitta, AIST, Japan  ....................................................................... 361\\nComponent Analysis in Artificial Vision / Oscar Déniz Suárez, University of Las Palmas de Gran Canaria, Spain; and Gloria Bueno García, University of Castilla-La Mancha, Spain  .................................................. 367\\nComputational Methods in Biomedical Imaging / Michele Piana, Universita’ di Verona, Italy  ..................... 372\\nComputer Morphogenesis in Self-Organizing Structures / Enrique Fernández-Blanco, \\nUniversity of A Coruña, Spain; and Julián Dorado, University of A Coruña, Spain ...................................... 377\\nComputer Vision for Wave Flume Experiments / Óscar Ibáñez, University of A Coruña, Spain; \\nand Juan Rabuñal Dopico, University of A Coruña, Spain  ............................................................................. 383\\nConditional Hazard Estimating Neural Networks / Antonio Eleuteri, Royal Liverpool University Hospital, \\nUK; Azzam Taktak, Royal Liverpool University Hospital, UK; Bertil Damato, Royal Liverpool University Hospital, UK; Angela Douglas, Liverpool Women’ s Hospital, UK; and Sarah Coupland, Royal Liverpool University Hospital, UK ................................................................................................................................... 390', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00623c0e-9e7e-46fe-b2ba-669901c9ba0c', embedding=None, metadata={'page_label': '19', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Configuration / Luca Anselma, Università di Torino, Italy; and Diego Magro, \\nUniversità di Torino, Italy  ................................................................................................................................ 396\\nConstraint Processing / Roman Barták, Charles University in Prague, Czech Republic  ................................ 404\\nContinuous ACO in a SVR Traffic Forecasting Model / Wei-Chiang Hong, Oriental Institute \\nof Technology, Taiwan  ...................................................................................................................................... 410\\nData Mining Fundamental Concepts and Critical Issues / John Wang, Montclair State University, USA; \\nQiyang Chen, Montclair State University, USA; and James Yao, Montclair State University, USA  ............... 418\\nData Warehousing Development and Design Methodologies / James Yao, Montclair State University, \\nUSA; and John Wang, Montclair State University, USA  ................................................................................. 424\\nDecision Making in Intelligent Agents / Mats Danielson, Stockholm University, Sweden & \\nRoyal Institute of Technology, Sweden; and Love Ekenberg, Stockholm University, Sweden & Royal Institute of Technology, Sweden ............................................................................................................. 431\\nDecision Tree Applications for Data Modelling / Man Wai Lee, Brunel University, UK; \\nKyriacos Chrysostomou, Brunel University, UK; Sherry Y. Chen, Brunel University, UK; and Xiaohui Liu, Brunel University, UK  ................................................................................................................. 437\\nDempster-Shafer Theory, The / Malcolm J. Beynon, Cardiff University, UK ................................................. 443\\nDependency Parsing: Recent Advances / Ruket Çakıcı, University of Edinburgh, UK ................................... 449\\nDesigning Unsupervised Hierarchical Fuzzy Logic Systems / M. Mohammadian, \\nUniversity of Canberra, Australia ................................................................................................................... 456\\nDevelopmental Robotics / Max Lungarella, University of Zurich, Switzerland; and Gabriel Gómez, \\nUniversity of Zurich, Switzerland  .................................................................................................................... 464\\nDevice-Level Majority von Neumann Multiplexing / Valeriu Beiu, United Arab Emirates University, \\nUAE; Walid Ibrahim, United Arab Emirates University, UAE; and Sanja Lazarova-Molnar, United Arab Emirates University, UAE ................................................................................................................................ 471\\nDifferent Approaches for Cooperation with Metaheuristics / José M. Cadenas, Universidad de \\nMurcia, Spain; Mª Carmen Garrido, Universidad de Murcia, Spain; Enrique Muñoz, Universidad de Murcia, Spain; Carlos Cruz-Corona, Universidad de Granada, Spain; David A. Pelta, Universidad de Granada, Spain; and José L. Verdegay, Universidad de Granada, Spain ............................. 480\\nDifferential Evolution with Self-Adaptation / Janez Brest, University of Maribor, Slovenia  ......................... 488\\nDiscovering Mappings Between Ontologies / Vikram Sorathia, Dhirubhai Ambani Institute of \\nInformation and Communication Technology, India; and Anutosh Maitra, Dhirubhai Ambani Institute of Information and Communication Technology, India ................................................................................... 494\\nDisk-Based Search / Stefan Edelkamp, University of Dortmund, Germany; and Shahid Jabbar, University of Dortmund, Germany .................................................................................................................. 501', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c9c8caa-2b2a-4776-b33f-01b0995d3f85', embedding=None, metadata={'page_label': '20', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Distributed Constraint Reasoning / Marius C. Silaghi, Florida Insitute of Technology, USA; \\nand Makoto Yokoo, Kyushu University, Japan ................................................................................................ 507\\nDistributed Representation of Compositional Structure / Simon D. Levy, Washington and Lee \\nUniversity, USA  ................................................................................................................................................ 514\\nEA Multi-Model Selection for SVM / Gilles Lebrun, University of Caen Basse-Normandie, \\nFrance; Olivier Lezoray, University of Caen Basse-Normandie, France; Christophe Charrier, \\nUniversity of Caen Basse-Normandie, France; and Hubert Cardot, University François-Rabelais of Tours, France ............................................................................................................................................... 520\\nEC Techniques in the Structural Concrete Field / Juan L. Pérez, University of A Coruña, Spain; Belén \\nGonzález-Fonteboa, University of A Coruña, Spain; and Fernando Martínez Abella, University of A Coruña, Spain ......................................................................................................................... 526\\nE-Learning in New Technologies / Nieves Pedreira, University of A Coruña, Spain; \\nJosé Ramón Méndez Salgueiro, University of A Coruña, Spain; and Manuel Martínez Carballo, University of A Coruña, Spain ......................................................................................................................... 532\\nEmerging Applications in Immersive Technologies / Darryl N. Davis, University of Hull, UK; and Paul M. Chapman, University of Hull, UK  ............................................................................................... 536\\nEmulating Subjective Criteria in Corpus Validation / Ignasi Iriondo, Universitat Ramon Llull, Spain; Santiago Planet, Universitat Ramon Llull, Spain; Francesc Alías, Universitat Ramon Llull, Spain; Joan-Claudi Socoró, Universitat Ramon Llull, Spain; and Elisa Martínez, Universitat Ramon Llull, Spain ................................................................................................................................................................ 541\\nVolume II\\nEnergy Minimizing Active Models in Artificial Vision / Gloria Bueno García, University of \\nCastilla – La Mancha, Spain; Antonio Martínez, University of Castilla – La Mancha, Spain; Roberto González, University of Castilla – La Mancha, Spain; and Manuel Torres, University of Castilla – La Mancha, Spain  ...................................................................................................... 547\\nEnsemble of ANN for Traffic Sign Recognition / M. Paz Sesmero Lorente, Universidad Carlos III \\nde Madrid, Spain; Juan Manuel Alonso-Weber, Universidad Carlos III de Madrid, Spain; Germán Gutiérrez Sánchez, Universidad Carlos III de Madrid, Spain; Agapito Ledezma Espino, Universidad Carlos III de Madrid, Spain; and Araceli Sanchis de Miguel, Universidad Carlos III de Madrid, Spain ......................................................................................................................................... 554\\nEnsemble of SVM Classifiers for Spam Filtering / Ángela Blanco, Universidad Pontificia de \\nSalamanca, Spain; and Manuel Martín-Merino, Universidad Pontificia de Salamanca, Spain  ..................... 561\\nEvolutionary Algorithms in Discredibility Detection / Bohumil Sulc, Czech Technical University \\nin Prague, Czech Republic; and David Klimanek, Czech Technical University in Prague, Czech Republic  ................................................................................................................................................. 567\\nEvolutionary Approaches for ANNs Design / Antonia Azzini, University of Milan, Italy; and Andrea G.B. Tettamanzi, University of Milan, Italy......................................................................................... 575', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee511637-b106-4913-b32c-46f816f1c730', embedding=None, metadata={'page_label': '21', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Evolutionary Approaches to Variable Selection / Marcos Gestal, University of A Coruña, Spain; \\nand José Manuel Andrade, University of A Coruña, Spain ............................................................................. 581\\nEvolutionary Computing Approach for Ad-Hoc Networks / Prayag Narula, University of Delhi, \\nIndia; Sudip Misra, Yale University, USA; and Sanjay Kumar Dhurandher, University of Delhi, India ................................................................................................................................................................. 589\\nEvolutionary Grammatical Inference / Ernesto Rodrigues, Federal University of Technology, \\nBrazil; and Heitor Silvério Lopes, Federal University of Technology, Brazil ................................................. 596\\nEvolutionary Robotics / J. A. Becerra, University of A Coruña, Spain; and R. J. Duro, University of \\nA Coruña, Spain ............................................................................................................................................... 603\\nEvolved Synthesis of Digital Circuits / Laurenţiu Ionescu, University of Pitesti, Romania; Alin Mazare, \\nUniversity of Pitesti, Romania; Gheorghe Şerban, University of Pitesti, Romania; and Emil Sofron, University of Pitesti, Romania  ......................................................................................................................... 609\\nEvolving Graphs for ANN Development and Simplification / Daniel Rivero, University of \\nA Coruña, Spain; and David Periscal, University of A Coruña, Spain  ........................................................... 618\\nFacial Expression Recognition for HCI Applications / Fadi Dornaika, Institut Géographique National, France; and Bogdan Raducanu, Computer Vision Center, Spain.................................................................... 625\\nFeature Selection / Noelia Sánchez-Maroño, University of A Coruña, Spain; and \\nAmparo Alonso-Betanzos, University of A Coruña, Spain ............................................................................... 632\\nFeed-Forward Artificial Neural Network Basics / Lluís A. Belanche Muñoz, Universitat \\nPolitècnica de Catalunya, Spain  ...................................................................................................................... 639\\nFinding Multiple Solutions with GA in Multimodal Problems / Marcos Gestal, University of \\nA Coruña, Spain; and Mari Paz Gómez-Carracedo, University of A Coruña, Spain  ..................................... 647\\nFull-Text Search Engines for Databases / László Kovács, University of Miskolc, Hungary; \\nand Domonkos Tikk, Budapest University of Technology and Economics, Hungary  ...................................... 654\\nFunctional Dimension Reduction for Chemometrics / Tuomas Kärnä, Helsinki University of \\nTechnology, Finland; and Amaury Lendasse, Helsinki University of Technology, Finland  ............................ 661\\nFunctional Networks / Oscar Fontenla-Romero, University of A Coruña, Spain; \\nBertha Guijarro-Berdiñas, University of A Coruña, Spain; and Beatriz Pérez-Sánchez, University of A Coruña, Spain ......................................................................................................................... 667\\nFuzzy Approximation of DES State / Juan Carlos González-Castolo, CINVESTAV Unidad \\nGuadalajara, Mexico; and Ernesto López-Mellado, CINVESTAV Unidad Guadalajara, Mexico  ................. 677\\nFuzzy Control Systems: An Introduction / Guanrong Chen, City University of Hong Kong, Hong Kong; and Young Hoon Joo, Kunsan National University, Korea ......................................................... 688\\nFuzzy Decision Trees / Malcolm J. Beynon, Cardiff University, UK .............................................................. 696', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='733e13bc-1f9d-4be3-b0a1-a203094fbaa6', embedding=None, metadata={'page_label': '22', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Fuzzy Graphs and Fuzzy Hypergraphs / Leonid S. Bershtein, Taganrog Technological Institute \\nof Southern Federal University, Russia; and Alexander V . Bozhenyuk, Taganrog Technological Institute of Southern Federal University, Russia ............................................................................................. 704\\nFuzzy Logic Applied to Biomedical Image Analysis / Alfonso Castro, University of A Coruña, Spain; and Bernardino Arcay, University of A Coruña, Spain ........................................................................ 710\\nFuzzy Logic Estimator for Variant SNR Environments / Rosa Maria Alsina Pagès, Universitat Ramon Llull, Spain; Clàudia Mateo Segura, Universitat Ramon Llull, Spain; and Joan-Claudi Socoró Carrié, Universitat Ramon Llull, Spain........................................................................................................................ 719\\nFuzzy Rule Interpolation / Szilveszter Kovács, University of Miskolc, Hungary  ............................................ 728\\nFuzzy Systems Modeling: An Introduction / Young Hoon Joo, Kunsan National University, Korea; and Guanrong Chen, City University of Hong Kong, Hong Kong, China  ...................................................... 734\\nGene Regulation Network Use for Information Processing / Enrique Fernandez-Blanco, University of \\nA Coruña, Spain; and J.Andrés Serantes, University of A Coruña, Spain ...................................................... 744\\nGenetic Algorithm Applications to Optimization Modeling / Pi-Sheng Deng, California State University \\nat Stanislaus, USA  ............................................................................................................................................ 748\\nGenetic Algorithms for Wireless Sensor Networks / João H. Kleinschmidt, State University of Campinas, \\nBrazil ................................................................................................................................................................ 755\\nGenetic Fuzzy Systems Applied to Ports and Coasts Engineering / Óscar Ibáñez, University of \\nA Coruña, Spain; and Alberte Castro Ponte, University of Santiago de Compostela, Spain  .......................... 759\\nGrammar-Guided Genetic Programming / Daniel Manrique, Inteligencia Artificial, Facultad de \\nInformatica, UPM, Spain; Juan Ríos, Inteligencia Artificial, Facultad de Informatica, UPM, Spain; and Alfonso Rodríguez-Patón, Inteligencia Artificial, Facultad de Informatica, UPM, Spain  ....................... 767\\nGranular Computing / Georg Peters, Munich University of Applied Sciences, Germany  ............................... 774\\nGrowing Self-Organizing Maps for Data Analysis / Soledad Delgado, Technical University of Madrid, Spain; Consuelo Gonzalo, Technical University of Madrid, Spain; Estíbaliz Martínez, Technical University of Madrid, Spain; and Águeda Arquero, Technical University of Madrid, Spain  .......................... 781\\nGTM User Modeling for aIGA Weight Tuning in TTS Synthesis / Lluís Formiga, Universitat Ramon Llull, \\nSpain; and Francesc Alías, Universitat Ramon Llull, Spain  ........................................................................... 788\\nHandling Fuzzy Similarity for Data Classification / Roy Gelbard, Bar-Ilan University, Israel; \\nand Avichai Meged, Bar-Ilan University, Israel .............................................................................................. 796\\nHarmony Search for Multiple Dam Scheduling / Zong Woo Geem, Johns Hopkins University, USA  ............ 803\\nHierarchical Neuro-Fuzzy Systems Part I / Marley Vellasco, PUC-Rio, Brazil; Marco Pacheco, \\nPUC-Rio, Brazil; Karla Figueiredo, UERJ, Brazil; and Flavio Souza, UERJ, Brazil  .................................... 808', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9fa10da1-5b30-4097-837b-fddd8e7d79ca', embedding=None, metadata={'page_label': '23', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Hierarchical Neuro-Fuzzy Systems Part II / Marley Vellasco, PUC-Rio, Brazil; Marco Pacheco, \\nPUC-Rio, Brazil; Karla Figueiredo, UERJ, Brazil; and Flavio Souza, UERJ, Brazil  .................................... 817\\nHierarchical Reinforcement Learning / Carlos Diuk, Rutgers University, USA; and \\nMichael Littman, Rutgers University, USA  ...................................................................................................... 825\\nHigh Level Design Approach for FPGA Implementation of ANNs / Nouma Izeboudjen, Center de \\nDéveloppement des Technologies Avancées (CDTA), Algérie; Ahcene Farah, Ajman University, \\nUAE; Hamid Bessalah, Center de Développement des Technologies Avancées (CDTA), Algérie; Ahmed. Bouridene, Queens University of Belfast, Ireland; and Nassim Chikhi, Center de Développement des Technologies Avancées (CDTA), Algérie  ......................................................... 831\\nHOPS: A Hybrid Dual Camera Vision System / Stefano Cagnoni, Università degli Studi di Parma, Italy; Monica Mordonini, Università degli Studi di Parma, Italy; Luca Mussi, Università degli Studi di Perugia, Italy; and Giovanni Adorni, Università degli Studi di Genova, Italy  ............................................... 840\\nHybrid Dual Camera Vision System / Stefano Cagnoni, Università degli Studi di Parma, Italy; Monica Mordonini, Università degli Studi di Parma, Italy; Luca Mussi, Università degli Studi di Perugia, Italy; and Giovanni Adorni, Università degli Studi di Genova, Italy  ............................................... 848\\nHybrid Meta-Heuristics Based System for Dynamic Scheduling / Ana Maria Madureira, \\nPolytechnic Institute of Porto, Portugal  .......................................................................................................... 853\\nHybrid System for Automatic Infant Cry Recognition I, A / Carlos Alberto Reyes-García, Instituto \\nNacional de Astrofísica Óptica y Electrónica, Mexico; Ramon Zatarain, Instituto Tecnologico de Culiacan, Mexico; Lucia Barron, Instituto Tecnologico de Culiacan, Mexico; and Orion Fausto Reyes-Galaviz, Universidad Autónoma de Tlaxcala, Mexico  ................................................... 860\\nHybrid System for Automatic Infant Cry Recognition II, A / Carlos Alberto Reyes-García, \\nInstituto Nacional de Astrofísica Óptica y Electrónica, Mexico; Sandra E. Barajas, Instituto Nacional de Astrofísica Óptica y Electrónica, Mexico; Esteban Tlelo-Cuautle, Instituto Nacional de Astrofísica Óptica y Electrónica, Mexico; and Orion Fausto Reyes-Galaviz, Universidad Autónoma de Tlaxcala, Mexico  .............................................................................................................................................................. 867\\nIA Algorithm Acceleration Using GPUs / Antonio Seoane, University of A Coruña, Spain; and Alberto Jaspe, University of A Coruña, Spain  ................................................................................................. 873\\nImproving the Naïve Bayes Classifier / Liwei Fan, National University of Singapore, Singapore; \\nand Kim Leng Poh, National University of Singapore, Singapore  .................................................................. 879\\nIncorporating Fuzzy Logic in Data Mining Tasks / Lior Rokach, Ben Gurion University, Israel ................... 884\\nIndependent Subspaces / Lei Xu, Chinese University of Hong Kong, Hong Kong & Peking University, \\nChina ................................................................................................................................................................ 892\\nInformation Theoretic Learning / Deniz Erdogmus, Northeastern University, USA; and \\nJose C. Principe, University of Florida, USA  .................................................................................................. 902', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='867688fe-3e38-4d00-bb4f-53560a5f2efb', embedding=None, metadata={'page_label': '24', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Intelligent Classifier for Atrial Fibrillation (ECG) / O.Valenzuela, University of Granada, Spain; \\nI.Rojas, University of Granada, Spain; F .Rojas, University of Granada, Spain; A.Guillen, \\nUniversity of Granada, Spain; L.J Herrera, University of Granada, Spain; F .J.Rojas, University of Granada, Spain; and M.Cepero, University of Granada, Spain  ..................................................................... 910\\nIntelligent MAS in System Engineering and Robotics / G. Nicolás Marichal, University of La Laguna, \\nSpain; and Evelio J. González, University of La Laguna, Spain  ..................................................................... 917\\nIntelligent Query Answering Mechanism in Multi Agent Systems / Safiye Turgay, Abant İzzet Baysal \\nUniversity, Turkey; and Fahrettin Yaman, Abant İzzet Baysal University, Turkey  .......................................... 924\\nIntelligent Radar Detectors / Raúl Vicen Bueno, University of Alcalá, Spain; Manuel Rosa Zurera, \\nUniversity of Alcalá, Spain; María Pilar Jarabo Amores, University of Alcalá, Spain; Roberto Gil Pita, University of Alcalá, Spain; and David de la Mata Moya, University of Alcalá, Spain ................................................................................................................................................................ 933\\nIntelligent Software Agents Analysis in E-Commerce I / Xin Luo, The University of New Mexico, USA; \\nand Somasheker Akkaladevi, Virginia State University, USA  .......................................................................... 940\\nIntelligent Software Agents Analysis in E-Commerce II / Xin Luo, The University of New Mexico, USA; \\nand Somasheker Akkaladevi, Virginia State University, USA  .......................................................................... 945\\nIntelligent Software Agents with Applications in Focus / Mario Janković-Romano, University of \\nBelgrade, Serbia; Milan Stanković, University of Belgrade, Serbia; and Ur oš Krčadinac, University \\nof Belgrade, Serbia  .......................................................................................................................................... 950\\nIntelligent Traffic Sign Classifiers / Raúl Vicen Bueno, University of Alcalá, Spain; Elena Torijano \\nGordo, University of Alcalá, Spain; Antonio García González, University of Alcalá, Spain; Manuel Rosa Zurera, University of Alcalá, Spain; and Roberto Gil Pita, University of Alcalá, Spain .......... 956\\nInteractive Systems and Sources of Uncertainties / Qiyang Chen, Montclair State University, USA; \\nand John Wang, Montclair State University, USA  ........................................................................................... 963\\nIntuitionistic Fuzzy Image Processing / Ioannis K. Vlachos, Aristotle University of Thessaloniki, Greece; \\nand George D. Sergiadis, Aristotle University of Thessaloniki, Greece  .......................................................... 967\\nKnowledge Management Systems Procedural Development / Javier Andrade, University of \\nA Coruña, Spain; Santiago Rodríguez, University of A Coruña, Spain; María Seoane, University of A Coruña, Spain; and Sonia Suárez, University of A Coruña, Spain .............................................................. 975\\nKnowledge Management Tools and Their Desirable Characteristics / Juan Ares, University of A Coruña, \\nSpain; Rafael García, University of A Coruña, Spain; María Seoane, University of A Coruña, Spain; and Sonia Suárez, University of A Coruña, Spain ........................................................................................... 982\\nKnowledge-Based Systems / Adrian A. Hopgood, De Montfort University, UK ............................................ 989\\nKohonen Maps and TS Algorithms / Marie-Thérèse Boyer-Xambeu, Université de Paris VII – LED, \\nFrance; Ghislain Deleplace, Université de Paris VIII – LED, France; Patrice Gaubert, Université de Paris 12 – ERUDITE, France; Lucien Gillard, CNRS – LED, France; and Madalina Olteanu, Université de Paris I – CES SAMOS, France  .................................................................. 996', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7cebf34e-c30b-427e-9e5d-058bdb864069', embedding=None, metadata={'page_label': '25', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Learning in Feed-Forward Artificial Neural Networks I / Lluís A. Belanche Muñoz, Universitat \\nPolitècnica de Catalunya, Spain  .................................................................................................................... 1004\\nLearning in Feed-Forward Artificial Neural Networks II / Lluís A. Belanche Muñoz, Universitat \\nPolitècnica de Catalunya, Spain  .................................................................................................................... 1012\\nLearning Nash Equilibria in Non-Cooperative Games / Alfredo Garro, University of Calabria, Italy......... 1018\\nLearning-Based Planning / Sergio Jiménez Celorrio, Universidad Carlos III de Madrid, Spain; \\nand Tomás de la Rosa Turbides, Universidad Carlos III de Madrid, Spain .................................................. 1024\\nLongitudinal Analysis of  Labour Market Data with SOM, A / Patrick Rousset, CEREQ, France; \\nand Jean-Francois Giret, CEREQ, France  ................................................................................................... 1029\\nManaging Uncertainties in Interactive Systems / Qiyang Chen, Montclair State University, USA; \\nand John Wang, Montclair State University, USA  ......................................................................................... 1036\\nMany-Objective Evolutionary Optimisation / Francesco di Pierro, University of Exeter, UK; \\nSoon-Thiam Khu, University of Exeter, UK; and Dragan A. Savić, University of Exeter, UK  ...................... 1042\\nMapping Ontologies by Utilising Their Semantic Structure / Yi Zhao, Fernuniversitaet in Hagen, \\nGermany; and Wolfgang A. Halang, Fernuniversitaet in Hagen, Germany  ................................................. 1049\\nMathematical Modeling of Artificial Neural Networks / Radu Mutihac, University of Bucharest, \\nRomania ......................................................................................................................................................... 1056\\nMicroarray Information and Data Integration Using SAMIDI / Juan M. Gómez, Universidad Carlos III \\nde Madrid, Spain; Ricardo Colomo, Universidad Carlos III de Madrid, Spain; Marcos Ruano, Universidad Carlos III de Madrid, Spain; and Ángel García, Universidad Carlos III de Madrid, Spain ....1064\\nMobile Robots Navigation, Mapping, and Localization Part I / Lee Gim Hee, DSO National \\nLaboratories, Singapore; and Marcelo H. Ang Jr., National University of Singapore, Singapore  ............... 1072\\nMobile Robots Navigation, Mapping, and Localization Part II / Lee Gim Hee, DSO National \\nLaboratories, Singapore; and Marcelo H. Ang Jr., National University of Singapore, Singapore  ............... 1080\\nModal Logics for Reasoning about Multiagent Systems / Nikolay V . Shilov, Russian Academy \\nof Science, Institute of Informatics Systems, Russia; and Natalia Garanina, Russian Academy of Science, Institute of Informatics Systems, Russia  ......................................................................................................... 1089\\nModularity in Artificial Neural Networks / Ricardo Téllez, Technical University of Catalonia, \\nSpain; and Cecilio Angulo, Technical University of Catalonia, Spain  .......................................................... 1095\\nMorphological Filtering Principles / Jose Crespo, Universidad Politécnica de Madrid, Spain  ................... 1102\\nMREM, Discrete Recurrent Network for Optimization / Enrique Mérida-Casermeiro, \\nUniversity of Málaga, Spain; Domingo López-Rodríguez, University of Málaga, Spain; and Juan M. Ortiz-de-Lazcano-Lobato, University of Málaga, Spain  ................................................................. 1112', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b202f2d-8819-4e4b-b662-87a42c616878', embedding=None, metadata={'page_label': '26', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Volume III\\nMultilayer Optimization Approach for Fuzzy Systems / Ivan N. Silva, University of São Paulo, Brazil; \\nand Rogerio A. Flauzino, University of São Paulo, Brazil  ............................................................................ 1121\\nMulti-Layered Semantic Data Models / László Kovács, University of Miskolc, Hungary; and \\nTanja Sieber, University of Miskolc, Hungary  ............................................................................................... 1130\\nMultilogistic Regression by Product Units / P .A. Gutiérrez, University of Córdoba, Spain; C. Hervás, \\nUniversity of Córdoba, Spain; F .J. Martínez-Estudillo, INSA – ETEA, Spain; and M. Carbonero, \\nINSA – ETEA, Spain  ...................................................................................................................................... 1136\\nMulti-Objective Evolutionary Algorithms / Sanjoy Das, Kansas State University, USA; and \\nBijaya K. Panigrahi, Indian Institute of Technology, India ........................................................................... 1145\\nMulti-Objective Training of Neural Networks / M. P . Cuéllar, Universidad de Granada, Spain; \\nM. Delgado, Universidad de Granada, Spain; and M. C. Pegalajar, University of Granada, Spain ........... 1152\\n“Narrative” Information and the NKRL Solution / Gian Piero Zarri, LaLIC, University Paris 4-Sorbonne, France  ........................................................................................................................................................... 1159\\n“Narrative” Information Problems / Gian Piero Zarri, LaLIC, University Paris 4-Sorbonne, France  ........ 1167\\nNatural Language Processing and Biological Methods / Gemma Bel Enguix, Rovira i \\nVirgili University, Spain; and M. Dolores Jiménez López, Rovira i Virgili University, Spain  ....................... 1173\\nNatural Language Understanding and Assessment / Vasile Rus, The University of Memphis, USA; Philip McCarthy, University of Memphis, USA; Danielle S. McNamara, The University of Memphis, USA; and Art Graesser, University of Memphis, USA  ................................................................................... 1179\\nNavigation by Image-Based Visual Homing / Matthew Szenher, University of Edinburgh, UK ................... 1185\\nNelder-Mead Evolutionary Hybrid Algorithms / Sanjoy Das, Kansas State University, USA  ...................... 1191\\nNeural Control System for Autonomous Vehicles / Francisco García-Córdova, Polytechnic University \\nof Cartagena (UPCT), Spain; Antonio Guerrero-González, Polytechnic University of Cartagena (UPCT), Spain; and Fulgencio Marín-García, Polytechnic University of Cartagena (UPCT), Spain  ........................ 1197\\nNeural Network-Based Visual Data Mining for Cancer Data / Enrique Romero, Technical University of \\nCatalonia, Spain; Julio J. Valdés,National Research Council Canada, Canada; and Alan J. Barton, National Research Council Canada, Canada ................................................................................................ 1205\\nNeural Network-Based Process Analysis in Sport / Juergen Perl, University of Mainz, Germany  .............. 1212\\nNeural Networks and Equilibria, Synchronization, and Time Lags / Daniela Danciu, University \\nof Craiova, Romania; and Vladimir Răsvan, University of Craiova, Romania  ............................................ 1219', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a37f85df-77d7-4e3c-b768-4fef825a837e', embedding=None, metadata={'page_label': '27', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Networks and HOS for Power Quality Evaluation / Juan J. González De la Rosa, \\nUniversities of Cádiz-Córdoba, Spain; Carlos G. Puntonet, University of Granada, Spain; \\nand A. Moreno-Muñoz, Universities of Cádiz-Córdoba, Spain ..................................................................... 1226\\nNeural Networks on Handwritten Signature Verification / J. Francisco Vargas, University of \\nLas Palmas de Gran Canaria, Spain & Universidad de Antioquia, Colombia; and Miguel A. Ferrer, University of Las Palmas de Gran Canaria, Spain ....................................................................................... 1232\\nNeural/Fuzzy Computing Based on Lattice Theory / Vassilis G. Kaburlasos, Technological Educational Institution of Kavala, Greece .................................................................................................... 1238\\nNew Self-Organizing Map for Dissimilarity Data, A / Tien Ho-Phuoc, GIPSA-lab, France; and Anne Guerin-Dugue, GIPSA-lab, France  ............................................................................................... 1244\\nNLP Techniques in Intelligent Tutoring Systems / Chutima Boonthum, Hampton University, USA; \\nIrwin B. Levinstein, Old Dominion University, USA; Danielle S. McNamara, The University of Memphis, USA; Joseph P . Magliano, Northern Illinois University, USA; and Keith K. Millis, The University of Memphis, USA  ................................................................................................................... 1253\\nNon-Cooperative Facial Biometric Identification Systems / Carlos M. Travieso González, University \\nof Las Palmas de Gran Canaria, Spain; and Aythami Morales Moreno, University of Las Palmas de Gran Canaria, Spain ................................................................................................................................. 1259\\nNonlinear Techniques for Signals Characterization / Jesús Bernardino Alonso Hernández, University \\nof Las Palmas de Gran Canaria, Spain; and Patricia Henríquez Rodríguez, University of Las Palmas de Gran Canaria, Spain ................................................................................................................................. 1266\\nOntologies and Processing Patterns for Microarrays / Mónica Miguélez Rico, University of A Coruña, \\nSpain; José Antonio Seoane Fernández, University of A Coruña, Spain; and Julián Dorado de la Calle, University of A Coruña, Spain ....................................................................................................................... 1273\\nOntologies for Education and Learning Design / Manuel Lama, University of Santiago de Compostela, \\nSpain; and Eduardo Sánchez, University of Santiago de Compostela, Spain  ............................................... 1278\\nOntology Alignment Overview / José Manuel Vázquez Naya, University of A Coruña, Spain; \\nMarcos Martínez Romero, University of A Coruña, Spain; Javier Pereira Loureiro, University of A Coruña, Spain; and Alejandro Pazos Sierra, University of A Coruña, Spain ............................................ 1283\\nOntology Alignment Techniques / Marcos Martínez Romero, University of A Coruña, Spain; \\nJosé Manuel Vázquez Naya, University of A Coruña, Spain; Javier Pereira Loureiro, University of A Coruña, Spain; and Norberto Ezquerra, Georgia Institute of Technology, USA.................. 1290\\nOptimization of the Acoustic Systems / V . Romero-García, Polytechnic University of Valencia, Spain; E. Fuster-Garcia, Polytechnic University of Valencia, Spain; J. V . Sánchez-Pérez, Polytechnic University of Valencia, Spain; L. M. Garcia-Raffi, Polytechnic University of Valencia, Spain; X. Blasco, Polytechnic University of Valencia, Spain; J. M. Herrero, Polytechnic University of Valencia, Spain; and J. Sanchis, Polytechnic University of Valencia, Spain  ................................................. 1296', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ca42fa73-1344-4b28-b8ed-401213f40094', embedding=None, metadata={'page_label': '28', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Particle Swarm Optimization and Image Analysis / Stefano Cagnoni, Università degli Studi di Parma, \\nItaly; and Monica Mordonini, Università degli Studi di Parma, Italy  .......................................................... 1303\\nPersonalized Decision Support Systems / Neal Shambaugh, West Virginia University, USA  ....................... 1310\\nPlanning Agent for Geriatric Residences / Javier Bajo, Universidad Pontificia de Salamanca, Spain; \\nDante I. Tapia, Universidad de Salamanca, Spain; Sara Rodríguez, Universidad de Salamanca, Spain; and Juan M. Corchado, Universidad de Salamanca, Spain  .......................................................................... 1316\\nPrivacy-Preserving Estimation / Mohammad Saad Al-Ahmadi, King Fahd University of Petroleum and \\nMinerals (KFUPM), Saudi Arabia; and Rathindra Sarathy, Oklahoma State University, USA  ................... 1323\\nProtein Structure Prediction by Fusion, Bayesian Methods / Somasheker Akkaladevi, \\nVirginia State University, USA; Ajay K. Katangur, Texas A&M University – Corpus Christi, USA; and Xin Luo, The University of New Mexico, USA  ........................................................................................ 1330\\nPrototype Based Classification in Bioinformatics / Frank-M. Schleif, University of Leipzig, Germany; \\nThomas Villmann, University of Leipzig, Germany; and Barbara Hammer, Technical University of Clausthal, Germany ....................................................................................................................................... 1337\\nRandomized Hough Transform / Lei Xu, Chinese University of Hong Kong, Hong Kong & Peking University, China; and Erkki Oja, Helsinki University of Technology, Finland  ........................................... 1343\\nRanking Functions / Franz Huber, California Institute of Technology, USA................................................. 1351\\nRBF Networks for Power System Topology Verification / Robert Lukomski, Wroclaw University of \\nTechnology, Poland; and Kazimierz Wilkosz, Wroclaw University of Technology, Poland ........................... 1356\\nRepresenting Non-Rigid Objects with Neural Networks / José García-Rodríguez, University of \\nAlicante, Spain; Francisco Flórez-Revuelta, University of Alicante, Spain; and Juan Manuel García-Chamizo, University of Alicante, Spain  ............................................................................................. 1363\\nRoadmap on Updates, A / Fernando Zacarías Flores, Benemérita Universidad Autónoma de Puebla, México; Dionicio Zacarías Flores, Benemérita Universidad Autónoma de Puebla, Mexico; Rosalba Cuapa Canto, Benemérita Universidad Autónoma de Puebla, Mexico; and Luis Miguel Guzmán Muñoz, Benemérita Universidad Autónoma de Puebla, Mexico  ................................. 1370\\nRobot Model of Dynamic Appraisal and Response, A / Carlos Herrera, Intelligent Systems Research Centre University of Ulster, North Ireland; Tom Ziemke, University of Skovde, Sweden; and Thomas M. McGinnity, Intelligent Systems Research Centre University of Ulster, University of Ulster, North Ireland .................................................................................................................................................. 1376\\nRobots in Education / Muhammad Ali Yousuf, Tecnologico de Monterrey – Santa Fe Campus, México  .....1383\\nRobust Learning Algorithm with LTS Error Function / Andrzej Rusiecki, Wroclaw University of Technology, Poland ........................................................................................................................................ 1389\\nRough Set-Based Neuro-Fuzzy System / Kai Keng Ang, Institute for Infocomm Research, Singapore; \\nand Chai Quek, Nanyang Technological University, Singapore .................................................................... 1396', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b640fd9c-c337-44a4-8d69-57846c9a3887', embedding=None, metadata={'page_label': '29', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Rule Engines and Agent-Based Systems / Agostino Poggi, Università di Parma, Italy; and \\nMichele Tomaiuolo, Università di Parma, Italy  ............................................................................................ 1404\\nSequence Processing with Recurrent Neural Networks / Chun-Cheng Peng, University of London, UK; \\nand George D. Magoulas, University of London, UK ................................................................................... 1411\\nShortening Automated Negotiation Threads via Neural Nets / Ioanna Roussaki, National Technical University of Athens, Greece; Ioannis Papaioannou, National Technical University of Athens, Greece; and Miltiades Anagnostou, National Technical University of Athens, Greece .............................................. 1418\\nSigned Formulae as a New Update Process / Fernando Zacarías Flores, Benemérita Universidad \\nAutónoma de Puebla, Mexico; Dionicio Zacarías Flores, Benemérita Universidad Autónoma de Puebla, Mexico; Rosalba Cuapa Canto, Benemérita Universidad Autónoma de Puebla, Mexico; and Luis Miguel Guzmán Muñoz, Benemérita Universidad Autónoma de Puebla, Mexico  ................................. 1426\\nSolar Radiation Forecasting Model / Fatih Onur Hocaoğlu, Anadolu University Eskisehir, Turkey; \\nÖmer Nezih Gerek, Anadolu University Eskisehir, Turkey; and Mehmet Kurban, Anadolu University Eskisehir, Turkey ............................................................................................................................................ 1433\\nSpeech-Based Clinical Diagnostic Systems / Jesús Bernardino Alonso Hernández, University of \\nLas Palmas de Gran Canaria, Spain; and Patricia Henríquez Rodríguez, University of Las Palmas de Gran Canaria, Spain ................................................................................................................................. 1439\\nState of the Art in Writer’s Off-Line Identification / Carlos M. Travieso González, University of \\nLas Palmas de Gran Canaria, Spain; and Carlos F . Romero, University of Las Palmas de Gran Canaria, Spain ............................................................................................................................................... 1447\\nState-of-the-Art on Video-Based Face Recognition / Yan Yan, Tsinghua University, Beijing, China; \\nand Yu-Jin Zhang, Tsinghua University, Beijing, China ................................................................................ 1455\\nStationary Density of Stochastic Search Processes / Arturo Berrones, Universidad Autónoma de Nuevo \\nLeón, México; Dexmont Peña, Universidad Autónoma de Nuevo León, Mexico; and Ricardo Sánchez, Universidad Autónoma de Nuevo León, Mexico  ............................................................... 1462\\nStatistical Modelling of Highly Inflective Languages / Mirjam Sepesy Maučec, University of \\nMaribor, Slovenia; and Zdravko Kačič, University of Maribor, Slovenia  ..................................................... 1467\\nStatistical Simulations on Perceptron-Based Adders / Snorre Aunet, University of Oslo, Norway & Centers for Neural Inspired Nano Architectures, Norway; and Hans Kristian Otnes Berge, University of Oslo, Norway ............................................................................................................................ 1474\\nStochastic Approximation Monte Carlo for MLP Learning / Faming Liang, Texas A&M \\nUniversity, USA  .............................................................................................................................................. 1482\\nStream Processing of a Neural Classifier I / M. Martínez-Zarzuela, University of Valladolid, Spain; \\nF . J. Díaz Pernas, University of Valladolid, Spain; D. González Ortega, University of Valladolid, Spain; J. F . Díez Higuera, University of Valladolid, Spain; and M. Antón Rodríguez, University of Valladolid, Spain .............................................................................................................................................................. 1490', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c6cb1c8-cd99-40e7-87ea-1277be6c2306', embedding=None, metadata={'page_label': '30', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Stream Processing of a Neural Classifier II / M. Martínez-Zarzuela, University of Valladolid, Spain; \\nF . J. Díaz Pernas, University of Valladolid, Spain; D. González Ortega, University of Valladolid, Spain; \\nJ. F . Díez Higuera, University of Valladolid, Spain; and M. Antón Rodríguez, University of Valladolid, Spain .............................................................................................................................................................. 1497\\nStudy of the Performance Effect of Genetic Operators, A / Pi-Sheng Deng, California State University \\nat Stanislaus, USA  .......................................................................................................................................... 1504\\nSupervised Learning of Fuzzy Logic Systems / M. Mohammadian, University of Canberra, Australia  ......1510\\nSupport Vector Machines / Cecilio Angulo, Technical University of Catalonia, Spain; and \\nLuis Gonzalez-Abril, Technical University of Catalonia, Spain  .................................................................... 1518\\nSurvey on Neural Networks in Automated Negotiations, A / Ioannis Papaioannou, National Technical University of Athens, Greece; Ioanna Roussaki, National Technical University of Athens, Greece; and Miltiades Anagnostou, National Technical University of Athens, Greece .............................................. 1524\\nSwarm Intelligence Approach for Ad-Hoc Networks / Prayag Narula, University of Delhi, India; \\nSudip Misra, Yale University, USA; and Sanjay Kumar Dhurandher, University of Delhi, India ................. 1530\\nSwarm Robotics / Amanda J.C. Sharkey, University of Sheffield, UK  .......................................................... 1537\\nSymbol Grounding Problem / Angelo Loula, State University of Feira de Santana, Brazil & State \\nUniversity of Campinas (UNICAMP), Brazil; and João Queiroz, State University of Campinas (UNICAMP), Brazil & Federal University of Bahia, Brazil  ................................................................................................ 1543\\nSymbolic Search / Stefan Edelkamp, University of Dortmund, Germany  ..................................................... 1549\\nSynthetic Neuron Implementations / Snorre Aunet, University of Oslo, Norway & Centers for \\nNeural Inspired Nano Architectures, Norway ................................................................................................ 1555\\nTeaching Machines to Find Names / Raymond Chiong, Swinburne University of Technology, \\nSarawak Campus, Malaysia........................................................................................................................... 1562\\nThermal Design of Gas-Fired Cooktop Burners Through ANN / T.T. Wong, The Hong Kong Polytechnic University, Hong Kong; and C.W. Leung, The Hong Kong Polytechnic University, Hong Kong  ................. 1568\\n2D Positioning Application in PET Using ANNs, A / Fernando Mateo, Universidad Politécnica de Valencia, Spain; Ramón J. Aliaga, Universidad Politécnica de Valencia, Spain; Jorge D. Martínez, Universidad Politécnica de Valencia, Spain; José Mª Monzó, Universidad Politécnica de Valencia, Spain; and Rafael Gadea, Universidad Politécnica de Valencia, Spain  ................................................................... 1576\\n2D-PAGE Analysis Using Evolutionary Computation / Pablo Mesejo, University of A Coruña, Spain; \\nEnrique Fernández-Blanco, University of A Coruña, Spain; Diego Martínez-Feijóo, University of A Coruña, Spain; and Francisco J. Blanco, Juan Canalejo Hospital, Spain  ................................................... 1583\\nVisualizing Cancer Databases Using Hybrid Spaces / Julio J. Valdés, National Research Council Canada, \\nCanada; and Alan J. Barton, National Research Council Canada, Canada  ................................................ 1589', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d53f08b3-f715-47c1-9190-b7a7bb11baf4', embedding=None, metadata={'page_label': '31', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='V oltage Instability Detection Using Neural Networks / Adnan Khashman, Near East University, \\nTurkey; Kadri Buruncuk, Near East University, Turkey; and Samir Jabr, Near East University, \\nTurkey ............................................................................................................................................................. 1596\\nWave Reflection at Submerged Breakwaters / Alberte Castro Ponte, University of Santiago de Compostela, Spain; Gregorio Iglesias Rodriguez, University of Santiago de Compostela, Spain; Francisco Taveira Pinto, University of Santiago de Compostela, Spain; and Rodrigo Carballo Sanchez, University of Santiago de Compostela, Spain  ................................................................................................ 1603\\nWeb-Based Assessment System Applying Many-Valued Logic / Sylvia Encheva, Haugesund \\nUniversity College, Norway; and Sharil Tumin, University of Bergen, Norway ........................................... 1610\\nWorkflow Management Based on Mobile Agent Technology / Marina Flores-Badillo, CINVESTAV \\nUnidad Guadalajara, Mexico; and Ernesto López-Mellado, CINVESTAV Unidad Guadalajara, Mexico ...1615', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='050d493b-b216-495e-9466-56772bad4822', embedding=None, metadata={'page_label': '32', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  xxxi\\nPreface\\nThrough the history the man has always hoped the boost of three main characteristics: physical, metaphysical \\nand intellectual.\\nFrom the physical viewpoint he invented and developed all kind of tools: levers, wheels, cams, pistons, etc., \\nuntil achieving the sophisticated machines existing nowadays.\\nRegarding the metaphysical aspect, the initial celebration of magical-animistic rituals led to attempts, either \\nreal or literary, for creating ex nihilo life: life from inert substance. The most actual approaches involve the cryoconservation of deceased people for them to be returned to life in the future; the generation of life at the laboratories by means of cells, tissues, organs, systems or individuals created from previously frozen stem cells is also currently aimed.  \\nThe third aspect considered, the intellectual one, is the most interesting here. There have been multiple \\ncontributions, since devices that increased the calculi ability as the abacus appeared, until the later theoreti-cal proposals for trying to solve problems, as the Ars Magna by Ramón Lull. The first written reference of the Artificial Intelligence that is known is The Iliad, where Homer describes the visit of the goddess Thetis and her son Achilles to the workshop of Hephaestus, god of smiths:  At once he was helped along by female servants made of gold, who moved to him. They look like living servant girls, possessing minds, hearts with intelligence, vocal chords, and strength.\\nHowever, the first reference of Artificial Intelligence, as it is currently understood, can be found in the proposal \\nmade by J. McCarthy to the Rockefeller Foundation in 1956; this proposal hoped for funds that might support  a month-lasting meeting of twelve researchers of the Dartmouth Summer Research Project in order to establish the basis of the, McCarthy-named, Artificial Intelligence.\\nAlthough the precursors of the Artificial Intelligence (S. Ramón y Cajal, N. Wienner, D. Hebb, C. Shannon and \\nJ. McCulloch, among many others), come from multiple science disciplines, the true driving forces (A. Turing, J. von Neumann, M. Minsky, T. Gödell,…) emerge in the second third of the XX century with the apparition of certain tools, the computers, capable of handling fairly complex problems. Some other scientists, as J. Hopfield or J. Holland, proposed at the last third of the century some biology-inspired approaches that enabled the treat-ment of complex problems of the real world that even might require certain adaptive ability.\\nAll this long and productive trend of the history of the Artificial Intelligence demanded an encyclopaedia that \\nmight give expression to the current situation of this multidisciplinary topic, where researches from multiple fields as neuroscience, computing science, cognitive sciences, exact sciences and different engineering areas converge. \\nThis work intends to provide a wide and well balanced coverage of all the points of interest that currently \\nexist in the field of Artificial Intelligence, from the most theoretical fundamentals to the most recent industrial applications.\\nMultiple researches have been contacted and several notifications have been performed in different forums \\nof the scientific field dealt here.\\nAll the proposals have been carefully revised by the editors for balancing, as far as possible, the contributions, \\nwith the intention of achieving an accurately wide document that might exemplify this field.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f42fde6f-0d9c-4bf4-9292-77f84b8a73bb', embedding=None, metadata={'page_label': '33', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xxxii  \\nA first selection was performed after the reception of all the proposals and it was later sent to three external \\nexpert reviewers in order to carry out a double-blind revision based on a peer review. As a result of this strict \\nand complex process, and before the final acceptance, a high number of contributions (80% approximately) were rejected or required to be modified.\\nThe effort of the last two years is now believed to be worthwhile; at least this is the belief of the editors \\nwho, with the invaluable help of a high number of people mentioned in the acknowledgements, have managed to get this complete encyclopaedia off the ground. The numbers speak for themselves: 233 articles published that have been carried out by 442 authors from 38 different countries and also revised by 238 scientific review -\\ners. The diverse and comprehensive coverage of the disciplines directly related with the Artificial Intelligence is also believed to contribute to a better understanding of all the researching related to this important field of study. It was also intended that the contributions compiled in this work might have a considerable impact on the expansion and the development of the body of knowledge related to this wide field, for it to be an important reference source used by researchers and system developers of this area. It was hoped that the encyclopaedia might be an effective help in order to achieve a better understanding of concepts, problems, trends, challenges and opportunities related to this field of study; it should be useful for the research colleagues, for the teaching personnel, for the students, etc. The editors will be happy to know that this work could inspire the readers for contributing to new advances and discoveries in this fantastic work area that might themselves also contribute to a better life quality of different society aspects: productive processes, health care or any other area where a system or product developed by techniques and procedures of Artificial Intelligence might be used.\\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c04f427f-fbea-426f-9e79-a801254901fa', embedding=None, metadata={'page_label': '34', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  xxxiii\\nAbout the Editors\\nJuan Ramón Rabuñal Dopico is associate professor in the Department of Information and Communications \\nTechnologies, University of A Coruña (Spain). He finished his graduate in computer science in 1996, and in 2002, he became a PhD in computer science with his thesis “Methodology for the Development of Knowledge Extraction Systems in ANNs” and he became a PhD in civil engineering in 2008. He has worked on several Spanish and European projects and has published many books and papers in several international journals. He is currently working in the areas of evolutionary computation, artificial neural networks, and knowledge extrac -\\ntion systems. \\nJulian Dorado is associate professor in the Faculty of Computer Science, University of A Coruña (Spain). He \\nfinished his graduate in computer science in 1994. In 1999, he became a PhD, with a special mention of Euro -\\npean doctor. In 2004, he finished his graduate in biology. He has worked as a teacher of the university for more than 8 years. He has published many books and papers in several journals and international conferences. He is presently working on bioinformatics, evolutionary computing, artificial neural networks, computer graphics, and data mining.\\nAlejandro Pazos is professor in computer science, University of A Coruña (Spain). He was born in Padron in \\n1959. He is MD by Faculty of Medicine, University of Santiago de Compostela in 1987. He obtained a Master of Knowledge Engerineering in 1989 and a PhD in computer science in 1990 from the Polytechnique University of Madrid. He also archives the PhD grade in Medicine in 1996 by the University Complutese of Madrid. He has worked with research groups at Georgia Institute of Technology, Havard Medical School, Stanford University, Politechnique University of Madrid, etc. He funded and is the director of the research laboratory Artificial Neural Networks and Adaptative Systems in Computer science Faculty and is co-director of the Medical Informatics and Radiology Diagnostic Center at the University of A Coruña. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d82d740f-45cf-4f91-b4d7-3f68e0ed6d11', embedding=None, metadata={'page_label': '35', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9983fe8-907d-43ba-89b9-f51bb623c10c', embedding=None, metadata={'page_label': '36', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\nAActive Learning with SVM\\nJun Jiang\\nCity University of Hong Kong, Hong Kong\\nHorace H. S. Ip\\nCity University of Hong Kong, Hong Kong\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nWith the increasing demand of multimedia information retrieval, such as image and video retrieval from the Web, there is a need to find ways to train a classifier when the training dataset is combined with a small number of labelled data and a large number of unlabeled one. Tradi-tional supervised or unsupervised learning methods are not suited to solving such problems particularly when the problem is associated with data in a high-dimen-sion space. In recent years, many methods have been proposed that can be broadly divided into two groups: semi-supervised and active learning  (AL). Support \\nVector Machine (SVM) has been recognized as an ef-ficient tool to deal with high-dimensionality problems, a number of researchers have proposed algorithms of Active Learning with SVM (ALSVM) since the turn of the Century. Considering their rapid development, we review, in this chapter, the state-of-the-art of ALSVM for solving classification problems.\\nBACKGROUND\\nThe general framework of AL can be described as in Figure 1. It can be seen clearly that its name – active \\nlearning – comes from the fact that the learner can improve the classifier by actively choosing the “opti -\\nmal” data from the potential query set Q and adding it \\ninto the current labeled training set L after getting its \\nlabel during the processes. The key point of AL is its sample selection criteria.\\nAL in the past was mainly used together with neu-\\nral network algorithm and other learning algorithms. Statistical AL is one classical method, in which the sample minimizing either the variance (D. A. Cohn, Ghahramani, & Jordan, 1996), bias (D. A. Cohn, 1997) or generalisation error (Roy & McCallum, 2001) is queried to the oracle. Although these methods have strong theoretical foundation, there are two common problems limiting their application: one is how to estimate the posterior distribution of the samples, and the other is its prohibitively high computation cost. To deal with the above two problems, a series of version \\nspace based AL methods, which are based on the assumption that the target function can be perfectly expressed by one hypothesis in the version space and in which the sample that can reduce the volume of the version space is chosen, have been proposed. Examples are query by committee (Freund, Seung, Shamir, & Tishby, 1997), and SG AL (D. Cohn, Atlas, & Ladner, 1994). However the complexity of version space made them intractable until the version space based ALSVMs have emerged.\\nThe success of SVM in the 90s has prompted re-\\nsearchers to combine AL with SVM to deal with the semi-supervised learning problems, such as distance-based (Tong & Koller, 2001), RETIN (Gosselin & Cord, 2004) and Multi-view (Cheng & Wang, 2007) based ALSVMs. In the following sections, we summarize existing well-known ALSVMs under the framework of version space theory , and then briefly describe \\nsome mixed strategies. Lastly, we will discuss the research trends for ALSVM and give conclusions for the chapter.\\nVERSION SPACE BASED ACTIVE LEARNING WITH SVM\\nThe idea of almost all existing heuristic ALSVMs is explicitly or implicitly to find the sample which can reduce the volume of the version space. In this section, \\nwe first introduce their theoretical foundation and then review some typical ALSVMs.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ac56435-ce67-4faf-96b5-4cc470a260d7', embedding=None, metadata={'page_label': '37', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18  Active Learning with SVM\\nVersion Space Theory\\nBased on the Probability Approximation Correct learn-\\ning model, the goal of machine learning is to find a consistent classifier which has the lowest generaliza -\\ntion error bound. The Gibbs generalization error bound (McAllester, 1998) is defined as\\n( )( ) ( )\\uf8f7\\n\\uf8f8\\uf8f6\\uf8ec\\n\\uf8ed\\uf8eb+\\uf8f7\\uf8f7\\n\\uf8f8\\uf8f6\\n\\uf8ec\\uf8ec\\n\\uf8ed\\uf8eb\\n\\uf8f7\\uf8f7\\n\\uf8f8\\uf8f6\\n\\uf8ec\\uf8ec\\n\\uf8ed\\uf8eb=2\\nln1ln1, , ,em\\nz V P mz P m\\nHH Gibbs\\nwhere PH denotes a prior distribution over hypothesis \\nspace H, V(z) denotes the version space of the training \\nset z, m is the number of z and d is a constant in [0, 1]. \\nIt follows that the generalization error bound of the \\nconsistent classifiers is controlled by the volume of the version space if the distribution of the version space is uniform. This provides a theoretical justification for version space based ALSVMs.\\nQuery by Committee with SVM\\nThis algorithm was proposed by (Freund et al., 1997) in which 2k classifiers were randomly sampled and the sample on which these classifiers have maximal disagreement can approximately halve the version \\nspace and then will be queried to the oracle. However, the complexity of the structure of the version space leads to the difficulty of random sampling within it. (Warmuth, Ratsch, Mathieson, Liao, & Lemmem, 2003) successfully applied the algorithm of playing billiard to randomly sample the classifiers in the SVM version space and the experiments showed that its performance was comparable to the performance of standard dis-\\ntance-based ALSVM (SD-ALSVM)  which will be \\nintroduced later. The deficiency is that the processes are time-consuming.\\nStandard Distance Based Active Learning with SVM \\nFor SVM, the version space can be defined as:\\n{ }m i x w y w W w Vi i ,..., 1 , 0 ) ( ( , 1 |= > Φ • = ∈ =\\nwhere (.)Φ denotes the function which map the original \\ninput space X into a high-dimensional space ) (XΦ , and \\nW denotes the parameter space. SVM has two proper-\\nties which lead to its tractability with AL. The first is its duality property that each point w in V corresponds \\nto one hyperplane in \\n) (XΦ  which divides ) (XΦ  into \\ntwo parts and vice versa. The other property is that the solution of SVM w\\n* is the center of the version \\nspace when the version space is symmetric or near to its center when it is asymmetric.\\nBased on the above two properties, (Tong & Koller, \\n2001) inferred a lemma that the sample nearest to the Initialize Step: An classifier h is trained on the initial labeled training set L\\nstep 1: The learner evaluates each data x in potential query set Q (subset of or whole \\nunlabeled data set U) and query the sample x* which has lowest EvalFun (x, L, \\nh, H) to the oracle and get its label y*;\\nstep 2: The learner update the classifier h with the enlarged training set { L + ( x*, \\ny*)};\\nstep 3: Repeat step 1 and 2 until stopping training;\\nWhere \\n\\uf0d8\\tEvalFun (x, L, h, H ): the function of evaluating potential query x (the lowest \\nvalue is the best here)\\n\\uf0d8\\t L: the current labeled training set\\n\\uf0d8\\t H: the hypothesis spaceFigure 1. Framework of active learning', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1ab2ed1-17cc-4cf4-9d78-0ae8afe17661', embedding=None, metadata={'page_label': '38', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18Active Learning with SVM\\nA\\ndecision boundary can make the expected size of the \\nversion space decrease fastest. Thus the sample nearest to the decision boundary will be queried to the oracle (Figure 2). This is the so-called SD-ALSVM which has low additional computations for selecting the queried sample and fine performance in real applications.\\nBatch Running Mode Distance Based Active Learning with SVM\\nWhen utilizing batch query, (Tong & Koller, 2001) simply selected multiple samples which are nearest to the decision boundary. However, adding a batch of such samples cannot ensure the largest reduction of the size of version space, such as an example shown in figure 3. Although every sample can nearly halve the version space, three samples together can still reduce about 1/2, instead of 7/8, of the size of the version space. It can be observed that this was ascribed to the small angles between their induced hyperplanes.\\nTo overcome this problem, (Brinker, 2003) proposed \\na new selection strategy by incorporating diversity \\nmeasure that considers the angles between the induced hyperplanes. Let the labeled set be L and the pool query \\nset be Q in the current round, then based on the diversity \\ncriterion the further added sample x\\nq should be\\n) , ( ) , () , (\\nmax min\\ni i j ji j\\nL x Q xqx x k x x kx x k\\nx\\ni j ∈ ∈=\\n) , ( ) , () , (\\ni i j ji j\\nx x k x x kx x kFigure 2a. The projection of the parameter space around the Version Space\\nFigure 2b. In the induced feature spaceFigure 2. Illustration of standard distance-based ALSVM\\na\\nbcH y perplane i nduc ed by  \\nS upport V ec tor\\nH y perplane i nduc ed by  the c andidate s am ple\\nT he s olution of S V M\\nW*W*\\nV ers ion S pac eT he larges t  ins c ribed hy pers phere\\nmargin +\\n++++++abcS upport V ec tors\\nC andidate U nlabeled \\nS am ples++ 1 C las s-1 C las s', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='87414cad-c8e3-4068-a2ce-3e790e4df320', embedding=None, metadata={'page_label': '39', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18  Active Learning with SVM\\nwhere  denotes the cosine value of the angle between \\ntwo hyperplanes induced by xj and xi, thus it is known \\nas angle diversity criterion. It can be observed that the reduced volume of the version space in figure 4 is larger than that in Figure 3.\\nRETIN Active Learning\\nLet ] ... 1 [ ) (n j j I∈  be the samples in a potential query set \\nQ, and r(i, k) be the function that, at iteration i, codes \\nthe position k in the relevance ranking according to \\nthe distance to the current decision boundary, then a sequence can be obtained as follows:\\uf07b \\uf07b\\nrelevant leastn i r\\ndata queriedm i s i r i s i r i r\\nrelevant mosti r I I I I I) , ( 1 ) ( , ( ) ( , ( ) 2 , ( ) 1 , (,..., ,..., ,..., ,\\uf034 \\uf034 \\uf034 \\uf033\\uf034 \\uf034 \\uf034 \\uf032 \\uf031− +\\nIn SD-ALSVM, s(i) is such as 1 ) ( , ( ) ) ( , ( ,...,− +m i s i r i s i r I I  \\nare the m closest samples to the SVM boundary. This \\nstrategy implicitly relies on a strong assumption: an accurate estimation of SVM boundary. However, the decision boundary is usually unstable at the initial iterations. (Gosselin & Cord, 2004) noticed that, even if the decision boundary may change a lot during the earlier iterations, the ranking function r() is quite stable. \\nThus they proposed a balanced selection criterion that Figure 3. One example of simple batch querying with “a”, “b” and “c” samples with pure SD-ALSVM\\n a bc\\nH y perplane i nduc ed by  \\nS upport V ec tor\\nH y perplane i nduc ed by  \\nthe c andidate s am ple\\nT he s olution of S V M\\nW*W*\\nV ers ion S pac eT he larges t  ins c ribed hy pers phere\\nFigure 4. One example of batch querying with “a”, “b” and “c” samples by incorporating diversity into SD-\\nALSVM\\n a\\nbc\\nH y perplane i nduc ed by  \\nS upport V ec tor\\nH y perplane i nduc ed by  the c andidate s am ple\\nT he s olution of S V M\\nW*W*\\nV ers ion S pac eT he larges t  i ns c ribed \\nhy pers phere', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53d33f74-64de-4cd2-9d39-d2522485ed96', embedding=None, metadata={'page_label': '40', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18Active Learning with SVM\\nAis independent on the frontier and in which an adaptive \\nmethod was designed to tune s during the feedback \\niterations. It was expressed by\\n) ) ( ) , ( ( ) ( ) 1 (i r i r h i s i sirr rel + = +\\nwhere ) ( ) , (y x k y x h− × = which characterizes the \\nsystem dynamics (k is a positive constant), rrel(i) and \\nrirrl(i) denote the number of relevant and irrelevant \\nsamples in the queried set in the ith iteration. This way, \\nthe number of relevant and irrelavant samples in the queried set will be roughly equal.\\nMean Version Space Criterion\\n(He, Li, Zhang, Tong, & Zhang, 2004) proposed a selection criterion by minimizing the mean version \\nspace which is defined as\\n) | 1 ( ) ( ( ) | 1 ( ) ( ( ) (k k k i k k k i k MVS x y P x V Vol x y P x V Vol x C− = + = =− +\\nwhere ) ( (k ix V Vol+ ( ) ) ( (k ix V Vol−) denotes the volume of \\nthe version space after adding an unlabelled sample xk \\ninto the ith round training set. The mean version space \\nincludes both the volume of the version space and the posterior probabilities. Thus they considered that the criterion is better than the SD-ALSVM. However, the computation of this method is time-consuming.\\nMulti-View Based Active Learning\\nDifferent from the algorithms which are based only on one whole feature set, multi-view methods are based \\non multiple sub-feature ones. Several classifiers are first trained on different sub-feature sets. Then the samples on which the classifiers have the largest dis -\\nagreements comprise the contention set from which queried samples are selected. first (I. Muslea, Minton, & Knoblock, 2000) applied in AL and (Cheng & Wang, 2007) implemented it with ALSVM to produce a Co-SVM algorithm which was reported to have better performance than the SD-ALSVM.\\nMultiple classifiers can find the rare samples be -\\ncause they observe the samples with different views. Such property is very useful to find the diverse parts belonging to the same category. However, multi-view based methods demand that the relevant classifier can classify the samples well and that all feature sets are uncorrelated. It is difficult to ensure this condition in real applications. \\nMIXED ACTIVE LEARNING\\nInstead of single AL strategies in the former sections, we will discuss two mixed AL modes in this section: one is combining different selection criteria and another is incorporating semi-supervised learning into AL.\\nHybrid Active Learning\\nContrast to developing a new AL algorithm that works well for all situations, some researchers argued that combining different methods, which are usually complementary, is a better way, for each method has its advantages and disadvantages. The intuitive structure of the hybrid strategy is parallel mode. The key point here is how to set the weights of different AL methods.\\nThe simplest way is to set fixed weights according \\nto experience and it was used by most existing meth-ods. The Most Relevant/Irrelevant (L. Zhang, Lin, & Zhang, 2001) strategies can help to stabilize the decision boundary, but have low learning rates; while standard distance-based methods have high learning rates, but have unstable frontiers at the initial feedbacks. Consid-ering this, (Xu, Xu, Yu, & Tresp, 2003) combined these two strategies to achieve better performance than only using a single strategy. As stated before, the diversity \\nand distance-based strategies are also complementary and (Brinker, 2003), (Ferecatu, Crucianu, & Boujemaa, 2004) and (Dagli, Rajaram, & Huang, 2006) combined angle, inner product and entropy diversity  strategy with \\nstandard distance-based one respectively. \\nHowever, the strategy of the fixed weights can not fit \\nwell into all datasets and all learning iterations. So the weights should be set dynamically. In (Baram, El-Yaniv, & Luz, 2004), all the weights were initialized with the same value, and were modified in the later iterations by using EXP4 algorithm. In this way, the resulting AL algorithm is empirically shown to consistently perform almost as well as and sometimes outperform the best algorithm in the ensemble.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='51af223d-86b5-4301-94e6-f89ae8b8400a', embedding=None, metadata={'page_label': '41', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18  Active Learning with SVM\\nSemi-Supervised Active Learning\\n1. Active Learning with Transductive SVM \\nIn the first stages of SD-ALSVM, a few labeled data \\nmay lead to great deviation of the current solution from the true solution; while if unlabeled samples are considered, the solution may be closer to the true solu-tion. (Wang, Chan, & Zhang, 2003) showed that the closer the current solution is to the true one, the larger the size of the version space will be reduced. They incorporated Transductive SVM (TSVM) to produce more accurate intermediate solutions. However, sev-eral studies (T. Zhang & Oles, 2000) challenged that TSVM might not be so helpful from unlabeled data in theory and in practice. (Hoi & Lyu, 2005) applied the semi-supervised learning techniques based on the Gaussian fields and Harmonic functions instead and the improvements were reported to be significant.\\n2. Incorporating EM into Active Learning(McCallum & Nigam, 1998) combined Expectation \\nMaximization (EM) with the strategy of querying by committee. And (Ion Muslea, Minton, & Knoblock, 2002) integrated Multi-view AL algorithm with EM to get the Co-EMT algorithm which can work well in the situation where the views are incompatible and correlated.\\nFUTURE TRENDS\\nHow to Start the Active Learning\\nAL can be regarded as the problem of searching target \\nfunction in the version space, so a good initial classifier is important. When the objective category is diverse, the initial classifier becomes more important, for bad one may result in converging to a local optimal solu-tion, i.e., some parts of the objective category may not be correctly covered by the final classifier. Two-stage (Cord, Gosselin, & Philipp-Foliguet, 2007), long-term learning (Yin, Bhanu, Chang, & Dong, 2005), and pre-cluster (Engelbrecht & BRITS, 2002) strategies are promising.Feature-Based Active Learning\\nIn AL, the feedback from the oracle can also help to identify the important features, and (Raghavan, Madani, & Jones, 2006) showed that such works can improve the performance of the final classifier significantly. In (Su, Li, & Zhang, 2001), Principal Components Analysis was used to identify important features. To our knowledge, there are few reports addressing the issue.\\nThe Scaling of Active Learning\\nThe scaling of AL to very large database has not been extensively studied yet. However, it is an important issue for many real applications. Some approaches have been proposed on how to index database (Lai, Goh, & Chang, 2004) and how to overcome the concept complexities accompanied with the scalability of the dataset (Panda, Goh, & Chang, 2006).\\nCONCLUSION\\nIn this chapter, we summarize the techniques of ALSVM which have been an area of active research since 2000. We first focus on the descriptions of heuristic ALSVM approaches within the framework of the theory of ver-sion space minimization. Then mixed methods which can complement the deficiencies of single ones are introduced and finally future research trends focus on techniques for selecting the initial labeled training set, feature-based AL and the scaling of AL to very large database. \\nREFERENCES\\nBaram, Y ., El-Yaniv, R., & Luz, K. (2004). Online Choice of Active Learning Algorithms. Journal of \\nMachine Learning Research, 5, 255-291.\\nBrinker, K. (2003). Incorporating Diversity in Ac-\\ntive Learning with Support Vector Machines. Paper \\npresented at the International Conference on Machine Learning.\\nCheng, J., & Wang, K. (2007). Active learning for \\nimage retrieval with Co-SVM. Pattern Recognition, \\n40(1), 330-334.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='edc4a5a8-1b19-4daa-92df-781a66a510cc', embedding=None, metadata={'page_label': '42', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18Active Learning with SVM\\nACohn, D., Atlas, L., & Ladner, R. (1994). Improving \\nGeneralization with Active Learning. Machine Learn-\\ning, 15, 201-221.\\nCohn, D. A. (1997). Minimizing Statistical Bias with \\nQueries. In Advances in Neural Information Processing \\nSystems 9, Also appears as AI Lab Memo 1552, CBCL Paper 124. M. Mozer et al, eds.\\nCohn, D. A., Ghahramani, Z., & Jordan, M. I. (1996). \\nActive Learning with Statistical Models. Journal of \\nArtificial Intelligence Research, 4, 129-145.\\nCord, M., Gosselin, P. H., & Philipp-Foliguet, S. (2007). \\nStochastic exploration and active learning for image retrieval. Image and Vision Computing, 25 (1), 14-23.\\nDagli, C. K., Rajaram, S., & Huang, T. S. (2006). Utiliz-\\ning Information Theoretic Theoretic Diversity  for SVM Active Learning. Paper presented at the International Conference on Pattern Recognition, Hong Kong.\\nEngelbrecht, A. P., & BRITS, R. (2002). Supervised \\nTraining Using an Unsuerpvised Approach to Active Learning. Neural Processing Letters, 15 , 14.\\nFerecatu, M., Crucianu, M., & Boujemaa, N. (2004). Reducing the redundancy in the selection of samples for SVM-based relevance feedback  \\nFreund, Y ., Seung, H. S., Shamir, E., & Tishby, N. \\n(1997). Selective Sampling Using the Query by Com-mittee Algorithm. Machine Learning, 28, 133-168.\\nGosselin, P. H., & Cord, M. (2004). RETIN AL: an \\nactive learning strategy for image category retrieval. Paper presented at the International Conference on Image Processing.\\nHe, J., Li, M., Zhang, H.-J., Tong, H., & Zhang, C. \\n(2004). Mean version space: a new active learning \\nmethod for content-based image retrieval.  Paper \\npresented at the International Multimedia Conference Proceedings of the 6th ACM SIGMM International Workshop on Mulitimedia Information Retrieval.\\nHoi, S. C. H., & Lyu, M. R. (2005). A semi-supervised \\nactive learning framework for image retrieval.  Paper \\npresented at the IEEE Computer Society Conference \\non Computer Vision and Pattern Recognition.\\nLai, W.-C., Goh, K., & Chang, E. Y . (2004, June). On \\nScalability of Active Learning for Formulating Query \\nConcepts (long version of the ICME invited paper). Paper presented at the Workshop on Computer Vision Meets Databases (CVDB) in cooperation with ACM International Conference on Management of Data (SIGMOD), Paris.\\nMcAllester, D. A. (1998). Some P AC Bayesian Theo-\\nrems. Paper presented at the Proceedings of the 11th \\nAnnual Conference on Computational Learning Theory, Madison, Wisconsin.\\nMcCallum, A. K., & Nigam, K. (1998). Employing EM \\nand Pool-Based Active Learning for Text Classification. \\nPaper presented at the Proceedings of 15th International Conference on Machine Learning.\\nMuslea, I., Minton, S., & Knoblock, C. A. (2000). Selec-\\ntive Sampling with Redundant Views. Paper presented \\nat the Proceedings of the 17th National Conference on Artificial Intelligence.\\nMuslea, I., Minton, S., & Knoblock, C. A. (2002). \\nActive+Semi-Supervised Learning = Robust Multi-View Learning.  Paper presented at the Proceedings of the 19th \\nInternational Conference on Machine Learning.\\nPanda, N., Goh, K., & Chang, E. Y . (2006). Active \\nLearning in Very Large Image Databases Journal of \\nMultimedia Tools and Applications Special Issue on Computer Vision Meets Databases .\\nRaghavan, H., Madani, O., & Jones, R. (2006). Ac-tive Learning with Feedback on Both Features and Instances. Journal of Machine Learning Research, 7, \\n1655-1686.\\nRoy, N., & McCallum, A. (2001). Toward Optimal Ac-\\ntive Learning Through Sampling Estimation of Error \\nReduction.  Paper presented at the Proceedings of 18th \\nInternational Conference on Machine Learning.\\nSu, Z., Li, S., & Zhang, H. (2001). Extraction of Fea-\\nture Subspaces for Content-based Retrieval Using \\nRelevance Feedback.  Paper presented at the ACM \\nMultimedia, Ottawa, Ontario, Canada.\\nTong, S., & Koller, D. (2001). Support Vector Machine \\nActive Learning with Application to Text Classification. Journal of Machine Learning Research, 45-66.\\nWang, L., Chan, K. L., & Zhang, Z. (2003). Bootstrap-\\nping SVM active learning by incorporating unlabelled \\nimages for image retrieval.  Paper presented at the \\nProceeding of IEEE Computer Vision and Pattern Recognition.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='217a837e-d6cb-4f6b-8cac-877cbf3345df', embedding=None, metadata={'page_label': '43', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18  Active Learning with SVM\\nWarmuth, M. K., Ratsch, G., Mathieson, M., Liao, J., \\n& Lemmem, C. (2003). Active Learning in the Drug Discovery Process. Journal of Chemical Information \\nSciences, 43 (2), 667-673.\\nXu, Z., Xu, X., Yu, K., & Tresp, V . (2003). A Hybrid \\nRelevance-feedback Approach to Text Retrieval.  Paper \\npresented at the Proceedings of the 25th European Conference on Information Retrieval Research, Lecture Notes in Computer Science.\\nYin, P., Bhanu, B., Chang, K., & Dong, A. (2005). \\nIntegrating Relevance Feedback Techniques for Image Retrieval Using Reinforcement Learning IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence, 27(10), 1536-1551.\\nZhang, L., Lin, F., & Zhang, B. (2001). Support Vector \\nMachine Learning for Image Retrieval.  Paper presented \\nat the International Conference on Image Processing.Zhang, T., & Oles, F. (2000). A Probability Analysis \\non The Value of Unlabeled Data for Classification \\nProblems. Paper presented at the Proceeding of 17th International Conference of Machine Learning, San Francisco, CA.KEy TERMS\\nHeuristic Active Learning: The set of active \\nlearning algorithms in which the sample selection criteria is based on some heuristic objective function. For example, version space based active learning is to select the sample which can reduce the size of the version space.\\nHypothesis Space: The set of all hypotheses \\nin which the objective hypothesis is assumed to be found.\\nSemi-Supervised Learning: The set of learning \\nalgorithms in which both labelled and unlabelled data in the training dataset are directly used to train the classifier.\\nStatistical Active Learning: The set of active \\nlearning algorithms in which the sample selection criteria is based on some statistical objective function, such as minimization of generalisation error, bias and variance. Statistical active learning is usually statisti-cally optimal.\\nSupervised Learning: The set of learning algo-\\nrithms in which the samples in the training dataset are all labelled.\\nUnsupervised Learning: The set of learning al-\\ngorithms in which the samples in training dataset are all unlabelled.\\nVersion Space: The subset of the hypothesis space \\nwhich is consistent with the training set.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e030ea9d-f1d8-4e1c-9e6d-3d537b9bd384', embedding=None, metadata={'page_label': '44', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\nAAdaptive Algorithms for Intelligent Geometric \\nComputing\\nM. L. Gavrilova\\nUniversity of Calgary, Canada\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nThis chapter spans topics from such important areas as Artificial Intelligence, Computational Geometry and Biometric Technologies. The primary focus is on the proposed Adaptive Computation Paradigm and \\nits applications to surface modeling and biometric processing.\\nAvailability of much more affordable storage and \\nhigh resolution image capturing devices have contrib-uted significantly over the past few years to accumulat -\\ning very large datasets of collected data (such as GIS maps, biometric samples, videos etc.). On the other hand, it also created significant challenges driven by the higher than ever volumes and the complexity of the data, that can no longer be resolved through acquisition of more memory, faster processors or optimization of existing algorithms. These developments justified the need for radically new concepts for massive data stor-age, processing and visualization. To address this need, the current chapter presents the original methodology based on the paradigm of the Adaptive Geometric \\nComputing . The methodology enables storing complex \\ndata in a compact form, providing efficient access to it, preserving high level of details and visualizing dynamic changes in a smooth and continuous manner. \\nThe first part of the chapter discusses adaptive al -\\ngorithms in real-time visualization , specifically in GIS \\n(Geographic Information Systems) applications. Data \\nstructures such as Real-time Optimally Adaptive Mesh (ROAM) and Progressive Mesh (PM) are briefly sur -\\nveyed. The adaptive method Adaptive Spatial Memory \\n(ASM), developed by R. Apu and M. Gavrilova, is then introduced. This method allows fast and efficient visualization of complex data sets representing terrains, landscapes and Digital Elevation Models (DEM). Its advantages are briefly discussed. \\nThe second part of the chapter presents application \\nof adaptive computation paradigm and evolutionary computing to missile simulation. As a result, patterns of complex behavior can be developed and analyzed. The final part of the chapter marries a concept of adaptive computation and topology-based techniques \\nand discusses their application to challenging area of \\nbiometric computing . \\nBACKGROUND\\nFor a long time, researchers were pressed with questions on how to model real-world objects (such as terrain, facial structure or particle system) realistically, while at the same time preserving rendering efficiency and space. As a solution, grid, mesh, TIN, Delaunay triangulation-based and other methods for model representation were developed over the last two decades. Most of these are static methods, not suitable for rendering dynamic scenes or preserving higher level of details.\\nIn 1997, first methods for dynamic model represen -\\ntation: Real-time Optimally Adapting Mesh (ROAM) (Duchaineauy et. al., 1997, Lindstrom and Koller, 1996) and Progressive Mesh (PM)  (Hoppe, 1997) were \\ndeveloped. Various methods have been proposed to reduce a fine mesh into an optimized representation so that the optimized mesh contains less primitives and yields maximum detail. However, this approach had two major limitations. Firstly, the cost of optimization is very expensive (several minutes to optimize one medium sized mesh). Secondly, the generated non-uniform mesh is still static. As a result, it yields poor quality when only a small part of the mesh is being observed. Thus, even with the further improvements, these methods were not capable of dealing with large amount of complex data or significantly varied level of details. They have soon were replaced by a different computational model for rendering geometric meshes (Li Sheng et. al. 2003, Shafae and Pajarola, 2003). The model employs a continuous refinement criteria based on an error metric to optimally adapt to a more accurate representation. Therefore, given a mesh representation and a small change in the viewpoint, the optimized mesh ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24249b5d-afbc-4eb6-a410-48f1cdcfc428', embedding=None, metadata={'page_label': '45', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  Adaptive Algorithms for Intelligent Geometric Computing\\nfor the next viewpoint can be computed by refining the \\nexisting mesh. \\nADAPTIVE GEOMETRIC COMPUTING\\nThis chapter presents Adaptive Multi-Resolution \\nTechnique for real-time terrain visualization utiliz-ing a clever way of optimizing mesh dynamically for smooth and continuous visualization with a very high efficiency (frame rate) (Apu and Gavrilova (2005) (2007)). Our method is characterized by the efficient representation of massive underlying terrain, utilizes efficient transition between detail levels, and achieves frame rate constancy ensuring visual continuity. At the core of the method is adaptive processing: a formalized \\nhierarchical representation that exploits the subsequent \\nrefinement principal. This allows us a full control over the complexity of the feature space. An error metric is assigned by a higher level process where objects (or fea-tures) are initially classified into different labels. Thus, this adaptive method is highly useful for feature space representation. In 2006, Gavrilova and Apu showed that such methods can act as a powerful tool not only for terrain rendering, but also for motion planning and adaptive simulations (Apu and Gavrilova, 2006). They introduced Adaptive Spatial Memory (ASM) model \\nthat utilizes adaptive approach for real-time online algorithm for multi-agent collaborative motion plan-ning. They have demonstrate that the powerful notion of adaptive computation can be applied to perception and understanding of space. Extension of this method for 3D motion planning as part of collaborative research with Prof. I. Kolingerova group has been reported to be significantly more efficient than conventional methods (Broz et.al., 2007). \\nWe first move to discuss evolutionary computing . \\nWe demonstrate the power of adaptive computation by developing and applying adaptive computational model to missile simulation (Apu and Gavrilova, 2006). The \\ndeveloped adaptive algorithms described above have a property that spatial memory units can form, refine and collapse to simulate learning, adapting and responding to stimuli. The result is a complex multi-agent learning algorithm that clearly demonstrates organic behaviors such as sense of territory, trails, tracks etc. observed in flocks/herds of wild animals and insects. This gives a motivation to explore the mechanism in application to swarm behavior modeling. \\nSwarm Intelligence (SI) is the property of a system \\nwhereby the collective behaviors of unsophisticated agents interacting locally with their environment cause coherent functional global patterns to emerge (Bo-nabeau, 1999). Swarm intelligence provides a basis for exploration of a collective (distributed) behavior of a group of agents without centralized control or the provision of a global model. Agents in such system have limited perception (or intelligence) and cannot individually carry out the complex tasks. According to Bonebeau, by regulating the behavior of the agents in the swarm, one can demonstrate emergent behavior and intelligence as a collective phenomenon. Although the swarming phenomenon is largely observed in biological organisms such as an ant colony or a flock of birds, it is recently being used to simulate complex dynamic systems focused towards accomplishing a well-defined objective (Kennedy, 2001, Raupp ans Thalmann, 2001). \\nFigure 1. Split and merge operations in ASM model\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce437584-f5d4-47f3-b7a7-26abbf92ea9e', embedding=None, metadata={'page_label': '46', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Algorithms for Intelligent Geometric Computing\\nALet us now investigate application of the adaptive \\ncomputational paradigm and swarm intelligence con-\\ncept to missile behavior simulation (Apu and Gavrilova, 2006). First of all, let us note that complex strategic behavior can be observed by means of a task oriented artificial evolutionary process in which behaviors of individual missiles are described in surprising simplic-ity. Secondly, the global effectiveness and behavior of the missile swarm is relatively unaffected by disruption or destruction of individual units. From a strategic point of view, this adaptive behavior is a strongly desired property in military applications, which motivates our interest in applying it to missile simulation. Note that this problem was chosen as it presents a complex challenge for which an optimum solution is very hard to obtain using traditional methods. The dynamic and competitive relationship between missiles and turrets makes it extremely difficult to model using a determin-istic approach. It should also be noted that the problem has an easy evaluation metric that allows determining fitness values precisely. \\nNow, let us summarize the idea of evolutionary \\noptimization by applying genetic algorithm to evolve the missile genotype. We are particularly interested in observing the evolution of complex 3D formations and tactical strategies that the swarm learns to maximize their effectiveness during an attack simulation run. The simulation is based on attack, evasion and defense. While the missile sets strategy to strike the target, the battle ship prepares to shoot down as many missiles as possible (Figure 2 illustrates the basic missile ma-neuvers). Each attempt to destroy the target is called an attack simulation run. Its effectiveness equals to the number of missiles hitting the target. Therefore the outcome of the simulation is easily quantifiable. On the other hand, the interaction between missiles and the battleship is complex and nontrivial. As a result, war strategies may emerge in which a local penalty (i.e. sacrificing a missile) can optimize global efficiency (i.e. deception strategy). The simplest form of information known to each missile is its position and orientation and the location of the target. This information is aug-mented with information about missile neighborhood and environment, which influences missile navigation pattern. For actual missile behavior simulation, we use strategy based on the modified version of Boids flocking technique. \\nWe have just outlined the necessary set of actions \\nto reach the target or interact with the environment. This is the basic building block of missile navigation. The gene string is another important part that reflects the complexity with which such courses of action could be chosen. It contains a unique combination of maneuvers (such as attack, evasion, etc.) that evolve to create complex combined intelligence. We describe the fitness of the missile gene in terms of collective performance. After investigating various possibilities, we developed and used a two dimensional adaptive fitness function to evolve the missile strains in one evolutionary system. Details on this approach can be found in (Apu and Gavrilova, 2006). \\nAfter extensive experimentation, we have found \\nmany interesting characteristics, such as geometric at-tack formation and organic behaviors observed among swarms in addition to the highly anticipated strategies such as simultaneous attack, deception, retreat and other strategies (see Figure 3). We also examined the adaptability by randomizing the simulation coordinates, distance, initial formation, attack rate, and other param-eters of missiles and measured the mean and variance of the fitness function. Results have shown that many of the genotypes that evolved are highly adaptive to \\nthe environment. \\nWe have just reviewed the application of the adap-\\ntive computational paradigm to swarm intelligence and briefly described the efficient tactical swarm simulation method (Apu and Gavrilova 2006). The results clearly demonstrate that the swarm is able to develop complex strategy through the evolutionary process of genotype mutation. This contribution among other works on \\nFigure 2. Basic maneuvers for a missile using the Gene String', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dde14fce-1217-4ab4-b776-bf2efed2c969', embedding=None, metadata={'page_label': '47', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Algorithms for Intelligent Geometric Computing\\nadaptive computational intelligence will be profiled in \\ndetail in the upcoming book as part of Springer-Verlag book series on Computational Intelligence (Gavrilova, 2007).\\nAs stated in the introduction, adaptive computation \\nis based on a variable complexity level of detail para-digm, where a physical phenomenon can be simulated by the continuous process of local adaptation of spatial complexity. As presented by M. Gavrilova in Plenary Lecture at 3IA Eurographics Conference, France in 2006, the adaptive paradigm is a powerful compu-\\ntational model that can also be applied to vast area of biometric research. This section therefore reviews methods and techniques based on adaptive geomet-ric methods in application to biometric problems. It emphasizes advantages that intelligent approach to geometric computing brings to the area of complex biometric data processing  (Gavrilova 2007). \\nIn information technology, biometric s refers to a \\nstudy of physical and behavioral characteristics with the purpose of person identification (Yanushkevich, Gavrilova, Wang and Srihari, 2007). In recent years, the area of biometrics has witnessed a tremendous growth, partly as a result of a pressing need for increased secu-rity, and partly as a response to the new technological advances that are literally changing the way we live. Availability of much more affordable storage and the high resolution image biometric capturing devices have contributed to accumulating very large datasets of biometric data. In the earlier sections, we have studied the background of the adaptive mesh generation. Let us now look at the background research in topology-based data structures, and its application to biometric research. This information is highly relevant to goals of modeling and visualizing complex biometric data. At the same time as adaptive methodology was developing in GIS, interest to topology-based data structures, such \\nas Voronoi diagrams and Delaunay triangulations, \\nhas grown significantly. Some preliminary results on utilization of these topology-based data structures in \\nbiometric began to appear. For instance, research on image processing using V oronoi diagrams was presented in (Liang and Asano, 2004, Asano, 2006), studies of utilizing V oronoi diagram for fingerprint synthesis were conducted by (Bebis et. al., 1999, Capelli et. al. 2002), and various surveys of methods for modeling of human faces using triangular mesh appeared in (Wen and Huang, 2004, Li and Jain, 2005, Wayman et. al. 2005). Some interesting results were recently obtained in the BTLab, University of Calgary, through the development of topology-based feature extrac-tion algorithms for fingerprint matching (Wang et. al. 2006, 2007, illustration is found in Figure 4), 3D facial expression modeling (Luo et. al. 2006) and iris syn-thesis (Wecker et. al. 2005). A comprehensive review of topology-based approaches in biometric modeling and synthesis can be found in recent book chapter on the subject (Gavrilova, 2007). \\nIn this chapter, we propose to manage the challenges \\narising from large volumes of complex biometric data through the innovative utilization of the adaptive para-digm. We suggest combination of topology-based and hierarchy based methodology to store and search for biometric data, as well as to optimize such representation based on the data access and usage. Namely, retrieval of the data, or creating real-time visualization can be based on the dynamic patter of data usage (how often, what type of data, how much details, etc.), recorded and analyzed in the process of the biometric system being used for recognition and identification purposes. \\n \\n (a) Deception pattern (b) Distraction pattern (c) Organic motion patternFigure 3. Complex formation and attack patterns evolved', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9342d7c9-cf48-4d1c-8bc1-b03cb4d35ed3', embedding=None, metadata={'page_label': '48', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Algorithms for Intelligent Geometric Computing\\nA\\nIn addition to using this information for optimized \\ndata representation and retrieval, we also propose to incorporate intelligent learning techniques to predict most likely patters of the system usage and to represent and organize data accordingly. \\nOn a practical side, to achieve our goal, we propose a \\nnovel way to represent complex biometric data through the organization of the data in a hierarchical tree-like structure. Such organization is similar in principle to the Adaptive Memory Subdivision (AMS), capable of \\nrepresenting and retrieving varies amount of informa-tion and level of detail that needs to be represented. Spatial quad-tree is used to hold the information about the system, as well as the instructions on how to process this information. Expansion is realized through the spatial subdivision technique that refines the data and increases level of details, and the collapsing is real-ized through the merge operation that simplifies the data representation and makes it more compact. The greedy strategy is used to optimally adapt to the best \\nrepresentation based on the user requirements, amount of available data and resources, required resolution and so on. This powerful technique enables us to achieve the goal of compact biometric data representation, that allows for instance to efficiently store minor details of the modeled face (e.g. scars, wrinkles) or detailed patterns of the iris.\\nFUTURE TRENDS\\nIn addition to data representation, adaptive technique can be highly useful in biometric feature extraction with the purpose of fast and reliable retrieval and matching of the biometric data, and in implementing dynamic changes to the model. The methodology has a high potential of becoming one of the key approaches in biometric data modeling and synthesis.\\nCONCLUSION\\nThe chapter reviewed the adaptive computational paradigm in application to surface modeling, evolu-tionary computing and biometric research. Some of the key future developments in the upcoming years will undoubtedly highlight the area, inspiring new genera-tions of intelligent biometric systems with adaptive behavior. \\nREFERENCES\\nApu R. & Gavrilova M (2005) Geo-Mass: Modeling Massive Terrain in Real-Time, GEOMATICA J . 59(3), \\n313-322.\\nApu R. & Gavrilova M. (2006) Battle Swarm: An Evo-\\nlutionary Approach to Complex Swarm Intelligence, 3IA Int. C. Comp. Graphics and AI, Limoges, France, \\n139-150.\\nApu, R & Gavrilova, M. (2007) Fast and Efficient \\nRendering System for Real-Time Terrain Visualization,  \\nIJCSE Journal, 2(2), 5/6. \\nApu, R. & Gavrilova, M. (2006) An Efficient Swarm \\nNeighborhood Management for a 3D Tactical Simulator, IEEE-CS proceedings, ISVD 2006 , 85- 93Figure 4. Delaunay triangulation based technique for fingerprint matching\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4e842839-0246-415c-81b5-5dba1b7db64d', embedding=None, metadata={'page_label': '49', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Algorithms for Intelligent Geometric Computing\\nAsano, T. (2006) Aspect-Ratio V oronoi Diagram with \\nApplications, ISVD 2006, IEEE-CS proceedings, 32-\\n39 \\nBebis G., Deaconu T & Georiopoulous, M. (1999) \\nFingerprint Identification using Delaunay Triangula-tion, ICIIS 99, Maryland, 452-459\\nBonabeau, E., Dorigo, M. & Theraulaz, G. (1999) \\nSwarm Intelligence: From Natural to Artificial Systems, NY: Oxford Univ. Press\\nBroz, P., Kolingerova, I, Zitka, P., Apu R. & Gavrilova \\nM. (2007) Path planning in dynamic environment using an adaptive mesh, SCCG 2007, Spring Conference on Computer Graphics 2007, ACM SIGGRAPH\\nCapelli R, Maio, D, Maltoni D. (2002) Synthetic Fin-gerprint-Database Generation, ICPR 2002, Canada, \\nvol 3, 369-376\\nDuchaineauy, M. et. al. (1997) ROAMing Terrain: \\nReal-Time Optimally Adapting Meshes, IEEE Visu-\\nalization ’97 , 81-88\\nGavrilova M.L. (2007) Computational Geometry and Image Processing in Biometrics: on the Path to Conver-gence, in Book Image Pattern Recognition: Synthesis and Analysis in Biometrics, Book Chapter 4, 103-133, World Scientific Publishers\\nGavrilova M.L. Computational Intelligence: A Geom-\\netry-Based Approach, in book series Studies in Com-putational Intelligence, Springer-Verlag, Ed. Janusz \\nKacprzyk, to appear.\\nGavrilova, M.L. (2006) IEEE_CS Book of the 3\\nrd \\nInternational Symposium on V oronoi Diagrams in \\nScience and Engineering, IEEE-CS, Softcover, 2006, \\n270 pages.\\nGavrilova, M.L. (2006) Geometric Algorithms in 3D \\nReal-Time Rendering and Facial Expression Modeling, 3IA’2006 Plenary Lecture, Eurographics, Limoges, France, 5-18\\nHoppe, H. (1997) View-Dependent Refinement of \\nProgressive Meshes, SIGGRAPH ’97 Proceedings, \\n189-198\\nKennedy, J., Eberhart, R. C., & Shi, Y . (2001) Swarm \\nIntelligence , San Francisco: Morgan Kaufmann Pub-\\nlishersLi Sheng, Liu Xuehui & Wu Enhau, (2003) Feature-Based Visibility-Driven CLOD for Terrain, In Proc. \\nPacific Graphics 2003, 313-322, IEEE Press\\nLi, S. & Jain, A. (2005) Handbook of Face Recogni-\\ntion. Springer-Verlag \\nLiang X.F. & Asano T. (2004) A fast denoising method \\nfor binary fingerprint image, IASTED, Spain, 309-\\n313\\nLindstrom, P. & Koller, D. (1996) Real-time continuous \\nlevel of detail rendering of height fields , SIGGRAPH \\n1996 Proceedings , 109-118\\nLuo, Y , Gavrilova, M. & Sousa M.C. (2006) NPAR by Example: line drawing facial animation from photo-graphs, CGIV’06, IEEE, Computer Graphics, Imaging \\nand Visualization , 514-521\\nRaupp S. & Thalmann D. (2001) Hierarchical Model for Real Time Simulation of Virtual Human Crowds, IEEE Trans. on Visualization and Computer Graphics  \\n7(2), 152-164\\nShafae, M. & Pajarola, R. (2003) Dstrips: Dynamic \\nTriangle Strips for Real-Time Mesh Simplification and Rendering, Pacific Graphics 2003, 271-280\\nWang, C, Luo, Y , Gavrilova M & Rokne J. (2007) Fingerprint Image Matching Using a Hierarchical Approach, in Book Computational Intelligence in Information Assurance and Security, Springer SCI Series, 175-198\\nWang, H, Gavrilova, M, Luo Y . & J. Rokne (2006) An \\nEfficient Algorithm for Fingerprint Matching, ICPR \\n2006, Int. C. on Pattern Recognition , Hong Kong, \\nIEEE-CS, 1034-1037\\nWayman J, Jain A, Maltoni D & Maio D. (2005) Bio-\\nmetric Systems: Technology, Design and Performance Evaluation, Book, Springer\\nWecker  L, Samavati, F & Gavrilova M (2005) Iris \\nSynthesis: A Multi-Resolution Approach, GRAPHITE \\n2005, ACM Press. 121-125Wen, Z. & Huang, T. (2004) 3D Face Processing: \\nModeling, Analysis and Synthesis, Kluwer\\nYanushkevich, S, Gavrilova M., Wang, P & Srihari \\nS. (2007) Image Pattern Recognition: Synthesis and Analysis in Biometrics, Book World Scientific ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b8ec9d6-0d90-49a0-bca2-2c153dc89d8d', embedding=None, metadata={'page_label': '50', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Algorithms for Intelligent Geometric Computing\\nAKEy TERMS\\nAdaptive Geometric Model (AGM): A new ap-\\nproach to geometric computing utilizing adaptive com-\\nputation paradigm. The model employs a continuous refinement criteria based on an error metric to optimally adapt to a more accurate representation.\\nAdaptive Multi-Resolution Technique (AMRT):  \\nFor real-time terrain visualization is a method that utilizes a clever way of optimizing mesh dynamically for smooth and continuous visualization with a high efficiency.\\nAdaptive Spatial Memory (ASM): A hybrid \\nmethod based on the combination of traditional hier-archical tree structure with the concept of expanding or collapsing tree nodes.\\nBiometric Technology (BT): An area of study of \\nphysical and behavioral characteristics with the purpose of person authentication and identification.Delaunay Triangulation (DT): A computational \\ngeometry data structure dual to V oronoi diagram.\\nEvolutionary Paradigm (EP):  The collective \\nname for a number of problem solving methods utiliz-ing principles of biological evolution, such as natural selection and genetic inheritance.\\nSwarm Intelligence (SI): The property of a system \\nwhereby the collective behaviors of unsophisticated agents interacting locally with their environment cause coherent functional global patterns to emerge.\\nTopology-Based Techniques (TBT): A group of \\nmethods using geometric properties of a set of objects in the space and their proximity \\nVoronoi Diagram (VD): A fundamental computa-\\ntional geometry data structure that stores topological information for a set of objects.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e911ea7-0078-41a5-b7ff-a7ada7ddb0b7', embedding=None, metadata={'page_label': '51', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAdaptive Business Intelligence\\nZbigniew Michalewicz\\nThe University of Adelaide, Australia\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nSince the computer age dawned on mankind, one of the most important areas in information technology has been that of “decision support.” Today, this area is more important than ever. Working in dynamic and ever-changing environments, modern-day managers are responsible for an assortment of far reaching de-cisions: Should the company increase or decrease its \\nworkforce? Enter new markets? Develop new products? Invest in research and development? The list goes on. But despite the inherent complexity of these issues and the ever-increasing load of information that business managers must deal with, all these decisions boil down to two fundamental questions: \\nWhat is likely to happen in the future?What is the best decision right now?\\nWhether we realize it or not, these two questions \\npervade our everyday lives — both on a personal and professional level. When driving to work, for instance, we have to make a traffic prediction before we can choose the quickest driving route. At work, we need to predict the demand for our product before we can decide how much to produce. And before investing in a foreign market, we need to predict future exchange rates and economic variables. It seems that regardless of the decision being made or its complexity, we first need to make a prediction of what is likely to happen in the future, and then make the best decision based on that prediction. This fundamental process underpins the basic premise of Adaptive Business Intelligence .\\nBACKGROUND\\nSimply put, Adaptive Business Intelligence is the discipline of combining prediction, optimization, and adaptability into a system capable of answering these two fundamental questions: What is likely to happen \\nin the future? and What is the best decision right now?  ••(Michalewicz et al. 2007). To build such a system, we first need to understand the methods and techniques that enable prediction, optimization, and adaptability (Dhar and Stein, 1997). At first blush, this subject matter is nothing new, as hundreds of books and articles have already been written on business intelligence (Vitt et al., 2002; Loshin, 2003), data mining and prediction methods (Weiss and Indurkhya, 1998; Witten and Frank, 2005), forecasting methods (Makridakis et al., 1988), optimization techniques (Deb 2001; Coello et al. 2002; Michalewicz and Fogel, 2004), and so forth. However, none of these has explained how to combine these various technologies into a software system that is capable of predicting, optimizing, and adapting. Adap-\\ntive Business Intelligence  addresses this very issue.\\nClearly, the future of the business intelligence in-\\ndustry lies in systems that can make decisions, rather than tools that produce detailed reports (Loshin 2003). As most business managers now realize, there is a world of difference between having good knowledge and detailed reports, and making smart decisions. Michael Kahn, a technology reporter for Reuters in San Francisco, makes a valid point in the January 16, 2006 story entitled “Business intelligence software looks to future”:\\n“But analysts say applications that actually answer \\nquestions rather than just present mounds of data is the key driver of a market set to grow 10 per cent in 2006 or about twice the rate of the business software industry in general. \\n‘Increasingly you are seeing applications being de-\\nveloped that will result in some sort of action,’ said Brendan Barnacle, an analyst at Pacific Crest Equi -\\nties. ‘It is a relatively small part now, but it is clearly where the future is. That is the next stage of business intelligence.’”', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9389369b-3a45-4d34-a109-afc18d6b94ba', embedding=None, metadata={'page_label': '52', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Business Intelligence\\nAMAIN FOCUS OF THE CHAPTER\\n“The answer to my problem is hidden in my data … but \\nI cannot dig it up!” This popular statement has been \\naround for years as business managers gathered and stored massive amounts of data in the belief that they contain some valuable insight. But business manag-ers eventually discovered that raw data are rarely of any benefit, and that their real value depends on an organization’s ability to analyze them. Hence, the need emerged for software systems capable of retrieving, summarizing, and interpreting data for end-users (Moss and Atre, 2003). \\nThis need fueled the emergence of hundreds of \\nbusiness intelligence  companies that specialized in \\nproviding software systems and services for extract-ing knowledge  from raw data. These software systems \\nwould analyze a company’s operational data and provide knowledge in the form of tables, graphs, pies, charts, and other statistics. For example, a business intelligence report may state that 57% of customers are between the ages of 40 and 50, or that product X sells much better in Florida than in Georgia.\\n1\\nConsequently, the general goal of most business \\nintelligence systems was to: (1) access data from a variety of different sources; (2) transform these data into information, and then into knowledge; and (3) provide an easy-to-use graphical interface to display this knowledge. In other words, a business intelligence system was responsible for collecting and digesting data, and presenting knowledge in a friendly way (thus en-hancing the end-user’s ability to make good decisions). The diagram in Figure 1 illustrates the processes that underpin a traditional business intelligence system.\\nAlthough different texts have illustrated the relation-\\nship between data and knowledge in different ways (e.g., Davenport and Prusak, 2006; Prusak, 1997; Shortliffe and Cimino, 2006), the commonly accepted distinction between data, information, and knowledge is: \\nData are collected on a daily basis in the form of bits, numbers, symbols, and “objects.” Information is “organized data,” which are pre-\\nprocessed, cleaned, arranged into structures, and stripped of redundancy. Knowledge  is “integrated information,” which \\nincludes facts and relationships that have been perceived, discovered, or learned.\\nBecause knowledge is such an essential component \\nof any decision-making process (as the old saying goes, “Knowledge is power!”), many businesses have \\nviewed knowledge as the final objective. But it seems that knowledge is no longer enough. A business may “know” a lot about its customers — it may have hun -\\ndreds of charts and graphs that organize its customers by age, preferences, geographical location, and sales history — but management may still be unsure of what decision to make! And here lies the difference between “decision support” and “decision making”: all the knowledge in the world will not guarantee the right or best decision.\\nMoreover, recent research in psychology indicates \\nthat widely held beliefs can actually hamper the deci-sion-making process. For example, common beliefs like “the more knowledge we have, the better our decisions will be,” or “we can distinguish between useful and irrelevant knowledge,” are not supported by empirical evidence. Having more knowledge merely increases our confidence, but it does not improve the accuracy of our decisions. Similarly, people supplied with “good” and “bad” knowledge often have trouble distinguishing •\\n•\\n•\\n  \\nK \\nN \\nO \\nW \\nL \\nE \\nD \\nG \\nE  \\n \\n \\nD \\nA \\nT \\nA \\n  \\nData  \\nMining I \\nN  \\nF \\nO \\nR \\nM \\nA \\nT \\nI \\nO \\nN \\n  \\nData  \\nPrep aration  Figure 1. The processes that underpin a traditional business intelligence system', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d749529f-67bf-4d84-b427-96f734e5feb7', embedding=None, metadata={'page_label': '53', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Business Intelligence\\nbetween the two, proving that irrelevant knowledge \\ndecreases our decision-making effectiveness. \\nToday, most business managers realize that a \\ngap exists between having the right knowledge and making the right decision. Because this gap affects management’s ability to answer fundamental business questions (such as “What should be done to increase profits? Reduce costs? Or increase market share?”), the future of business intelligence lies in systems that can provide answers and recommendations, rather than mounds of knowledge in the form of reports. The future \\nof business intelligence lies in systems that can make decisions! As a result, there is a new trend emerging in \\nthe marketplace called Adaptive Business Intelligence. \\nIn addition to performing the role of traditional busi-ness intelligence (transforming data into knowledge), Adaptive Business Intelligence also includes the deci-sion-making process, which is based on prediction and optimization as shown in Figure 2. \\nWhile business intelligence is often defined as “a \\nbroad category of application programs and technolo-gies for gathering, storing, analyzing, and providing access to data,” the term Adaptive Business Intelligence \\ncan be defined as “the discipline of using prediction and optimization techniques to build self-learning ‘decision-ing’ systems” (as the above diagram shows). Adaptive Business Intelligence  systems include elements of data \\nmining, predictive modeling, forecasting, optimization, and adaptability, and are used by business managers to make better decisions.\\nThis relatively new approach to business intelligence \\nis capable of recommending the best course of action (based on past data), but it does so in a very special way: An Adaptive Business Intelligence system incorporates prediction and optimization modules to recommend near-optimal decisions, and an “adaptability module” for improving future recommendations. Such systems can help business managers make decisions that in-crease efficiency, productivity, and competitiveness. Furthermore, the importance of adaptability  cannot be \\noveremphasized. After all, what is the point of using a software system that produces sub par schedules, inac-curate demand forecasts, and inferior logistic plans, time after time? Would it not be wonderful to use a software system that could adapt to changes in the marketplace? \\nA software system that could improve with time? \\nFUTURE TRENDS\\nThe concept of adaptability is certainly gaining popu-larity, and not just in the software sector. Adaptability has already been introduced in everything from auto-matic car transmissions (which adapt their gear-change patterns to a driver’s driving style), to running shoes (which adapt their cushioning level to a runner’s size and stride), to Internet search engines (which adapt their search results to a user’s preferences and prior search history). These products are very appealing for individual consumers, because, despite their mass pro-duction, they are capable of adapting to the preferences of each unique owner after some period of time.\\nThe growing popularity of adaptability is also \\nunderscored by a recent publication of the US De-\\nFigure 2. Adaptive business intelligence system \\nD\\nE\\nC\\nI\\nS\\nI\\nO\\nNOptimization\\nPredictionK\\nN\\nO\\nW\\nL\\nE\\nD\\nG\\nEAdaptability\\nD\\nA\\nT\\nAData \\nMiningI\\nN\\nF\\nO\\nR\\nM\\nA\\nT\\nI\\nO\\nNData \\nPreparation', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e545d1d7-5d67-4184-aa17-3ccd4e2f3d81', embedding=None, metadata={'page_label': '54', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Business Intelligence\\nApartment of Defense. This lists 19 important research \\ntopics for the next decade and many of them include the term “adaptive”: Adaptive  Coordinated Control in \\nthe Multi-agent 3D Dynamic Battlefield, Control for Adaptive  and Cooperative Systems, Adaptive  System \\nInteroperability, Adaptive  Materials for Energy-Absorb-\\ning Structures, and Complex Adaptive  Networks for \\nCooperative Control.\\nFor sure, adaptability was recognized as important \\ncomponent of intelligence quite some time ago: Alfred Binet (born 1857), French psychologist and inventor of the first usable intelligence test, defined intelligence \\nas “... judgment, otherwise called good sense, practi-cal sense, initiative, the faculty of adapting one’s self \\nto circumstances.” Adaptability is a vital component of any intelligent system, as it is hard to argue that a system is “intelligent” if it does not have the capacity to adapt. For humans, the importance of adaptability is obvious: our ability to adapt was a key element in the evolutionary process. In psychology, a behavior or trait is adaptive when it helps an individual adjust and function well within a changing social environment. In the case of artificial intelligence, consider a chess program capable of beating the world chess master: Should we call this program intelligent? Probably not. We can attribute the program’s performance to its ability to evaluate the current board situation against a multitude of possible “future boards” before selecting the best move. However, because the program cannot learn or adapt to new rules, the program will lose its effectiveness if the rules of the game are changed or modified. Consequently, because the program is inca-pable of learning or adapting to new rules, the program is not intelligent. \\nThe same holds true for any expert system. No one \\nquestions the usefulness of expert systems in some en-vironments (which are usually well defined and static), but expert systems that are incapable of learning and adapting should not be called “intelligent.” Some expert knowledge was programmed in, that is all.\\nSo, what are the future trends for Adaptive Business \\nIntelligence? In words of Jim Goodnight, the CEO of SAS Institute (Collins et al. 2007):\\n“Until recently, business intelligence was limited to \\nbasic query and reporting, and it never really provided that much intelligence ….”However, this is about to change. Keith Collins, the \\nChief Technology Officer of SAS Institute (Collins et al. 2007) believes that:\\n“A new platform definition is emerging for business \\nintelligence, where BI is no longer defined as simple query and reporting. […] In the next five years, we’ll also see a shift in performance management to what we’re calling predictive performance management, where analytics play a huge role in moving us beyond just simple metrics to more powerful measures.”\\nFurther, Jim Davis, the VP Marketing of SAS \\nInstitute (Collins et al. 2007) stated:\\n“In the next three to five years, we’ll reach a tipping \\npoint where more organizations will be using BI to focus on how to optimize processes and influence the bottom line ….”\\nFinally, it would be important to incorporate adapt-\\nability  in prediction and optimization components of \\nthe future Adaptive Business Intelligence systems.\\nThere are some recent, successful implementations \\nof Adaptive Business Intelligence systems reported (e.g., Michalewicz et al. 2005), which provide daily decision support for large corporations and result in multi-million dollars return on investment. There are also companies (e.g., www.solveitsoftware.com) which specialize in development of Adaptive Business Intelli-gence tools. However, further research effort is required. For example, most of the research in machine learning has focused on using historical data to build prediction models. Once the model is built and evaluated, the goal is accomplished. However, because new data arrive at regular intervals, building and evaluating a model is just the first step in Adaptive Business Intelligence. Because these models need to be updated regularly (something that the adaptability module is responsible for), we expect to see more emphasis on this updating process in machine learning research. Also, the frequency of updating the prediction module, which can vary from seconds (e.g., in real-time currency trading systems), to weeks and months (e.g., in fraud detection systems) may require different techniques and methodologies. In general, Adaptive Business Intelligence systems would include the research results from control theory, statistics, operations research, machine learning, and modern heuristic methods, to name a few. We also ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='526929aa-7ef2-4d77-a5e0-dae16d1f4f82', embedding=None, metadata={'page_label': '55', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  Adaptive Business Intelligence\\nexpect that major advances will continue to be made in \\nmodern optimization techniques. In the years to come, more and more research papers will be published on constrained and multi-objective optimization problems, and on optimization problems set in dynamic environ-ments. This is essential, as most real-world business problems are constrained, multi-objective, and set in a time-changing environment. \\nCONCLUSION\\nIt is not surprising that the fundamental components of Adaptive Business Intelligence are already emerging in other areas of business. For example, the Six Sigma \\nmethodology is a great example of a well-structured, data-driven methodology for eliminating defects, waste, and quality-control problems in many industries. This methodology recommends the sequence of steps shown in Figure 3.\\nNote that the above sequence is very close “in \\nspirit” to part of the previous diagram, as it describes (in more detail) the adaptability control loop. Clearly, we have to “measure,” “analyze,” and “improve,” as we operate in a dynamic environment, so the process of improvement is continuous. The SAS Institute proposes another methodology, which is more oriented towards data mining activities. Their methodology recommends the sequence of steps shown in Figure 4.\\nAgain, note that the above sequence is very close \\nto another part of our diagram, as it describes (in more detail) the transformation from data to knowledge. It is not surprising that businesses are placing consider-able emphasis on these areas, because better decisions usually translate into better financial performance. And better financial performance is what Adaptive Business Intelligence is all about. Systems based on Adaptive Business Intelligence aim at solving real-world busi-ness problems that have complex constraints, are set in time-changing environments, have several (possibly conflicting) objectives, and where the number of pos -\\nsible solutions is too large to enumerate. Solving these problems requires a system that incorporates modules for prediction, optimization, and adaptability. \\nREFERENCES\\nCoello, C.A.C., Van Veldhuizen, A.A., and Lamont, G.B. (2002). Evolutionary algorithms for solving multi-\\nobjective problems. Kluwer Academic.\\nCollins, K., Goodnight, J., Hagström, M., Davis, J. \\n(2007). The future of business intelligence: Four ques-tions, four views. SASCOM, First quarter, 2007.\\nDavenport, T.H. and Prusak, L. (2006). Working knowl-\\nedge. Academic Internet Publishers.Deb, K. (2001). Multi-objective optimization using \\nevolutionary algorithms .Wiley.\\nDhar, V . and Stein, R., (1997). Seven methods for \\ntransforming corporate data into business intelligence. \\nPrentice Hall.\\nLoshin, D. (2003). Business intelligence: The savvy \\nmanager’ s guide. Margan Kaufmann.Makridakis, S., Wheelwright, S.C., and Hyndman, \\nR.J. (1998). Forecasting: Methods and applications. \\nWiley.\\nMichalewicz, Z. and Fogel, D.B. (2004). How to solve \\nit: Modern heuristics,  2nd edition. Springer.\\n Define  Control  Improve  Analyze  Measure  \\n Sample  Assess  Model  Modify  Explore  Figure 4. SAS Institute recommended methodolgy sequenceFigure 3. Six Sigma methodology sequence', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90a48b43-5600-4ace-81db-607c5b0cec0c', embedding=None, metadata={'page_label': '56', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Business Intelligence\\nAMichalewicz, Z., Schmidt, M., Michalewicz, M., and \\nChiriac, C. (2005). A decision-support system based on computational intelligence: A case study. IEEE \\nIntelligent Systems, 20 (4), 44-49. \\nMichalewicz, Z., Schmidt, M., Michalewicz, M., and Chiriac, C. (2007). Adaptive business intelligence. \\nSpringer.\\nMoss, L. T. and Atre, S. (2003). Business intelligence \\nroadmap. Addison Wesley.Prusak, L. (1997). Knowledge in organizations. But-\\nterworth-Heinemann.Shortliffe, E. H. and Cimino, J. J. Eds (2006). Biomedi-\\ncal informatics: Computer applications in health care \\nand biomedicine.  Springer.\\nVitt, E., Luckevich, M., and Misner, S. (2002). Business \\nintelligence: Making better decisions faster. Microsoft \\nPress.\\nWeiss, S. M. and Indurkhya, N., (1998).  Predictive \\ndata mining. Morgan Kaufmann.Witten, I. H. and Frank, E. (2005). Data mining: \\nPractical machine learning tools and techniques, 2nd \\nedition. Morgan Kaufmann.TERMS AND DEFINITIONS\\nAdaptive Business Intelligence: The discipline of \\nusing prediction and optimization techniques to build self-learning ‘decisioning’ systems”.\\nBusiness Intelligence: A collection of tools, \\nmethods, technologies, and processes needed to transform data into actionable knowledge.\\nData: Pieces collected on a daily basis in the form \\nof bits, numbers, symbols, and “objects.” \\nData Mining:  The application of analytical methods \\nand tools to data for the purpose of identifying patterns, relationships, or obtaining systems that perform useful tasks such as classification, prediction, estimation, or affinity grouping.\\nInformation: “Organized data,” which are prepro -\\ncessed, cleaned, arranged into structures, and stripped of redundancy. \\nKnowledge: “Integrated information,” which in -\\ncludes facts and relationships that have been perceived, discovered, or learned.\\nOptimization: Process of finding the solution that \\nis the best fit to the available resources.\\nPrediction:  A statement or claim that a particular \\nevent will occur in the future.\\nENDNOTE\\n1 Note that business intelligence  can be defined both \\nas a “state” (a report that contains knowledge) and a “process” (software responsible for converting data into knowledge).', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='941fc921-91e7-4efc-b0df-6599a69e0db4', embedding=None, metadata={'page_label': '57', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAdaptive Neural Algorithms for PCA and ICA\\nRadu Mutihac\\nUniversity of Bucharest, Romania\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nArtificial neural networks (ANNs) (McCulloch & Pitts, 1943) (Haykin, 1999) were developed as models of their biological counterparts aiming to emulate the real neural systems and mimic the structural organization and func-tion of the human brain. Their applications were based on the ability of self-designing to solve a problem by learning the solution from data. A comparative study of neural implementations running principal component analysis (PCA) and independent component analysis (ICA) was carried out. Artificially generated data ad-ditively corrupted with white noise in order to enforce randomness were employed to critically evaluate and assess the reliability of data projections. Analysis in both time and frequency domains showed the superiority of the estimated independent components (ICs) relative to principal components (PCs) in faithful retrieval of the genuine (latent) source signals.\\nNeural computation belongs to information pro-\\ncessing dealing with adaptive, parallel, and distributed (localized) signal processing. In data analysis, a com-mon task consists in finding an adequate subspace of multivariate data for subsequent processing and inter-pretation. Linear transforms are frequently employed in data model selection due to their computational and conceptual simplicity. Some common linear transforms are PCA, factor analysis (FA), projection pursuit (PP), and, more recently, ICA (Comon, 1994). The latter emerged as an extension of nonlinear PCA (Hotelling, 1993) and developed in the context of blind source separation (BSS) (Cardoso, 1998) in signal and array processing. ICA is also related to recent theories of the visual brain (Barlow, 1991), which assume that consecutive processing steps lead to a progressive re-duction in the redundancy of representation (Olshausen and Field, 1996).\\nThis contribution is an overview of the PCA and \\nICA neuromorphic architectures and their associated algorithmic implementations increasingly used as ex-ploratory techniques. The discussion is conducted on artificially generated sub- and super-Gaussian source signals. BACKGROUND\\nIn neural computation, transforming methods amount to unsupervised learning, since the representation is only learned from data without any external control. Irrespective of the nature of learning, the neural adap-tation may be formally conceived as an optimization problem: an objective function describes the task to be performed by the network and a numerical optimization procedure allows adapting network parameters (e.g., connection weights, biases, internal parameters). This process amounts to search or nonlinear programming in a quite large parameter space. However, any prior knowledge available on the solution might be efficiently exploited to narrow the search space. In supervised learning, the additional knowledge is incorporated in the net architecture or learning rules (Gold, 1996). A less extensive research was focused on unsupervised learning. In this respect, the mathematical methods usually employed are drawn from classical constrained multivariate nonlinear optimization and rely on the Lagrange multipliers method, the penalty or barrier techniques, and the classical numerical algebra tech-niques, such as deflation/renormalization (Fiori, 2000), the Gram-Schmidt orthogonalization procedure, or the projection over the orthogonal group (Yang, 1995).\\nPCA and ICA Models\\nMathematically, the linear stationary PCA and ICA mod-els can be defined on the basis of a common data model. Suppose that some stochastic processes are represented \\nby three random (column) vectors \\n( ) ( ),  Nt t ∈ x n \\uf052 \\nand ( )Mt∈s\\uf052 with zero mean and finite covariance, \\nwith the components of ( ) ( ) ( ) ( ){ } 1 2, ,...,M t s t s t s t=s  \\nbeing statistically independent and at most one Gauss-\\nian. Let A be a rectangular constant full column rank \\nN M× matrix with at least as many rows as columns \\n(N M≥ ), and denote by t the sample index (i.e., time \\nor sample point) taking the discrete values t = 1, 2, ..., ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97b0d78c-b6d1-44da-869a-46ea902f991d', embedding=None, metadata={'page_label': '58', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Neural Algorithms for PCA and ICA\\nAT. We postulate the existence of a linear relationship \\namong these variables like:\\n( ) ( ) ( ) ( ) ( )\\n1  M\\ni i\\nit t t s t t\\n== + = + ∑ x As n a n  (1)\\nHere ( )ts, ( )tx, ( )tn , and A are the sources, \\nthe observed data, the (unknown) noise in data, and the (unknown) mixing matrix, respectively, whereas \\n, 1, 2,...,=ii Ma  are the columns of A. Mixing is sup-\\nposed to be instantaneous, so there is no time delay \\nbetween a (latent) source variable ( )is t  mixing into \\nan observable (data) variable  ( )jx t , with i = 1, 2, ..., \\nM and j = 1, 2, ..., N.\\nConsider that the stochastic vector process \\n( ){ } ∈\\uf052Ntx  has the mean ( ){ } 0= E tx  and the covari-\\nance matrix ( ) ( ){ }  =TE t txC x x . The goal of PCA is \\nto identify the dependence structure in each dimension \\nand to come out with an orthogonal transform matrix \\nW of size ×L N  from \\uf052N to \\uf052L, <L N , such that \\nthe L-dimensional output vector ( ) ( )=t ty W x  suf-\\nficiently represents the intrinsic features of the input \\ndata, and where the covariance matrix yC of ( ){ }ty  \\nis a diagonal matrix D with the diagonal elements ar-\\nranged in descending order, , 1, 1 + +≥i i i id d . The restoration \\nof ( ){ }tx  from ( ){ }ty , say ( ){ }ˆtx , is consequently \\ngiven by ( ) ( )ˆ  =Tt t x W W x  (Figure 1). For a given \\nL, PCA aims to find an optimal value of W, such as to minimize the error function ( ) ( ){ } ˆ = −J E t t x x . \\nThe rows in W are the PCs of the stochastic process \\n( ){ }tx and the eigenvectors ,  1, 2,...,jj L= c  of the input \\ncovariance matrix xC. The subspace spanned by the \\nprincipal eigenvectors { }1 2, ,...,L c c c  with L N<, is \\ncalled the PCA subspace of dimensionality L.\\nThe ICA problem can be formulated as following: \\ngiven T realizations of ( )tx, estimate both the matrix \\nA and the corresponding realizations of ( )ts. In BSS \\nthe task is somewhat relaxed to finding the waveforms \\n( ){ }is t  of the sources knowing only the (observed) \\nmixtures ( ){ }jx t . If no suppositions are made about \\nthe noise, the additive noise term is omitted in (1). A \\npractical strategy is to include noise in the signals as supplementary term(s): hence the ICA model (Fig. 2) becomes:\\n( ) ( ) ( )\\n1M\\ni i\\nit t s t\\n== = ∑ x As a\\n   (2)\\nThe source separation consists in updating an unmix-\\ning matrix ( )tB , without resorting to any information \\nabout the spatial mixing matrix A, so that the output vec-\\ntor ( ) ( ) ( )  t t t= y B x  becomes an estimate ( ) ( ) ˆt t= y s  \\nof the original independent source signals ( )ts. The \\nseparating matrix ( )tB  is divided in two parts deal-\\ning with dependencies in the first two moments, i.e., \\nthe whitening matrix ( )tV , and the dependencies in \\nFigure 1. Schematic of the PCA model\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9247e048-1826-45b0-99af-20ff7f34ace0', embedding=None, metadata={'page_label': '59', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Neural Algorithms for PCA and ICA\\nhigher-order statistics, i.e., the orthogonal separating \\nmatrix ( )tW  in the whitened space (Fig. 2). If we as-\\nsume zero-mean observed data ( )tx, then we get by \\nwhitening a vector ( ) ( ) ( )  t t t= v V x  with decorrelated \\ncomponents. The subsequent linear transform ( )tW  \\nseeks the solution by an adequate rotation in the space of component densities and yields \\n( ) ( ) ( )  t t t= y W v  \\n(Fig. 2). The total separation matrix between the input and the output layer turns to be \\n( ) ( ) ( )  t t t= B W V\\n. In the standard stationary case, the whitening and \\nthe orthogonal separating matrices converge to some constant values after a finite number of iterations dur -\\ning learning, that is, \\n( )  t→ = B B W V .NEURAL IMPLEMENTATIONS\\nA neural approach to BSS entails a network that has mixtures of the source signals as input and produces approximations of the source signals as output (Figure 3). As a prerequisite, the input signals must be mutu-ally uncorrelated, a requirement usually fulfilled by PCA. The output signals must nevertheless be mutually independent, which leads in a natural way from PCA to ICA. The higher order statistics required by source separation can be incorporated into computations either explicitly or by using suitable nonlinearities. ANNs better fit the latter approach (Karhunen, 1996).\\nThe core of the large class of neural adaptive al-\\ngorithms consists in a learning rule and its associated optimization criterion (objective function). These two items differentiate the algorithms, which are actually families of algorithms parameterized by the nonlinear Figure 2. Schematic of the ICA model\\nFigure 3. A simple feed-forward ANN performing PCA and ICA\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a43c580-d703-448a-8dc2-031e6958d7fe', embedding=None, metadata={'page_label': '60', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Neural Algorithms for PCA and ICA\\nAfunction used. An update rule is specified by the itera -\\ntive incremental change ∆W of the rotation matrix W, \\nwhich gives the general form of the learning rule:\\nW → W + ∆W    (3)\\nNeural PCA\\nFirst, consider a single artificial neuron receiving an \\nM-dimensional input vector x. It gradually adapts its \\nweight vector w so that the function ( ){ }TE f w x  is \\nmaximized, where E is the expectation with respect to \\nthe (unknown) probability density of x and f is a con-\\ntinuous objective function. The function f is bounded by \\nsetting constant the Euclidian norm of w. A constrained \\ngradient ascent learning rule based on a sequence of sample functions for relatively small learning rates \\n \\n( )t is then (Oja, 1995):\\n \\n( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )( ) 1      a + = + −T Tt t t t t t g t tw w I w w x w w\\n      (4)\\nwhere g f ′=. Any PCA learning rules tend to find that \\ndirection in the input space along which the data has \\nmaximal variance. If all directions in the input space have equal variance, the one-unit case with a suitable nonlinearity is approximately minimizing the kurtosis of the neuron input. It means that the weight vector of the unit will be determined by the direction in the input space on which the projection of the input data is mostly clustered and deviates significantly from normality. This task is essentially the goal in the PP technique.\\nIn the case of single layer ANNs consisting of L \\nparallel units, with each unit i having the same M-\\nelement input vector x and its own weight vector \\niw that together comprise an M L× weight matrix \\n[ ]1 2, ,... ,L =W w w w  the following training rule ob-\\ntained from (4) is a generalization of the linear PCA learning rule (in matrix form):\\n \\n( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )( ) 1     T Tt t t t t t g t t a + = + − W W I W W x x W  \\n      (5)Due to the instability of the above nonlinear Heb-\\nbian learning rule for the multi-unit case, a different \\napproach based on optimizing two criteria simultane-ously was introduced (Oja, 1982):\\n \\n( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )( ) 1     T Tt t t t g t t t t+ = + + − W W x y I W W\\n      (6)\\nHere  ( )t  is chosen positive or negative depending \\non our interest in maximizing or minimizing, respec-\\ntively, the objective function ( ) ( ){ } 1T\\ni i J E f =w x w\\n. Similarly, ( )t is another gain parameter that is \\nalways positive and constrains the weight vectors to \\northonormality, which is imposed by an appropriate penalty function such as:\\n( ) ( ) ( )2 2\\n2\\n1,1 112 2M\\nT T\\ni i i i j\\nj j iJ\\n= ≠= − + ∑ w w w w w . \\nThis is the bigradient algorithm, which is iterated until the weight vectors have converged with the desired accuracy. This algorithm can use normalized Hebbian or anti-Hebbian learning in a unified formula. Starting from one-unit rule, the multi-unit bigradient algorithm can simultaneously extract several robust counterparts of the principal or minor eigenvectors of the data co-variance matrix (Wang, 1996).\\n In the case of multilayered ANNs, the transfer \\nfunctions of the hidden nodes can be expressed by radial basis functions (RBF), whose parameters could be learnt by a two-stage gradient descent strategy. A new growing RBF-node insertion strategy with different RBF is used in order to improve the net performances. The learning strategy is reported to save computational time and memory space in approximation of continuous and discontinuous mappings (Esposito et al., 2000).\\nNeural ICA\\nVarious forms of unsupervised learning have been implemented in ANNs beyond standard PCA like non-linear PCA and ICA. Data whitening can be neurally emulated by PCA with a simple iterative algorithm that \\nupdates the sphering matrix \\n( )tV :', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f5b712b-69f5-4d3d-ad0f-366e2f3dba12', embedding=None, metadata={'page_label': '61', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Neural Algorithms for PCA and ICA\\n( ) ( ) ( ) ( ) 1Tt t t+ = − −V V vv I   (7)\\nAfter getting the decorrelation matrix ( )tV , the ba-\\nsic task for ICA algorithms remains to come out with an \\northogonal matrix ( )tW , which is equivalent to a suit-\\nable rotation of the decorrelated data ( ) ( ) ( )t t t= v V x  \\naiming to maximize the product of the marginal densities \\nof its components. There are various neural approaches \\nto estimate the rotation matrix ( )tW . An important class \\nof algorithms is based on maximization of network \\nentropy (Bell, 1995). The BS nonlinear information maximization (infomax) algorithm performs online stochastic gradient ascent in mutual information (MI) between outputs and inputs of a network. By minimiz-ing the MI between outputs, the network factorizes the inputs into independent components. Considering \\na network with the input vector \\n( )tx, a weight matrix \\n( )tW , and a monotonically transformed output vector \\n( )0 g= +y Wx w , then the resulting learning rule for \\nthe weights and bias-weights, respectively, are:\\n \\n( )1\\n0 2      and     2T T−\\uf8ee \\uf8f9 ∆ = + − ∆ = −\\uf8f0 \\uf8fbW W x 1 y w 1 y\\n      (8)\\nIn the case of bounded variables, the interplay \\nbetween the anti-Hebbian term ( )2T−x 1 y  and the \\nantidecay term 1T−\\uf8ee \\uf8f9\\uf8f0 \\uf8fbW  produces an output density \\nthat is close to the flat constant distribution, which cor -\\nresponds to the maximum entropy distribution. Amari, \\nCichocki, and Yang (Amari, 1996) altered the BS in-fomax algorithm by using the natural gradient instead of the stochastic gradient to reduce the complexity of neural computations and significantly improving the speed of convergence. The update rule proposed for the separating matrix is:\\n( ) ( )  Tg\\uf8ee \\uf8f9 ∆ = −\\uf8f0 \\uf8fbW I Wx Wx W\\n  (9)\\nLee et al. (Lee, 2000) extended to both sub-and \\nsuper-Gaussian distributions the learning rule devel-oped from the infomax principle satisfying a general stability criterion and preserving the simple initial architecture of the network. Applying either natural or relative gradient (Cardoso, 1996) for optimization, their learning rule yields results that compete with fixed-point batch computations.\\nThe equivariant adaptive separation via indepen-\\ndence (EASI) algorithm introduced by Cardoso and Laheld (1996) is a nonlinear decorrelation method. The \\nobjective function \\n( ) ( ) { }= J E fW Wx  is subject to \\nminimization with the orthogonal constraint imposed \\non W and the nonlinearity g f ′= chosen according to \\ndata kurtosis. Its basic update rule equates to:\\n ( ) ( ) ( ) ∆ = − − + −T T Tg g W yy I y y y y W\\n      (10)\\nFixed-point (FP) algorithms are searching the \\nICA solution by minimizing mutual information (MI) \\namong the estimated components (Hyvärinen, 1997). The FastICA learning rule finds a direction w so that \\nthe projection of \\nTw x maximizes a contrast function \\nof the form ( ) ( ){ } ( ){ }2T\\nGJ E f E f \\uf8ee \\uf8f9 = −\\uf8f0 \\uf8fbw w x v  with \\nv standing for the standardized Gaussian variable. The learning rule is basically a Gram-Schmidt-like decor-relation method. \\nALGORITHM ASSESSMENT\\nWe comparatively run both PCA and ICA neural algorithms using synthetically generated time series additively corrupted with some white noise to alleviate strict determinism (Table 1 and Fig. 4.). Neural PCA was implemented using the bigradient algorithm since it works for both minimization and maximization of the criterion J\\n1 under the normality constraints enforced \\nby the penalty function J2. \\nThe neural ICA algorithms were the extended info-\\nmax of Bell and Sejnowski, a semi-adaptive fixed-point fast ICA algorithm (Hyvärinen & Oja, 1997), an adapted variant of EASI algorithm optimized for real data, and the extended generalized lambda distribution (EGLD) maximum likelihood-based algorithm.\\nIn the case of artificially generated sources, the ac -\\ncuracy of separating the latent sources by an algorithm ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d9a954a-a4bd-402b-b5dc-56a06bf64fbc', embedding=None, metadata={'page_label': '62', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Neural Algorithms for PCA and ICA\\nATable 1. The analytical form of the signals sources\\nSignal sources\\nModulated sinusoid:  ( ) ( ) ( ) 1 2 sin 149 cos 8S t t = ∗ ∗\\nSquare waves:  \\n ( ) ( ) ( ) ( ) 2 sin 12 9 cos 2 29S sign t = ∗ + ∗\\nSaw-tooth:   \\n ( ) ( )( )3 ,79 17 23S rem t = −\\nImpulsive curve:  \\n ( ) ( )( )( )54 , 23 11 9S rem t = −\\nExponential decay:  ( ) ( ) ( ) 5 5 exp 121 cos 37S t t = ∗ − ∗ ∗\\nSpiky noise:  \\n ( ) ( )( )( ) ( )( ) 6 1, .5 2 1 log 1,S rand T rand T = < ∗ − ∗\\nFigure 4. Sub-Gaussian (left) and super-Gaussian (right) source signals and their corresponding histograms \\n(bottom)\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e8ff38d8-ec80-4963-83fc-c3a8f456b5a7', embedding=None, metadata={'page_label': '63', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Neural Algorithms for PCA and ICA\\nperforming ICA can be measured by means of some \\nquantitative indexes. The first we used was defined as the signal-to-interference ratio ( SIR): \\n( )\\n( )2\\n10 2\\n1max 110 log\\nmaxN\\ni\\nT\\nii i iQSIRN Q Q Q == ⋅\\n−∑\\n  \\n      (11)\\nwhere =Q BA  is the overall transforming matrix of \\nthe latent source components, iQ is the i-th column \\nof Q, ( )maxiQ is the maximum element of iQ, and \\nN is the number of the source signals. The higher the \\nSIR is, the better the separation performance of the algorithm. \\nA secondly employed index was the distance be-\\ntween the overall transforming matrix Q and an ideal \\npermutation matrix, which is interpreted as the cross-talking error ( CTE):\\n1 1 1 11 1max maxN N N Nij ij\\ni j j i i jQ Q\\nCTEQ Q = = = =\\uf8eb \\uf8f6 \\uf8eb \\uf8f6\\n\\uf8ec \\uf8f7 \\uf8ec \\uf8f7 = − + −\\uf8ec \\uf8f7 \\uf8ec \\uf8f7\\uf8ed \\uf8f8 \\uf8ed \\uf8f8∑ ∑ ∑ ∑\\n \\n      (12)\\nAbove, ijQ is the ij-th element of Q, maxiQ is \\nthe maximum absolute valued element of the row i \\nin Q, and maxjQ is the maximum absolute valued \\nelement of the column j in Q. A permutation matrix is \\ndefined so that on each of its rows and columns, only one of the elements equals to unity while all the other elements are zero. It means that the CTE attains its \\nminimum value zero for an exact permutation matrix (i.e., perfect decomposition) and goes positively higher the more Q deviates from a permutation matrix (i.e., \\ndecomposition of lower accuracy). \\nWe defined the relative signal retrieval er-\\nror ( SRE) as the Euclidian distance between the \\nsource signals and their best matching estimated components normalized to the number of source signals, times the number of time samples, and times the module of the source signals:\\n( ) ( )\\n( )2\\n1 1\\n2\\n1 11 ,  1, 2,...,N T\\ni i\\ni t\\nN T\\ni\\ni tx t y t\\nSRE t TTN\\nx t= =\\n= =\\uf8eb \\uf8f6− \\uf8ee \\uf8f9\\uf8ec \\uf8f7 \\uf8f0 \\uf8fb\\uf8ed \\uf8f8= =\\n\\uf8eb \\uf8f6\\uf8ee \\uf8f9\\uf8ec \\uf8f7\\uf8f0 \\uf8fb\\uf8ed \\uf8f8∑ ∑\\n∑ ∑ \\n      (13)\\nThe lower the SRE is, the better the estimates ap-\\nproximate the latent source signals.\\nThe stabilized version of FastICA algorithm is at-\\ntractive by its fast and reliable convergence, and by the lack of parameters to be tuned. The natural gradient incorporated in the BS extended infomax performs better than the original gradient ascent and is compu-tationally less demanding. Though the BS algorithm is theoretically optimal in the sense of dealing with mutual information as objective function, like all neu-ral unsupervised algorithms, its performance heavily depends on the learning rates and its convergence is rather slow. The EGLD algorithm separates skewed distributions, even for zero kurtosis. In terms of com-putational time, the BS extended infomax algorithm was the fastest, FastICA more faithfully retrieved the sources among all algorithms under test, while the EASI algorithm came out with a full transform matrix Q that is the closest to unity. \\nFUTURE TRENDS\\nNeuromorphic methods in exploratory analysis and data mining are rapidly emerging applications of unsu-pervised neural training. In recent years, new learning algorithms have been proposed, yet their theoretical properties, range of optimal applicability, and compara-tive assessment have remained largely unexplored. No convergence theorems are associated with the training algorithms in use. Moreover, algorithm convergence heavily depends on the proper choice of the learning rate(s) and, even when convergence is accomplished, the neural algorithms are relatively slow compared with batch-type computations. Nonlinear and nonstationary neural ICA is expected to be developed due to ANNs ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8c5ebb5-0ae8-480c-8a10-8e53bc4285f9', embedding=None, metadata={'page_label': '64', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Neural Algorithms for PCA and ICA\\nAnonalgorithmic processing and their ability to learn \\nnonanalytical relationships if adequately trained. \\nCONCLUSION \\nBoth PCA and ICA share some common features like aiming at building generative models that are likely to have produced the observed data and performing information preservation and redundancy reduction. In a neuromorphic approach, the model parameters are treated as network weights that are changed during the learning process. The main difficulty in function approximation stems from choosing the network pa-rameters that have to be fixed a priori, and those that must be learnt by means of an adequate training rule.\\nPCA and ICA have major applications in data \\nmining and exploratory data analysis, such as signal characterization, optimal feature extraction, and data compression, as well as the basis of subspace classi-fiers in pattern recognition. ICA is much better suited than PCA to perform BSS, blind deconvolution, and equalization.\\nREFERENCES\\nAmari, S., Cichocki, A., & Yang, H. (1996). A new learning algorithm for blind source aeparation. In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), Advances in neural information processing , 8. Cam-\\nbridge, MA: MIT Press.\\nBarlow, H. B. (1991). Possible principles underlying \\nthe transformation of sensory messages. In W. A. Ro-senblith (Ed.), Sensory communication  (pp. 217-234). \\nCambridge, MA: MIT Press.\\nBell, A., & Sejnowski, T. (1995). An information-\\nmaximization approach to blind separation and blind deconvolution. Neural Computation , 7, 1129-1159.\\nCardoso, J.-F., & Laheld, B. H. (1996). Equivariant adaptive source separation. IEEE Transactions on \\nSignal Processing, 44, 3017-3030.\\nCardoso, J.-F. (1998). Blind signal separation: Statistical \\nprinciples. Proceeding IEEE , 9, 2009-2025.\\nComon, P. (1994). Independent component analysis, A new concept? Signal Processing, 36, 287-314.Esposito, A., Marinaro, M., & Scarpetta, S. (2000). Ap-proximation of continuous and discontinuous mappings \\nby a growing neural RBF-based algorithm.\\n Neural \\nNetworks , 13(6) 651-665 . \\nFiori, S., & Piazza, F. (2000). A general class of APEX-\\nlike PCA neural algorithms, IEEE Transactions on \\nCircuits and Systems - Part I. 47, 1394-1398.\\nGold, S., Rangarajan, A., & Mjolsness, E. (1996). \\nLearning with preknowledge: Clustering with point and graph matching distance. Neural Computation , \\n8, 787-804.\\nHaykin, S. (1999). Neural networks (2nd ed.). Engle-\\nwood Cliffs, NJ: Prentice Hall.Hotelling, H. (1993). Analysis of a complex of statis-\\ntical variables into principal components. Journal of \\nEducational Psychology , 24, 417-441 and 498-520.\\nHyvärinen, A., & Oja, E. (1997). A fast fixed-point algo -\\nrithm for ICA. Neural Computation , 9, 1483-1492.\\nKarhunen, J. (1996). Neural approaches to independent component analysis and source separation. Proceedings \\nESANN’96, Bruges, Belgium, 249-266.\\nLee, T.-W., Girolami, M., Bell, A. J., & Sejnowski, T. \\nJ. (2000). A unifying information-theoretic framework for ICA. Computers and Mathematics with Applica-\\ntions, 39, 1-21.\\nMcCulloch, W. S., & Pitts, W. (1943). A logical cal-\\nculus of ideas immanent in nervous activity. Bulletin \\nof Mathematical Biophysics , 5, 115-133.\\nOja, E., Karhunen, J., Wang, L., & Vigario, R. (1995). Principal and independent components in neural networks - Recent developments. Proceedings VIIth \\nWorkshop on Neural Nets, Vietri, Italy.\\nOja, E. (1982). Simplified neuron model as a principal \\ncomponent analyzer. Journal of Mathematical Biology , \\n15, 267-273.\\nOlshausen, B. A., & Field, D. J. (1996). Emergence of \\nsimple-cell receptive field properties by learning a spar -\\nse code for natural images. Nature, 381, 607-609.\\nWang, L. & Karhunen, J. (1996). A unified neural bigradient algorithm for robust PCA and MCA. Inter-\\nnational Journal of Neural Sysems,  7, 53-67.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='131cd47b-7cf0-4183-8f5c-49266178ca40', embedding=None, metadata={'page_label': '65', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  Adaptive Neural Algorithms for PCA and ICA\\nYang, B. (1995). Projection approximation subspace \\ntracking. IEEE Transactions on Signal Proces sing, \\n43, 1247-1252.\\nKEy TERMS\\nArtificial Neural Networks (ANNs): An informa-\\ntion-processing synthetic system made up of several simple nonlinear processing units connected by ele-ments that have information storage and programming functions adapting and learning from patterns, which mimics a biological  neural network.\\nBlind Source Separation (BSS): Separation of \\nlatent nonredundant (e.g., mutually statistically inde-pendent or decorrelated) source signals from a set of linear mixtures, such that the regularity of each result-ing signal is maximized, and the regularity between the signals is minimized (i.e. statistical independence is maximized) without (almost) any information on the sources.\\nConfirmatory Data Analysis (CDA):  An approach \\nwhich, subsequent to data acquisition, proceeds with the imposition of a prior model and analysis, estima-tion, and testing model parameters.Exploratory Data Analysis (EDA): An approach \\nbased on allowing the data itself to reveal its underly-ing structure and model heavily using the collection of techniques known as statistical graphics .\\nIndependent Component Analysis (ICA):  An \\nexploratory method for separating a linear mixture of latent signal sources into independent components as optimal estimates of the original sources on the basis of their mutual statistical independence and non-Gaus-sianity.\\nLearning Rule: Weight change strategy in a con-\\nnectionist system aiming to optimize a certain objective function. Learning rules are iteratively applied to the training set inputs with error gradually reduced as the weights are adapting.\\nPrincipal Component Analysis (PCA):  An or-\\nthogonal linear transform based on singular value decomposition that projects data to a subspace that preserves maximum variance.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a778140a-f3e7-44cb-9442-f119827d3331', embedding=None, metadata={'page_label': '66', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAdaptive Neuro-Fuzzy Systems\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nFuzzy logic became the core of a different approach \\nto computing. Whereas traditional approaches to computing were precise, or hard edged, fuzzy logic allowed for the possibility of a less precise or softer approach (Klir et al., 1995, pp. 212-242). An approach where precision is not paramount is not only closer to the way humans thought, but may be in fact easier to create as well (Jin, 2000). Thus was born the field of soft computing (Zadeh, 1994). Other techniques were added to this field, such as Artificial Neural Networks (ANN), and genetic algorithms, both modeled on bio-logical systems. Soon it was realized that these tools could be combined, and by mixing them together, they could cover their respective weaknesses while at the same time generate something that is greater than its parts, or in short, creating synergy.\\nAdaptive Neuro-fuzzy is perhaps the most prominent \\nof these admixtures of soft computing technologies (Mitra et al., 2000). The technique was first created when artificial neural networks were modified to work with fuzzy logic, hence the Neuro-fuzzy name (Jang et al., 1997, pp. 1-7). This combination provides fuzzy systems with adaptability and the ability to learn. It was later shown that adaptive fuzzy systems could be created with other soft computing techniques, such as genetic algorithms (Yen et al., 1998, pp. 469-490), Rough sets (Pal et al., 2003; Jensen et al., 2004, Ang et al., 2005) and Bayesian networks (Muller et al., 1995), but the Neuro-fuzzy name was widely used, so it stayed. In this chapter we are using the most widely used terminology in the field.\\nNeuro-fuzzy is a blanket description of a wide \\nvariety of tools and techniques used to combine any aspect of fuzzy logic with any aspect of artificial neural networks. For the most part, these combinations are just extensions of one technology or the other. For example, neural networks usually take binary inputs, but use weights that vary in value from 0 to 1. Adding fuzzy sets to ANN to convert a range of input values into values that can be used as weights is considered a Neuro-fuzzy solution. This chapter will pay particular interest to the sub-field where the fuzzy logic rules are modified by the adaptive aspect of the system. \\nThe next part of this chapter will be organized as \\nfollows: in section 1 we examine models and techniques used to combine fuzzy logic and neural networks together to create Neuro-fuzzy systems.  Section 2 provides an overview of the main steps involved in the development of adaptive Neuro-fuzzy systems. Section 3 concludes this chapter with some recommendations and future developments. \\nNEURO-FUZZy TECHNOLOGy\\nNeuro-fuzzy Technology is a broad term used to describe a field of techniques and methods used to combine fuzzy logic and neural networks together (Jin, 2003, pp. 111-140). Fuzzy logic and neural networks each have their own sets of strengths and weaknesses, and most attempts to combine these two technologies have the goal of using each techniques strengths to cover the others weaknesses. \\nNeural networks are capable of self-learning, clas-\\nsification and associating inputs with outputs. Neural networks can also become a universal function ap-proximator (Kosko, 1997, pp. 299; Nauck et al., 1998, Nauck et al. 1999). Given enough information about an unknown continuous function, such as its inputs Larbi EsmahiAthabasca University, Canada\\nKristian Williamson\\nStatistics Canada, Canada\\nElarbi Badidi\\nUnited Arab Emirates University, UAE', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='75e8f8d9-8d1d-471a-87ca-87252830028e', embedding=None, metadata={'page_label': '67', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Neuro-Fuzzy Systems\\nand outputs, the neural network can be trained to ap-\\nproximate it. The disadvantages of neural networks are they are not guaranteed to converge, that is to be trained properly, and after they have been trained they cannot give any information about why they take a particular course of action when given a particular input.\\nFuzzy logic Inference systems can give human \\nreadable and understandable information about why a particular course of action was taken because it is governed by a series of IF THEN rules. Fuzzy logic systems can adapt in a way that their rules and the pa-rameters of the fuzzy sets associated with those rules can be changed to meet some criteria. However fuzzy logic systems lack the capability for self-learning, and must be modified by an external entity. Another salient feature of fuzzy logic systems is that they are, like artificial neural networks, capable of acting as universal approximators.\\nThe common feature of being able to act as a uni-\\nversal approximator is the basis of most attempts to merge these two technologies. Not only it can be used to approximate a function but it can also be used by both neural networks, and fuzzy logic systems to approximate each other as well. (Pal et al., 1999, pp. 66)\\nUniversal approximation is the ability of a system \\nto replicate a function to some degree.  Both neural networks and fuzzy logic systems do this by using a non-mathematical model of the system (Jang et al., 1997, pp. 238; Pal et al., 1999, pp. 19). The term ap-proximate is used as the model does not have to match the simulated function exactly, although it is sometime possible to do so if enough information about the func-tion is available. In most cases it is not necessary or even desirable to perfectly simulate a function as this takes time and resources that may not be available and close is often good enough.\\nCategories of Neuro-Fuzzy Systems\\nEfforts to combine fuzzy logic and neural networks have been underway for several years and many methods have been attempted and implemented. These methods are of two major categories:\\nFuzzy Neural Networks (FNN): are neural networks that can use fuzzy data, such as fuzzy rules, sets and values (Jin, 2003, pp.205-220). •Neural-Fuzzy Systems (NFS): are fuzzy systems “augmented” by neural networks (Jin, 2003, pp.111-140).\\nThere also four main architectures used for imple-\\nmenting neuro-fuzzy systems:\\nFuzzy Multi-layer networks (Jang, 1993; Mitra et al., 1995; Mitra et al., 2000; Mamdani et al., 1999; Sugeno et al., 1988, Takagi et al., 1985).Fuzzy Self-Organizing Map networks (Drobics et al., 2000; Kosko, 1997, pp. 98; Haykin, 1999, pp. 443)Black-Box Fuzzy ANN (Bellazzi et al., 1999; Qiu, 2000; Monti, 1996) Hybrid Architectures (Zatwarnicki, 2005; Borzem-ski et al., 2003; Marichal et al., 2001; Rahmoun et al., 2001; Koprinska et al., 2000; Wang et al. 1999; Whitfort et al., 1995).\\nDEVELOPMENT OF ADAPTIVE  NEURO-FUZZy SySTEMS\\nDeveloping an Adaptive Neuro-fuzzy system is a pro-cess that is similar to the procedures used to create fuzzy logic systems, and neural networks. One advantage of this combined approach is that it is usually no more complicated than either approach taken individually. \\nAs noted above, there are two methods of creating \\na Neuro-fuzzy system; integrating fuzzy logic into a neural network framework (FNN), and implementing neural networks into a fuzzy logic system (NFS). A fuzzy neural network is just a neural network with some fuzzy logic components; hence is generally trained like a normal neural network is. \\nTraining Process: The training regimen for a NFS \\ndiffers slightly from that used to create a neural network and a fuzzy logic system in some key ways, while at the same time incorporating many improvements over those training methods. \\nThe training process of a Neuro-fuzzy system has \\nfive main steps: (V on Altrock, 1995, pp. 71-75)\\n• Obtain Training Data: The data must cover all \\npossible inputs and output, and all the critical regions of the function if it is to model it in an appropriate manner. •\\n•\\n••\\n•', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f75f91b2-eb8e-4813-a54f-113d6785d40f', embedding=None, metadata={'page_label': '68', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Neuro-Fuzzy Systems\\nA• Create a Fuzzy Logic System: The fuzzy system \\nmay be an existing system which is known to work, \\nsuch as one that has been in production for some time or one that has been created by following expert system development methodologies. \\n• Define the Neural Fuzzy Learning: This phase deals with defining what you want the system to learn. This allows greater control over the learning process while still allowing for rule knowledge discovery.\\n• Training Phase: To run the training algorithm. The algorithm may have parameters that can be adjusted to modify how the system is to be modi-fied during training. \\n• Optimization and Verification: Validation can \\ntake many forms, but will usually involve feeding the system a series of known inputs to determine if the system generates the desired output, and or is within acceptable parameters. Furthermore, the rules and membership functions may be extracted so they can be examined by human experts for correctness.\\nCONCLUSION AND FUTURE  DEVELOPMENTS\\nAdvantages of ANF systems: Although there are many \\nways to implement a Neuro-fuzzy system, the advan-tages described for these systems are remarkably uni-form across the literature. The advantages attributed to Neuro-fuzzy systems as compared to ANNs are usually related to the following aspects:\\n• Faster to train: This is due to the massive num-\\nber of connections present in the ANN, and the \\nnon-trivial number of calculations associated with each. As well, most neural fuzzy systems can be trained by going through the data once, whereas a neural network may need to be exposed to the same training data many times before it converges. \\n• Less computational resources: Neural fuzzy sys-\\ntem is smaller in size and contains fewer internal connections than a comparable ANN, hence it is faster and use significantly less resources. \\n• Offer the possibility to extract the rules: This is a major advantage over ANNs in that the rules governing a system can be communicated to the human users in an easily understandable form. Limitation of ANF systems: The greatest limitation \\nin creating adaptive systems is known as the “Curse of Dimensionality”, which is named after the exponen-tial growth in the number of features that the model has to keep track of as the number of input attributes increases. Each attribute in the model is a variable in the system, which corresponds to an axis in a multidi-mensional graph that the function is mapped into. The connections between different attributes correspond to the number of potential rules in the system as given by the formula: \\nN\\nrules = (Llingustic_terms)variables (Gorrostieta et al., 2006)\\nThis formula becomes more complicated if there are \\ndifferent numbers of linguistic variables (fuzzy sets) \\ncovering each attribute dimension. Fortunately there are ways around this problem. As the neural fuzzy system is only approximating the function being modeled, the system may not need all the attributes to achieve the desired results. \\nAnother area of criticism in the Neuro-fuzzy field is \\nrelated to aspects that can’t be learned or approximated. One of the most known aspects here is the caveat at-tached to the universal approximation. In fact, the function being approximated has to be continuous; a continuous function is a function that does not have a singularity, a point where it goes to infinity. Other functions that Adaptive Neuro-fuzzy systems may have problems learning are things like encryption algorithms, which are purposely designed to be resistant to this type of analysis. \\nFuture developments: Predicting the future has \\nalways been hard; however for ANF technology the future expansion has been made easy because of the widespread use of its basis technology (neural networks and fuzzy logic). Mixing of these technologies creates synergies as they remediate to each other weaknesses. ANF technology allows complex system to be grown instead of someone having to build them. \\nOne of the most promising areas for ANF systems \\nis System Mining. There exist many cases where we wish to automate a system that cannot be systematically described in a mathematical manner. This means there is no way of creating a system using classical development methodologies (i.e. Programming a simulation.). If we have an adequately large set of examples of inputs and their corresponding outputs, ANF can be used to get a model of the system. The rules and their associated ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0c3234f-b9c3-41f0-80df-a1a5ccb410e8', embedding=None, metadata={'page_label': '69', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Neuro-Fuzzy Systems\\nfuzzy sets can then be extracted from this system and \\nexamined for details about how the system works. This knowledge can be used to build the system directly. One interesting application of this technology is to audit existing complex systems. The extracted rules could be used to determine if the rules match the exceptions of what the system is supposed to do, and even detect fraud actions. Alternatively, the extracted model may show an alternative, and or more efficient manner of implementing the system. \\nREFERENCES\\nAng, K. K. & Quek, C. (2005). RSPOP: Rough Set-Based Pseudo Outer-Product Fuzzy Rule Identification Algorithm. Neural Computation, (17) 1, 205-243.\\nBellazzi, R., Guglielmann, R. & Ironi L. (1999). A \\nqualitative-fuzzy framework for nonlinear black-box system identification. In Dean T., editor, Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence (IJCAI 99), volume 2, pages 1041—1046. Morgan Kaufmann Publishers. \\nBorzemski, L. & Zatwarnicki, K. (2003). A fuzzy adap-\\ntive request distribution algorithm for cluster-based Web systems. In the Proceedings Eleventh Euromicro Conference on Parallel, Distributed and Network-Based Processing, 119 - 126. Institute of Electrical & Electronics Engineering Publisher.\\nChavan, S., Shah, K., Dave, N., Mukherjee, S., Abra-\\nham, A., & Sanyal, S. (2004). Adaptive neuro-fuzzy intrusion detection systems. In Proceedings of the International Conference on Information Technology: Coding and Computing, ITCC 2004, 70 - 74 V ol.1. Institute of Electrical & Electronics Engineering Publisher.\\nDrobics, M., Winiwater & W., Bodenhofer, U. (2000). \\nInterpretation of self-organizing maps with fuzzy rules. In Proceedings of the 12th IEEE International Confer-ence on Tools with Artificial Intelligence (ICTAI’00), p. 0304. IEEE Computer Society Press.\\nGorrostieta, E. & Pedraza, C. (2006). Neuro Fuzzy \\nModeling of Control Systems. In Proceedings of the 16th IEEE International Conference on Electronics, Communications and Computers (CONIELECOMP 2006), 23 – 23. IEEE Computer Society Publisher.Haykin, S. (1999). Neural Networks: A Comprehensive Foundation. Prentice Hall Publishers, 2nd edition \\nJang, J. S. R., Sun C. T. & Mizutani E. (1997). Neuro-\\nFuzzy and Soft Computing: A Computational Approach to Learning and Machine Intelligence. Prentice Hall Publishers, US Ed edition.\\nJang, J.-S.R. (1993). ANFIS: adaptive-network-based \\nfuzzy inference system. IEEE Transactions on Systems, Man and Cybernetics, (23) 3, 665 – 685.\\nJensen, R. & Shen, Q. (2004). Semantics-preserving \\ndimensionality reduction: rough and fuzzy-rough-based approaches. IEEE Transactions on Knowledge and Data Engineering, (16) 12, 1457 – 1471.\\nJin Y . (2000). Fuzzy modeling of high-dimensional \\nsystems: Complexity reduction and interpretability improvement. IEEE Transactions on Fuzzy Systems, (8) 2, 212-221.\\nJin, Y . (2003). Advanced Fuzzy Systems Design and \\nApplications. Physica-Verlag Heidelberg Publishers; 1 edition.\\nKlir, G. J. & Yuan, B. (1995). Fuzzy Sets and Fuzzy \\nLogic: Theory and Applications. Prentice Hall PTR Publishers; 1st edition. \\nKoprinska, L. & Kasabov, N. (2000). Evolving fuzzy \\nneural network for camera operations recognition. In Proceedings of the 15th International Conference on Pattern Recognition, 523 - 526 vol.2. IEEE Computer Society Press Publisher.\\nKosko, B. (1997). Fuzzy Engineering. Prentice Hall \\nPublishers, 1st edition.\\nMamdani, E. H. & Assilian, S. (1999). An Experiment \\nin Linguistic Synthesis with a Fuzzy Logic Controller. International Journal of Human-Computer Studies, (51) 2, 135-147.\\nMarichal, G.N., Acosta, L., Moreno, L., Mendez, J.A. \\n& Rodrigo, J. J. (2001). Obstacle Avoidance for a Mobile Robot: A neuro-fuzzy approach. Fuzzy Sets and Systems, (124) 2, 171- 179.\\nMitra, S. & Hayashi Y . (2000). Neuro-fuzzy rule gen-\\neration: survey in soft computing framework. IEEE Transactions on Neural Networks, (11) 3, 748 – 768.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c813adab-70be-46f4-99d5-0061d6e2706f', embedding=None, metadata={'page_label': '70', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Neuro-Fuzzy Systems\\nAMitra, S. & Pal, S. K. (1995). Fuzzy multi-layer percep-\\ntron, inferencing and rule generation. IEEE Transactions on Neural Networks, (6) 1, 51-63.\\nMonti, A. (1996). A fuzzy-based black-box approach \\nto IGBT modeling. In Proceedings of the Third IEEE International Conference on Electronics, Circuits, and Systems: ICECS ‘96. 1147 - 1150 vol.2. Institute of Electrical & Electronics Engineering Publisher.\\nMuller, P. & Insua, D.R. (1998). Issues in Bayesian \\nAnalysis of Neural Network Models. Neural Computa-tion (10) 3, 749-770. \\nNauck, D. & Kruse R. (1999). Neuro-fuzzy systems \\nfor function approximation. Fuzzy Sets and Systems (101) 261-271. \\nNauck, D. & Kruse, R. (1998). A neuro-fuzzy approach \\nto obtain interpretable fuzzy systems for function ap-proximation. In Wcci 98: Proceedings of Fuzz-IEEE ‘98, 1106 - 1111 vol.2. IEEE World Congress on Computational Intelligence. Institute of Electrical & Electronics Engineering Publisher.\\nPal, S. K. & Mitra S. (1999). Neuro-Fuzzy Pattern \\nRecognition: Methods in Soft Computing. John Wiley & Sons Publishers, 1st edition. \\nPal, S.K., Mitra, S. & Mitra, P. (2003). Rough-fuzzy \\nMLP: modular evolution, rule generation, and evalu-ation. IEEE Transactions on Knowledge and Data Engineering, (15) 1, 14 – 25.\\nQiu F. (2000). Opening the black box of neural networks \\nwith fuzzy set theory to facilitate the understanding of remote sensing image processing. In Proceedings of the IEEE 2000 International Geoscience and Remote Sensing Symposium: Taking the Pulse of the Planet: The Role of Remote Sensing in Managing the Environ-ment, IGARSS 2000. 1531 - 1533 vol.4. Institute of Electrical & Electronics Engineering Publisher.\\nRahmoun, A. & Berrani, S. (2001). A genetic-based \\nneuro-fuzzy generator: NEFGEN. ACS/IEEE Inter-national Conference on Computer Systems and Ap-plications, 18 – 23. Institute of Electrical & Electronics Engineering Publisher.\\nSugeno, M. & Kang, G. T. (1998). Structure identifi-\\ncation of fuzzy model. Fuzzy Sets and Systems, (28) 1, 15-33.Takagi T. & Sugeno M. (1985). Fuzzy identification of systems and its applications to modeling and control. IEEE Transactions on Systems, Man, and Cybernetics, (15), 116-132.\\nV on Altrock, C. (1995). Fuzzy Logic and Neuro Fuzzy \\nApplications Explained. Prentice Hall Publishers.\\nWang L. & Yen J. (1999). Extracting Fuzzy Rules for \\nSystem Modeling Using a Hybrid of Genetic Algo-rithms and Kalman Filter. Fuzzy Sets Systems, (101) 353–362.\\nWhitfort, T., Matthews, C. & Jagielska, I. (1995). Auto-\\nmated knowledge acquisition for a fuzzy classification problem. In Kasabov, N. K. & Coghill, G. (Editors), Proceedings of the Second New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems, 227 – 230. IEEE Computer Society Press Publisher.\\nYen, J. & Langari, R. (1998). Fuzzy Logic: Intelligence, \\nControl, and Information. Prentice Hall Publishers.\\nZadeh, L. A. (1994). Fuzzy Logic, Neural Networks, \\nand Soft Computing. Communications of the ACM (37) 3, 77-84.\\nZatwarnicki, K. (2005). Proposal of a neuro-fuzzy \\nmodel of a WWW server. Proceedings of the Fifth In-ternational Conference on Intelligent Systems Design and Applications ISDA ‘05, 141 – 146. Institute of Electrical & Electronics Engineering Publisher.\\nKEy TERMS\\nArtificial Neural Networks (ANN):  An artificial \\nneural network, often just called a “neural network” (NN), is an interconnected group of artificial neurons that uses a mathematical model or computational model for information processing based on a connectionist approach to computation. Knowledge is acquired by the network from its environment through a learning process, and interneuron connection strengths (synaptic weighs) are used to store the acquired knowledge.\\nEvolving Fuzzy Neural Network (EFuNN): An \\nEvolving Fuzzy Neural Network is a dynamic archi-tecture where the rule nodes grow if needed and shrink by aggregation. New rule units and connections can be added easily without disrupting existing nodes. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='acead926-71bd-4463-8d67-9055e09413ef', embedding=None, metadata={'page_label': '71', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Neuro-Fuzzy Systems\\nThe learning scheme is often based on the concept of \\n“winning rule node”.\\nFuzzy Logic: Fuzzy logic is an application area of \\nfuzzy set theory dealing with uncertainty in reasoning. It utilizes concepts, principles, and methods developed within fuzzy set theory for formulating various forms of sound approximate reasoning. Fuzzy logic allows for set membership values to range (inclusively) between 0 and 1, and in its linguistic form, imprecise concepts like “slightly”, “quite” and “very”. Specifically, it al -\\nlows partial membership in a set.\\nFuzzy Neural Networks (FNN): are Neural Net-\\nworks that are enhanced with fuzzy logic capability such as using fuzzy data, fuzzy rules, sets and values.\\nNeuro-Fuzzy Systems (NFS): A neuro-fuzzy sys-\\ntem is a fuzzy system that uses a learning algorithm derived from or inspired by neural network theory to determine its parameters (fuzzy sets and fuzzy rules) by processing data samples.\\nSelf-Organizing Map (SOM):  The self-organiz-\\ning map is a subtype of artificial neural networks. It is trained using unsupervised learning to produce low dimensional representation of the training samples while preserving the topological properties of the input space. The self-organizing map is a single layer feed-forward network where the output syntaxes are arranged in low dimensional (usually 2D or 3D) grid. Each input is con-nected to all output neurons. Attached to every neuron there is a weight vector with the same dimensionality as the input vectors. The number of input dimensions is usually a lot higher than the output grid dimension. SOMs are mainly used for dimensionality reduction rather than expansion.\\nSoft Computing:  Soft Computing refers to a \\npartnership of computational techniques in computer science, artificial intelligence, machine learning and some engineering disciplines, which attempt to study, model, and analyze complex phenomena. The principle partners at this juncture are fuzzy logic, neuron-com-puting, probabilistic reasoning, and genetic algorithms. Thus the principle of soft computing is to exploit the tolerance for imprecision, uncertainty, and partial truth to achieve tractability, robustness, low cost solution, and better rapport with reality.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='34fa01ab-4bec-414c-9061-934d1a07574d', embedding=None, metadata={'page_label': '72', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAdaptive Technology and Its Applications\\nJoão José Neto\\nUniversidade de São Paulo, Brazil\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nBefore the advent of software engineering, the lack of memory space in computers and the absence of established programming methodologies led early programmers to use self-modification as a regular coding strategy.\\nAlthough unavoidable and valuable for that class \\nof software, solutions using self-modification proved inadequate while programs grew in size and complex-ity, and security and reliability became major require-ments. \\nSoftware engineering, in the 70’s, almost led to the \\nvanishing of self-modifying software, whose occurrence was afterwards limited to small low-level machine-language programs with very special requirements. \\nNevertheless, recent research developed in this area, \\nand the modern needs for powerful and effective ways to represent and handle complex phenomena in high-technology computers are leading self-modification to be considered again as an implementation choice in several situations. \\nArtificial intelligence strongly contributed for this \\nscenario by developing and applying non-conventional approaches, e.g. heuristics, knowledge representation and handling, inference methods, evolving software/hardware, genetic algorithms, neural networks, fuzzy systems, expert systems, machine learning, etc. \\nIn this publication, another alternative is proposed \\nfor developing Artificial Intelligence applications: the use of adaptive devices, a special class of abstractions whose practical application in the solution of current problems is called Adaptive Technology.\\nThe behavior of adaptive devices is defined by a \\ndynamic set of rules. In this case, knowledge may be represented, stored and handled within that set of rules by adding and removing rules that represent the addition or elimination of the information they represent.\\nBecause of the explicit way adopted for representing \\nand acquiring knowledge, adaptivity provides a very simple abstraction for the implementation of artificial learning mechanisms: knowledge may be comfortably gathered by inserting and removing rules, and handled by tracking the evolution of the set of rules and by inter-preting the collected information as the representation of the knowledge encoded in the rule set.\\nMAIN FOCUS OF THIS ARTICLE\\nThis article provides concepts and foundations on adaptivity and adaptive technology, gives a general formulation for adaptive abstractions in use and indi-cates their main applications.\\nIt shows how rule-driven devices may turn into \\nadaptive devices to be applied in learning systems modeling, and introduces a recently formulated kind of adaptive abstractions having adaptive subjacent devices. This novel feature may be valuable for implementing meta-learning, since it enables adaptive devices to change dynamically the way they modify their own set of defining rules.\\nA significant amount of information concerning \\nadaptivity and related subjects may be found at the (LTA Web site).\\nBACKGROUND\\nThis section summarizes the foundations of adaptivity and establishes a general formulation for adaptive rule-driven devices (Neto, 2001), non-adaptivity being the only restriction imposed to the subjacent device.\\nSome theoretical background is desirable for \\nthe study and research on adaptivity and Adaptive Technology: formal languages, grammars, automata, computation models, rule-driven abstractions and related subjects. \\nNevertheless, either for programming purposes or \\nfor an initial contact with the theme, it may be unprob-lematic to catch the basics of adaptivity even having no prior expertise with computer-theoretical subjects. \\nIn adaptive abstractions, adaptivity may be achieved \\nby attaching adaptive actions to selected rules chosen ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c603b60-27e2-4f71-809b-e173cc6c1480', embedding=None, metadata={'page_label': '73', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Technology and Its Applications\\nfrom the rule set defining some subjacent non-adap -\\ntive device.\\nAdaptive actions enable adaptive devices to dynami-\\ncally change their behavior without external help, by \\nmodifying their own set of defining rules whenever their subjacent rule is executed. \\nFor practical reasons, up to two adaptive actions are \\nallowed: one to be performed prior to the execution of its underlying rule, and the other, after it.\\nAn adaptive device behaves just as it were piecewise \\nnon-adaptive: starting with the configuration of its initial underlying device, it iterates the following two steps, until reaching some well-defined final configuration:\\n \\n• While no adaptive action is executed, run the underlying device;\\n• Modify the set of rules defining the device by executing an adaptive action.\\nRule-Driven Devices\\nA rule-driven device  is any formal abstraction whose \\nbehavior is described by a rule set that maps each pos-sible configuration of the device into a corresponding next one. \\nA device is deterministic  when, for any configuration \\nand any input, a single next configuration is possible. Otherwise, it is said  non-deterministic . \\nNon-deterministic devices allow multiple valid \\npossibilities for each move, and require backtracking, so deterministic equivalents are usually preferable in practice. \\n Assume that:\\n• D is some rule-driven device, defined as \\n( ) A c S R C D, , , ,0 = .\\n• C is its set of possible configurations.\\n• { } ( ) C S C R × ∪ × ⊆ is the set of rules describ-\\ning its behavior, where e denotes empty stimulus, \\nrepresenting no events at all.\\n• S is its set of valid input stimuli.\\n• C c∈0 is its initial configuration.\\n• C A⊆ is its set of final configurations.\\nLet 1) (\\n+ ⇒ir\\ni c c (for short, 1+ ⇒i ic c) denote the ap-\\nplication of some rule ( ) R c s c ri i∈ =+1, ,  to the current configuration ci in response to some input stimulus \\n{ }∪ ∈S s ,  yielding its next configuration 1+ic. \\nSuccessive applications of rules in response to a \\nstream *S w∈  of input stimuli, starting from the initial \\nconfiguration c0 and leading to some final configuration \\nA c∈ is denoted c cw*\\n0⇒  (The star postfix operator in \\nthe formulae denotes the Kleene closure: its preceding element may be re-instantiated or reapplied an arbitrary number of times).\\nWe say that D defines a sentence w if, and only if, \\nc cw*\\n0⇒  holds for some A c∈. The collection L(D) of all \\nsuch sentences is called the language defined by D:\\n( ){ }A c c c S w D Lw ∈ ⇒ ∈ =, | **\\n0 .\\nAdaptive (Rule-Driven) Devices\\nAn adaptive rule-driven device ( ) AM ND AD,0 =  \\nassociates an initial subjacent rule-driven device \\n ( ) A c S NR C ND, , , ,0 0 0= , to some adaptive mechanism \\nAM, that can dynamically change its behavior by modi-\\nfying its defining rules.\\nThat is accomplished by executing non-null adap-\\ntive actions chosen from a set AA of adaptive actions , \\nwhich includes the null adaptive action a0.\\nA built-in counter t starts at 0 and is self-incre-\\nmented upon any adaptive actions’ execution. Let Xj \\ndenote the value of X after j executions of adaptive \\nactions by AD.\\nAdaptive actions in AA call functions that map AD \\ncurrent set ARt of adaptive rules into ARt+1 by inserting \\nto and removing adaptive rules ar from AM.\\nLet AR be the set of all possible sets of adaptive \\nrules for AD. Any A ak∈  maps the current set of rules \\n ∈tAR AR into ∈+1tAR AR: \\n:ka AR → AR\\nAM associates to each rule NR nrp∈  of AD underlying \\ndevice ND a pair of adaptive actions AA aa bap p∈ , :\\nAA NR AA AM× × ⊆', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70ee30d6-6c45-4d85-b5f0-cd1f8e155849', embedding=None, metadata={'page_label': '74', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Technology and Its Applications\\nANotation\\nWhen writing elementary adaptive actions, [ ]ar? ,  [ ]ar+  \\nand  [ ]ar− respectively denote searching, inserting and \\neliminating adaptive rules that follow template ar.\\nNote that ar may contain references to parameters, \\nvariables and generators, in order to allow cross-refer-\\nencing among elementary adaptive actions inside an adaptive function.\\nGiven an underlying rule \\nNR nrp∈ , we define an \\nadaptive rule  AM arp∈  as:\\n ( )p p p paa nr ba ar, , =\\nFor each AD move, AM applies some arp in three \\nsteps:\\na. execution of adaptive action bap\\n before applying \\nthe subjacent rule nrp; \\nb. application of the underlying non-adaptive rule \\nnrp; \\nc. execution of adaptive action aap.\\nThe following algorithm sketches the overall op-\\neration of AD:\\n \\n1. Initialize c0, w;\\n2. If w is exhausted, go to 7 else get next event st; \\n3. For the current configuration ct, determine the set \\nCR of ct-compatible rules;\\na. if CR = ∅, reject w.\\nb. if CR = ( ) { } c s c C Rt′, , , apply ( ) c s ct′, , as in steps \\n4-6, leading AD to c ct′=+1 .\\nc. if   ( ) { }1 , , , 1 , | , ,> = ∈ = = n n k C c c s c r CRk k\\ntk\\uf04b , \\napply all rules rk in parallel, as in steps 4-6, leading \\nAD to , , , ,2 1 nc c c\\uf04b  respectively.\\n4. If  ,0a bap=  go to 2, else apply first bap. If rule arp \\nwere removed by bap, go to 3 aborting arp,  else \\nAD reached an intermediate configuration, then \\ngo to 2.\\n5. Apply nrp to the current (intermediate) configura -\\ntion, yielding a new intermediate configuration;6. Apply aap, yielding the next (stable) configuration \\nfor AD; go to 2\\n7. If some F ct∈+1 was reached, then AD accepts \\nw, otherwise AD rejects w; stop.\\nHierarchical Multi-Level Adaptive \\nDevices \\nLet us define a more elaborated adaptive device by generalizing the definition above. Call non-adaptive devices level-0 devices ; define level-1 devices  those \\nhaving subjacent level-0 devices, to each of whose rules a pair of level-1 adaptive actions are attached.\\nLet the subjacent device be some level-k adaptive \\ndevice. One may construct a level-(k+1) device attach-ing a pair of level-(k+1) adaptive actions to each of its rules. This is the induction step for the definition of hi-erarchically structured multi-level adaptive devices.\\nBesides the set of rules defining the subjacent level- k \\ndevice, for k > 0, adaptive functions’ subjacent device \\nperforms at its own level, which may use level-( k+1) \\nadaptive actions to modify the behavior of level- k \\nadaptive functions.\\nSo, for k > 0, level-( k+1) devices can change the way \\ntheir subjacent level-k  devices modify themselves. That \\nalso holds for k = 1, since even for k = 0 the (empty) \\nset of adaptive functions still exists.\\nNotation\\nThe absence of adaptive actions in non-adaptive rules nr is explicitly expressed by stating all level-0 rules r\\n0 in \\nthe form ( )0 0a nr a. Therefore, level- k rules rk take the \\ngeneral format ( )k k ka r b1− , with both bk and ak level- k \\nadaptive actions for any adaptive level 0≥k .\\nSo, level-k adaptive devices have all their defining \\nrules stated in the standard form\\n ( )( ) ( )( ) ( ) ( )k k k k a a a a c c a b b b1 10 0\\n1 1 , ,− −′\\uf04b \\uf04b ,\\nwith\\n( )( )( ) ( ) ( )1 10 0\\n1 1 , ,− −′k k a a a c c a b b\\uf04b \\uf04b\\nrepresenting one of the rules defining the subjacent \\nlevel-( k – 1) adaptive device.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c5b6f8a-7770-48c5-83fc-d634bd28467c', embedding=None, metadata={'page_label': '75', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  Adaptive Technology and Its Applications\\nHence, level-i adaptive actions can modify both the \\nset of level- i adaptive rules and the set of elementary \\nadaptive actions defining level-(i – 1) adaptive func-\\ntions.\\nA SIMPLE ILLUSTRATIVE EXAMPLE\\nIn the following example, graphical notation is used \\nfor clarity and conciseness. When drawing automata, (as usual) circles represent states; double-line circles indicate final states; arrows indicate transitions; labels on the arrows indicate tokens consumed by the transition and (optionally) an associated adaptive action. When representing adaptive functions, automata fragments in brackets stand for a group of transitions to be added (+) or removed (-) when the adaptive action is applied.\\nFigure 1 shows the starting shape of an adaptive \\nautomaton that accepts a\\nnb2nc3n, n≥0. At state 1, it \\nincludes a transition consuming a, which performs \\nadaptive action A( ). \\nFigure 2 defines how A( ) operate: • Using state 2 as reference, eliminate empty transi-\\ntions using states x and y \\n• Add a sequence starting at x, with two transitions \\nconsuming b\\n• Append the sequence of two empty transitions sharing state 2\\nAppend a sequence with three transitions consuming c, \\nending at y.\\nFigure 3 shows the first two shape changes of this \\nautomaton after consuming the two first symbols a \\n(at state  1) in sentence a2b4c6. In its last shape, the \\nautomaton trivially consumes the remaining b4c6, and \\ndoes not change any more.\\nThere are many other examples of adaptive devices \\nin the references. This almost trivial and intuitive case was shown here for illustration purposes only.\\nKnowledge Representation\\nThe preceding example illustrates how adaptive devices use the set of rules as their only element for represent-ing and handling knowledge. \\nA rule (here, a transition) may handle parametric \\ninformation in its components (here, the transition’s origin and destination states, the token labeling the transition, the adaptive function it calls, etc.). \\nRules may be combined together in order to represent \\nsome non-elementary information (here, the sequences of transitions consuming tokens “b” and “c” keep track of the value of n in each particular sentence). This way, rules and their components may work and may be in-terpreted as low-level elements of knowledge. \\nAlthough being impossible to impose rules on how \\nto represent and handle knowledge in systems repre-\\n1 2 3\\na /A()ε ε1 2 3\\na /A()ε εFigure 1. Initial configuration of the illustrative adap -\\ntive automaton\\nFigure 2. Adaptive function A ( )\\nx 2ε? [ ]\\n2 yε? [ ]\\nx 2 yεε– [ ]\\n2εε+ [ ]xb byc c c{ A() =\\n}x 2ε? [ ]x 2ε? [ ]\\n2 yε? [ ]2 yε? [ ]\\nx 2 yεε– [ ]x 2 yεε– [ ]\\n2εε+ [ ]xb byc c c\\n2εε+ [ ]xb byc c c{ A() =\\n}', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62b50746-1feb-4f57-a764-ac193569fe6e', embedding=None, metadata={'page_label': '76', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Technology and Its Applications\\nA\\nsented with adaptive devices, the details of the learning \\nprocess may be chosen according to the particular needs of each system being modeled.\\nIn practice, the learning behavior of an adaptive \\ndevice may be identified and measured by tracking the progress of the set of rules during its operation and interpreting the dynamics of its changes.\\nIn the above example, when transitions are added to \\nthe automaton by executing adaptive action\\n A ( ), one \\nmay interpret the length of the sequence of transitions consuming “b” (or “c”) as a manifestation of the knowl -\\nedge that is being gathered by the adaptive automaton on the value of n (its exact value becomes available after the sub-string of tokens “a” is consumed).\\n \\nFUTURE TRENDS\\nAdaptive abstractions represent a significant theoreti-cal advance in Computer Science, by introducing and exploring powerful non-classical concepts such as: time-varying behavior, autonomously dynamic rule sets, multi-level hierarchy, static and dynamic adap-tive actions.\\nThose concepts allow establishing a modeling style, \\nproper for describing complex learning systems, for efficiently solving traditionally hard problems, for dealing with self-modifying learning methods, and for providing computer languages and environments for comfortable elaboration of quality programs with dynamically-variant behavior. All those features are vital for conceiving, modeling, \\ndesigning and implementing applications in Artificial Intelligence, which benefits from adaptivity while expressing traditionally difficult-to-describe Artificial Intelligence facts.\\nListed below are features Adaptive Technology \\noffers to several fields of Computation, especially to Artificial Intelligence-related ones, indicating their main impacts and applications. \\n• Adaptive Technology provides a true computation \\nmodel, constructed around formal foundations. Most Artificial Intelligence techniques in use are very hard to express and follow since the connection between elements of the models and information they represent is often implicit, so their operation reasoning is difficult for a human to track and plan. Adaptive rule-driven devices concentrate all stored knowledge in their rules, and the whole logic that handles such information, in their adaptive actions. Such properties open for Artificial Intelligence the possibility to observe, understand and control adaptive-device-modeled phenomena. By following and interpreting how and why changes occur in the device set of rules, and by tracking semantics of adaptive actions, one can infer the reasoning of the model reactions to its input. \\n• Adaptive devices have enough processing power to model complex computations. In (Neto, 2000) some well-succeeded use cases are shown with Figure 3. Configurations of the adaptive automaton after executing A ( ) once and twice\\n11 2 3\\na /A()ε εb b c c c\\na /A()ε\\nε2b b b b\\n3c c c c c c', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ead3ddff-ab2d-4413-a967-81151708feb5', embedding=None, metadata={'page_label': '77', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Technology and Its Applications\\nsimple and efficient adaptive devices used instead \\nof complex traditional formulations.\\n• Adaptive Devices are Turing Machine-equiva-lent computation models that may be used in the construction of single-notation full specifications of programming languages, including lexical, syntactical, context-dependent static-semantic is-sues, language built-in features such as arithmetic operations, libraries, semantics, code generation and optimization, run-time code interpreting, etc.\\n• Adaptive devices are well suited for representing complex languages, including idioms. Natural language particularly require several features to be expressed and handled, as word inflexions, or -\\nthography, multiple syntax forms, phrase ordering, ellipsis, permutation, ambiguities, anaphora and others. A few simple techniques allow adaptive devices to deal with such elements, strongly sim-plifying the effort of representing and processing them. Applications are wide, including machine translation, data mining, text-voice and voice-text conversion, etc. \\n• Computer art is another fascinating potential application of adaptive devices. Music and other artistic expressions are forms of human language. Given some language descriptions, computers can capture human skills and automatically generate interesting outputs. Well-succeeded experiments were carried out in the field of music, with excel-lent results (Basseto, 1999).\\n• Decision-taking systems may use Adaptive Deci-sion Tables and Trees for constructing intelligent systems that accept training patterns, learn how to classify them, and therefore, classify unknown patterns. Well-succeeded experiments include: classifying geometric patterns, decoding sign languages, locating patterns in images, generat-ing diagnoses from symptoms and medical data, etc.\\n• Language inference uses Adaptive Devices to generate formal descriptions of languages from samples, by identifying and collecting structural information and generalizing on the evidence of repetitive or recursive constructs (Matsuno, 2006). \\n• Adaptive Devices can be used for learning pur-poses by storing as rules the gathered information on some monitored phenomenon. In educational systems, the behavior of both students and train-ers can be inferred and used to decide how to proceed.\\n• One can construct Adaptive Devices whose underlying abstraction is a computer language. Statements in such languages may be considered as rules defining behavior of a program. By at-taching adaptive rules to statements, the program becomes self-modifiable. Adaptive languages are needed for adaptive applications to be expressed naturally. For adaptivity to become a true pro-gramming style, techniques and methods must be developed to construct good adaptive software, since adaptive applications developed so far were usually produced in strict ad-hoc way.\\nCONCLUSION \\nAdaptive Technology concerns techniques, methods and subjects referring to actual application of adaptivity. \\nAdaptive automata (Neto, 1994) were first proposed \\nfor practical representation of context-sensitive lan-guages (Rubinstein, 1995). Adaptive grammars (Iwai, 2000) were employed as its generative counterpart (Burshteyn, 1990), (Christiansen, 1990), (Cabasino, 1992), (Shutt, 1993), (Jackson, 2006). \\nFor specification and analysis of real time reactive \\nsystems, works were developed based on adaptive versions of statecharts (Almeida Jr., 1995), (Santos, 1997). An interesting confirmation of power and usability of adaptive devices for modeling complex systems (Neto, 2000) was the successful use of Adap-tive Markov Chains in a computer music-generating device (Basseto, 1999). \\nAdaptive Decision Tables (Neto, 2001) and Adap-\\ntive Decision Trees (Pistori, 2006) are nowadays being experimented in decision-taking applications. \\nExperiments have been reported that explore the \\npotential of adaptive devices for constructing language inference systems (Neto, 1998), (Matsuno, 2006). \\nAn important area in which adaptive devices shows \\nits strength is the specification and processing of natural languages (Neto, 2003). Many other results are being achieved while representing syntactical context-depen-dencies of natural language. \\nSimulation and modeling of intelligent systems are \\nother concrete applications of adaptive formalisms, as illustrated in the description of the control mechanism ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5fad5032-bf15-47f7-b15e-dd77435971f9', embedding=None, metadata={'page_label': '78', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Adaptive Technology and Its Applications\\nAof an intelligent autonomous vehicle which collects \\ninformation from its environment and builds maps for navigation. \\nMany other applications for adaptive devices are \\npossible in several fields. \\nREFERENCES\\n(* or ** - downloadable from LTA Website; ** - in Portuguese only)\\nAlmeida Jr., J.R. (1995)**. STAD - Uma ferramenta \\npara representação e simulação de sistemas através de \\nstatecharts adaptativos.  São Paulo, 202p. Doctoral The-\\nsis. Escola Politécnica, Universidade de São Paulo.\\nBasseto, B.A., Neto, J.J. (1999)*. A stochastic musi-\\ncal composer based on adaptive algorithms. Anais do \\nXIX Congresso Nacional da Sociedade Brasileira de \\nComputação. SBC-99, V ol. 3, pp. 105-13.\\nBurshteyn, B. (1990). Generation and recognition \\nof formal languages by modifiable grammars. ACM \\nSIGPLAN Notices, v.25, n.12, p.45-53, 1990.\\nCabasino, S.; Paolucci, P.S.; Todesco, G.M. (1992). \\nDynamic parsers and evolving grammars. ACM SIG-\\nPLAN Notices, v.27, n.11, p.39-48, 1992.\\nChristiansen, H. (1990). A survey of adaptable gram-\\nmars. ACM SIGPLAN Notices, v.25, n.11, p.33-44. Iwai, M.K. (2000)**. Um formalismo gramatical ad-\\naptativo para linguagens dependentes de contexto.  São \\nPaulo 2000, 191p. Doctoral Thesis. Escola Politécnica, \\nUniversidade de São Paulo.\\nJackson, Q.T. (2006) . Adapting to Babel – Adaptivity \\nand context-sensitivity parsing: from a\\nnbncn to RNA – A \\nThotic Technology Partners Research Monograph.LTA Website: http://www.pcs.usp.br/~lta Matsuno, I.P. (2006)**. Um Estudo do Processo de In-\\nferência de Gramáticas Regulares e Livres de Contexto \\nBaseados em Modelos Adaptativos.  M.Sc. Dissertation, \\nEscola Politécnica, Universidade de São Paulo.\\nNeto, J.J.; Moraes, M.de. (2003)* Using Adaptive \\nFormalisms to Describe Context-Dependencies in \\nNatural Language. Computational Processing of the \\nPortuguese Language 6th International Workshop, PROPOR 2003,  LNAI V olume 2721, Faro, Portugal, June 26-27, Springer-Verlag, 2003, pp 94-97.\\nNeto, J. J. (2001)*. Adaptive Rule-Driven Devices - \\nGeneral Formulation and Case Study . Lecture Notes \\nin Computer Science. Watson, B.W. and Wood, D. \\n(Eds.): Implementation and Application of Automata - 6th International Conference, CIAA 2001, V ol.2494, Pretoria, South Africa, July 23-25, Springer-Verlag, 2001, pp. 234-250.\\nNeto, J.J. (1994)*. Adaptive automata for context-\\ndependent languages. ACM SIGPLAN Notices, v.29, \\nn.9, p.115-24, 1994.\\nNeto, J.J. (2000)*. Solving Complex Problems Ef-\\nficiently with Adaptive Automata. CIAA 2000 - Fifth \\nInternational Conference on Implementation and Ap-plication of Automata - London, Ontario, Canada.\\nNeto, J.J., Iwai, M.K. (1998)*. Adaptive automata for \\nsyntax learning. XXIV Conferencia Latinoamericana \\nde Informática CLEI’98, Quito - Ecuador, tomo 1, pp.135-146. \\nPistori, H.; Neto, J.J.; Pereira, M.C. (2006)* Adaptive \\nNon-Deterministic Decision Trees: General Formula-\\ntion and Case Study. INFOCOMP Journal of Computer Science, Lavras, MG.\\nRubinstein, R.S.; Shutt. J.N. (1995). Self-modifying \\nfinite automata: An introduction , Information process-\\ning letters, v.56, n.4, 24, p.185-90.Santos, J.M.N. (1997)**. Um formalismo adaptativo \\ncom mecanismos de sincronização para aplicações \\nconcorrentes. São Paulo, 98p. M.Sc. Dissertation. Escola Politécnica, Universidade de São Paulo.\\nShutt, J.N. (1993). Recursive adaptable grammar . \\nM.S. Thesis, Computer Science Department, Worcester \\nPolytechnic Institute, Worcester MA.\\nKEy TERMS\\nAdaptivity: Property exhibited by structures that \\ndynamically and autonomously change their own be-havior in response to input stimuli.\\nAdaptive Computation Model: Turing-powerful \\nabstraction that mimic the behavior of potentially self-modifying complex systems.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b60d9ad-ae96-44d9-8ac9-ceb163015aae', embedding=None, metadata={'page_label': '79', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Adaptive Technology and Its Applications\\nAdaptive Device: Structure with dynamic be-\\nhavior, with some subjacent device and an adaptive \\nmechanism.\\nAdaptive Functions and Adaptive Actions:  Adap-\\ntive actions are calls to adaptive functions, which can determine changes to perform on its layer’s rule set and on their immediately subjacent layer’s adaptive functions. \\nAdaptive Mechanism: Alteration discipline as-\\nsociated to an adaptive device’s rule set that change the behavior of its subjacent device by performing adaptive actions.\\nAdaptive Rule-Driven Device: Adaptive device \\nwhose behavior is defined by a dynamically changing set of rules, e.g. adaptive automata, adaptive gram-mars, etc.\\nContext-Dependency: Reinterpretation of terms, \\ndue to conditions occurring elsewhere in a sentence, e.g. agreement rules in English, type-checking in Pascal. Context-Sensitive (-Dependent) Formalism: \\nAbstraction capable of representing Chomsky type-1 or type-0 languages. Adaptive Automata and Adaptive Context-free Grammars are well suited to express such languages.\\nHierarchical (Multilevel) Adaptive Device: \\nStratified adaptive structures whose involving layer’s adaptive actions can modify both its own layer’s rules and its underlying layer’s adaptive functions. \\nSubjacent (or Underlying) Device: Any device \\nused as basis to formulate adaptive devices. The in-nermost of a multilevel subjacent device must be non-adaptive.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67d8aa4f-ca26-481c-8cdf-f31e001c03a9', embedding=None, metadata={'page_label': '80', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAdvanced Cellular Neural Networks Image\\nProcessing\\nJ. Álvaro Fernández\\nUniversity of Extremadura, Badajoz, Spain\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nSince its introduction to the research community in 1988, the Cellular Neural Network (CNN) (Chua & Yang, 1988) paradigm has become a fruitful soil for engineers and physicists, producing over 1,000 published scientific papers and books in less than 20 years (Chua & Roska, 2002), mostly related to Digital Image Processing (DIP). This Artificial Neural Net -\\nwork (ANN) offers a remarkable ability of integrating complex computing processes into compact, real-time programmable analogic VLSI circuits as the ACE16k (Rodríguez et al., 2004) and, more recently, into FPGA \\ndevices (Perko et al. , 2000).\\nCNN is the core of the revolutionary Analogic \\nCellular Computer (Roska et al., 1999), a program-\\nmable system based on the so-called CNN Universal Machine (CNN-UM) (Roska & Chua, 1993). Analogic CNN computers mimic the anatomy and physiology of many sensory and processing biological organs (Chua & Roska, 2002).\\nThis article continues the review started in this \\nEncyclopaedia under the title Basic Cellular Neural \\nNetwork Image Processing .\\nBACKGROUND\\nThe standard CNN architecture consists of an M × N \\nrectangular array of cells C(i,j) with Cartesian coordi-\\nnates (i,j), i = 1, 2, …, M, j = 1, 2, …, N. Each cell or \\nneuron C(i,j) is bounded to a sphere of influence Sr(i,j) \\nof positive integer radius r, defined by:\\n{ }\\n1 ,1( , ) ( , ) , max r\\nk M l NS i j C k l k i l j r\\n≤ ≤ ≤ ≤\\uf8f1 \\uf8fc \\uf8f4 \\uf8f4= − − ≤ \\uf8f2 \\uf8fd\\n\\uf8f4 \\uf8f4 \\uf8f3 \\uf8fe\\n      (1)\\nThis set is referred as a (2 r +1) × (2r +1) neigh-\\nbourhood. The parameter r controls the connectivity of a cell. When r > N /2 and M = N, a fully connected \\nCNN is obtained, a case that corresponds to the classic Hopfield ANN model.\\nThe state equation of any cell C(i,j) in the M × N \\narray structure of the standard CNN may be described by:\\n[ ]\\n( , ) ( , )( ) 1( ) ( , ; , ) ( ) ( , ; , )\\nrij\\nij kl kl ij\\nC k l S i jdz tC z t A i j k l y t B i j k l x Idt R ∈= − + ⋅ + ⋅ + ∑\\n      (2)\\nwhere C and R are values that control the transient \\nresponse of the neuron circuit (just like an RC filter), I \\nis generally a constant value that biases the state matrix Z = {z\\nij}, and Sr is the local neighbourhood defined in \\n(1), which controls the influence of the input data X = \\n{xij} and the network output Y = {yij} for time t.\\nThis means that both input and output planes interact \\nwith the state of a cell through the definition of a set of real-valued weights, A(i, j; k, l) and B(i, j; k, l), whose \\nsize is determined by r. The cloning templates A and \\nB are called the feedback and feed-forward operators, respectively.\\nAn isotropic CNN is typically defined with constant \\nvalues for r, I, A and B, implying that for an input image \\nX, a neuron C(i, j) is provided for each pixel ( i, j), with \\nconstant weighted circuits defined by the feedback and feed-forward templates A and B. The neuron state value \\nz\\nij is adjusted with the bias parameter I, and passed as \\ninput to an output function of the form:\\n( )1( ) 1 ( ) 12ij ij ij y z t z t= + − −\\n  (3)\\nThe vast majority of the templates defined in the \\nCNN-UM template compendium of (Chua & Roska, 2002) are based on this isotropic scheme, using r = 1 \\nand binary images in the input plane. If no feedback (i.e. A = 0) is used, then the CNN behaves as a convolu-\\ntion network, using B as a spatial filter, I as a threshold \\nand the piecewise linear output ( 3) as a limiter. Thus, ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28d6f9ff-388d-4134-9622-5bf9d41e3e0d', embedding=None, metadata={'page_label': '81', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Advanced Cellular Neural Networks Image Processing\\nvirtually any spatial filter from DIP theory can be \\nimplemented on such a feed-forward CNN, ensuring binary output stability via the definition of a central feedback absolute value greater than 1.\\nADVANCED CNN IMAGE PROCESSING\\nIn this section, a description of more complex CNN models is performed in order to provide a deeper insight into CNN design, including multi-layer structures and nonlinear templates, and also to illustrate its powerful DIP capabilities.\\nNonlinear Templates\\nA problem often addressed in DIP edge detection is the robustness against noise (Jain, 1989). In this sense, the EDGE CNN detector for grey-scale images given by\\nA = 2, 1 1 1\\n1 8 11 1 1 EDGEB− − −\\uf8ee \\uf8f9\\n\\uf8ef \\uf8fa= − −\\uf8ef \\uf8fa\\uf8ef \\uf8fa− − −\\uf8f0 \\uf8fb, I = -0.5\\n      (4)\\nis a typical example of a weak-against-noise filter, as a \\nresult of fixed linear feed-forward template combined with excitatory feedback. One way to provide the detector with more robustness against noise is via the definition of a nonlinear B template of the form:\\n0CONTOURb b b\\nB b b\\nb b b\\uf8ee \\uf8f9\\n\\uf8ef \\uf8fa=\\uf8ef \\uf8fa\\uf8ef \\uf8fa\\uf8f0 \\uf8fb where 0.5\\n1ij kl\\nij klx x th\\nb\\nx x th\\uf8f1 − >\\uf8f4=\\uf8f2− − ≤\\uf8f4\\uf8f3                   (5)\\nThis nonlinear template actually defines different \\ncoefficients for the surrounding pixels prior to perform \\nthe spatial filtering of the input image X. Thus, a CNN \\ndefined with nonlinear templates is generally dependent of X, and can not be treated as an isotropic model.\\nJust two values for the surrounding coefficients of B \\nare allowed: one excitatory for greater than a threshold th luminance differences with the central pixel (i.e. edge \\npixels), and the other inhibitory, doubled in absolute value, for similar pixels, where th is usually set around 0.5. The feedback template A = 2 remains unchanged, \\nbut the value for the bias I must be chosen from the \\nfollowing analysis:\\nFor a given state z\\nij element, the contribution wij \\nof the feed-forward nonlinear filter of ( 5) may be \\nexpressed as:\\n( )1.0 0.5\\n8 0.5\\n8 1.5ij s e\\ne e\\new p p\\np p\\np= − ⋅ + ⋅\\n= − − + ⋅\\n= − + ⋅   (6)\\nwhere ps is the number of similar pixels in the 3 × 3 \\nneighbourhood and pe the rest of edge pixels. E.g. if \\nthe central pixel has 8 edge neighbours, wij = 12 – 8 = \\n4, whereas if all its neighbours are similar to it, then w\\nij = –8. Thus, a pixel will be selected as edge depend-\\ning on the number of its edge neighbours, providing the possibility of noise reduction. For instance, edge detection for pixels with at least 3 edge neighbours forces that I ∈ (4, 5).\\nThe main result is that the inclusion of nonlinearities \\nin the definition of B coefficients and, by extension, \\nthe pixel-wise definition of the main CNN parameters gives rise to more powerful and complex DIP filters (Chua & Roska, 1993). \\nMorphologic Operators\\nMathematical Morphology is an important contributor to the DIP field. In the classic approach, every morpho -\\nlogic operator is based on a series of simple concepts from Set Theory. Moreover, all of them can be divided into combinations of two basic operators: erosion and dilation (Serra, 1982). Both operators take two pieces of data as input: the binary input image and the so-called structuring element, which is usually represented by a 3×3 template. \\nA pixel belongs to an object if it is active (i.e. its \\nvalue is 1 or black), whereas the rest of pixels are classified as background, zero-valued elements. Basic morphologic operators are defined using only object pixels, marked as 1 in the structuring element. If a pixel is not used in the match, it is left blank. Both dilation and erosion operators may be defined by the structuring elements', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b8cbe59-ce71-4272-8b6c-553a60e427c4', embedding=None, metadata={'page_label': '82', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Advanced Cellular Neural Networks Image Processing\\nA1 1 1\\n1 1 11 1 1\\n and 1\\n1 1 1\\n1\\n  (7)\\nfor 8 or 4-neighbour connectivity, respectively. In \\ndilation, the structuring element is placed over each input pixel. If any of the 9 (or 5) pixels considered in (7) is active, then the output pixel will be also active (Jain, 1989). The erosion operator can be defined as the dual of dilation, i.e. a dilation performed over the background. \\nMore complex morphologic operators are based \\non structuring elements that also contains background pixels. This is the case of the Hit and Miss Transform (HMT), a generalized morphologic operator used to identify certain local pixel configurations. For instance, the structuring elements defined by\\n0 1\\n0 1 10 0 0\\n and 1 1\\n0 1 0\\n0 0 0\\n  (8)\\nare used to find 90º convex corner object pixels within the image. A pixel will be selected as active in the output image if its local neighbourhood exactly matches with that defined by the structuring element. However, in order to calculate a full, non-orientated corner detector it will be necessary to perform 8 HMT, one for each rotated version of ( 8), OR-ing the 8 intermediate output \\nimages to obtain the final image (Fisher et al., 2004).\\nIn the CNN context, the HMT may be obtained in \\na straightforward manner by:\\nA = 2, \\n1 1:0 otherwiseij\\nHMT ijsB b= \\uf8f1=\\uf8f2\\n\\uf8f3, 0.5s I p= −  \\n      \\n      (9)\\nwhere S = {sij} is the structuring element and ps is the \\ntotal number of active pixels in it.\\nSince the input template B of the HTM CNN is \\ndefined via the structuring element S, and given that \\nthere are 29 = 512 distinct 3 × 3 possible structuring \\nelements, there will also be 512 different hit-and-miss \\nerosions. For achieving the opposite result, i.e. hit-and-miss dilation, the threshold must be the opposite of that in (9) (Chua & Roska, 2002).Dynamic Range Control CNN and Piecewise Linear Mappings\\nDIP techniques can be classified by the domain where they operate: the image or spatial domain or the transform domain (e.g. the Fourier domain). Spatial domain techniques are those who operate directly over the pixels within an image (e.g. its intensity level). A generic spatial operator can be defined by\\n[ ] ( , ) ( , )\\nrSY i j T X i j =   (10)\\nwhere X and Y are the input and output images, re-\\nspectively, and T is a spatial operator defined over a \\nneighbourhood Sr around each pixel X(i, j), as defined \\nin (1). Based on this neighbourhood, spatial operators can be grouped into two types: Single Point Process-ing Operators, also known as Mapping Operators, and Local Processing Operators, which can be defined by a spatial filter (i.e. 2D-discrete convolution) mask (Jain, 1989).\\nThe simplest form of T is obtained when S\\nr is 1 pixel \\nsize. In this case, Y only depends of the intensity value \\nof X for every pixel and T becomes an intensity level \\ntransformation function, or mapping, of the form\\ns = T(r)     (11)\\nwhere r and s are variables that represent grey \\nlevel in X and Y, respectively.\\nAccording to this formulation, mappings can be \\nachieved by direct application of a function over a \\nrange of input intensity levels. By properly choosing the form of T, a number of effects can be obtained, as \\nthe grey-level inversion, dynamic range compression or expansion (i.e. contrast enhancement), and threshold binarization for obtaining binary masks used in analysis and morphologic DIP.\\nA mapping is linear if its function T is also linear. \\nOtherwise, T is not linear and the mapping is also non-\\nlinear. An example of nonlinear mapping is the CNN output function (3). It consists of three linear segments: two saturated levels, –1 and +1, and the central linear segment with unitary slope that connects them. This function is said to be piecewise linear and is closely related to the well-known sigmoid function utilized in the Hopfield ANN (Chua & Roska, 1993). It performs a mapping of intensity values stored in Z in the [–1, ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72fbca11-c43c-4adc-8e9d-d59355f862f4', embedding=None, metadata={'page_label': '83', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Advanced Cellular Neural Networks Image Processing\\n+1] range. The bias I controls the average point of the \\ninput range, where the output function gives a zero-\\nvalued outcome.\\nStarting from the original CNN cell or neuron \\n(1)-(3), a brief review of the Dynamic Range Control (DRC) CNN model first defined in (Fernández et al., \\n2006) follows. This network is designed to perform a piecewise linear mapping T over X, with input range \\n[m–d, m+d] and output range [a, b]. Thus,\\n( )( )\\n( )( ) ( )\\n( ),\\n, , ,2 2\\n,a X i j m d\\nb a b aT X i j X i j m m d X i j m dd\\nb m d X i j−∞ < ≤ − \\uf8f1\\n\\uf8f4− + \\uf8f4= \\uf8ee \\uf8f9 − + − < ≤ + \\uf8f2 \\uf8f0 \\uf8fb\\uf8f4\\uf8f4 + < < +∞\\uf8f3\\n      (12)\\nIn order to be able to implement this function in \\na multi-layer CNN, the following constraints must be met:\\n2 b a− ≤  and 1d≤   (13)\\nA CNN cell which controls the desired input range \\ncan be defined with the following parameters:\\nA1 = 0, B1 = 1/d, I1 = -m/d  (14)\\nThis network performs a linear mapping between \\n[m–d, m+d] and [–1,+1]. Its output is the input of a \\nsecond CNN whose parameters are:A\\n2 = 0, B2 = (b – a)/2, I2 = (b + a)/2 (15)\\nThe output of this second network is exactly the \\nmapping T defined in (12) bounded by the constraints \\nof (13).\\nOne of the simplest techniques used in grey-scale \\nimage contrast enhancement is contrast stretching or \\nnormalization. This technique maximizes the dynamic range of the intensity levels within the image from suitable estimates of the maximum and minimum in-tensity values (Fisher et al., 2004). Thus, in the case \\nof normalized grey-scale images, where the minimum (i.e. black) and maximum (i.e. white) intensity levels are represented by 0 and 1 values, respectively; if such an image with dynamic intensity range [ f, g] ⊆ [0, +1] \\nis fed in the input of the 2-layer CNN defined by ( 14) \\nand (15), the following parameters will achieve the desired linear dynamic range maximization:a = 0, b = 1, m = (g + f)/2, d = (g – f)/2      (16)\\nThe DRC network can be easily applied to a first \\norder piecewise polynomial approximation of nonlinear, continuous mappings. One of the valid possibilities is the multi-layer DRC CNN implementation of error-controlled Chebyshev polynomials, as described in (Fernández et al., 2006). The possible mappings include, \\namong many others, the absolute value, logarithmic, exponential, radial basis and integer and real-valued power functions.\\nFUTURE TRENDS\\nThere is a continuous quest by engineers and special-ists: compete with and imitate nature, especially some “smart” animals. Vision is one particular area which computer engineers are interested in. In this context, the so-called Bionic Eye (Werblin et al., 1995) embedded \\nin the CNN-UM architecture is ideal for implementing many spatio-temporal neuromorphic models. \\nWith its powerful image processing toolbox and \\na compact VLSI implementation (Rodríguez et al., \\n2004), the CNN-UM can be used to program or mimic different models of retinas and even combinations of them. Moreover, it can combine biologically based models, biologically inspired models, and analogic artificial image processing algorithms. This combina-tion will surely bring a broader kind of applications and developments. \\nCONCLUSION \\nA number of other advances in the definition and characterization of CNN have been researched in the past decade. This includes the definition of methods for designing and implementing larger than 3 ×3 neigh-\\nbourhoods in the CNN-UM (Kék & Zarándy, 1998), the CNN implementation of some image compression techniques (Venetianer et al., 1995) or the design of \\na CNN-based Fast Fourier Transform algorithm over analogic signals (Perko et al., 1998), between many \\nothers.\\nIn this article, a general review of the main properties \\nand features of the Cellular Neural Network model has been addressed focusing on its DIP applications. The ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='647b0ab7-9b9e-4773-adc1-3ddecff597a2', embedding=None, metadata={'page_label': '84', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Advanced Cellular Neural Networks Image Processing\\nACNN is now a fundamental and powerful toolkit for \\nreal-time nonlinear image processing tasks, mainly due to its versatile programmability, which has powered its hardware development for visual sensing applications (Roska et al., 1999). \\nREFERENCES\\nChua, L.O., & Roska, T. (2002). Cellular Neural \\nNetworks and Visual Computing. Foundations and Applications . Cambridge, UK: Cambridge University \\nPress.\\nChua, L.O., & Roska, T. (1993). The CNN Paradigm. \\nIEEE Transactions on Circuits and Systems I: Funda-mental Theory and Applications , 40, 147–156.\\nChua, L.O., & Yang, L. (1988). Cellular Neural Net-works: Theory and Applications. IEEE Transactions \\non Circuits and Systems, 35, 1257–1290.\\nFernández, J.A., Preciado, V .M., & Jaramillo, M.A. \\n(2006). Nonlinear Mappings with Cellular Neural Networks. Lecture Notes in Computer Science, 4177, \\n350–359.\\nFisher, R., Perkins, S., Walker, A., & Wolfart, E. (2004). \\nHypermedia Image Processing Reference (HIPR2). Website: http://homepages.inf.ed.ac.uk/rbf/HIPR2, University of Edinburgh, UK.\\nJain, A.K. (1989). Fundamentals of Digital Image \\nProcessing. Englewood Cliffs, NJ, USA: Prentice-\\nHall.\\nKék, L., & Zarándy, A. (1998). Implementation of Large \\nNeighborhood Non-Linear Templates on the CNN Universal Machine. International Journal of Circuit \\nTheory and Applications,  26, 551-566.\\nPerko, M., Fajfar, I., Tuma, T., & Puhan, J. (1998). Fast Fourier Transform Computation Using a Digital CNN Simulator. 5\\nth IEEE International Workshop \\non Cellular Neural Network and Their Applications  \\nProceedings , 230-236.\\nPerko, M., Fajfar, I., Tuma, T., & Puhan, J. (2000). Low-Cost, High-Performance CNN Simulator Imple-mented in FPGA. 6\\nth IEEE International Workshop \\non Cellular Neural Network and Their Applications  \\nProceedings , 277-282.Rodríguez, A., Liñán, G., Carranza, L., Roca, E., Carmona, R., Jiménez, F., Domínguez, R., & Espejo, S. (2004). ACE16k: The Third Generation of Mixed-Signal SIMD-CNN ACE Chips Toward VSoCs. IEEE \\nTransactions on Circuits and Systems I: Regular Papers , \\n51, 851–863.\\nRoska, T., & Chua, L.O. (1993). The CNN Universal \\nMachine: An Analogic Array Computer. IEEE Transac-\\ntions on Circuits and Systems II: Analog and Digital Processing, 40, 163–173.\\nRoska, T., Zarándy, Á., Zöld, S., Földesy, P., & Szolgay, \\nP. (1999). The Computational Infrastructure of Ana-logic CNN Computing – Part I: The CNN-UM Chip Prototyping System. IEEE Transactions on Circuits \\nand Systems I: Fundamental Theory and Applications , \\n46, 261–268.\\nSerra, J. (1982). Image Analysis and Mathematical \\nMorphology. London, UK: Academic Press.Venetianer, P.L., Werblin, F., Roska, T., & Chua, L.O. \\n(1995). Analogic CNN Algorithms for Some Image Compression and Restoration Tasks. IEEE Transac-\\ntions on Circuits and Systems I: Fundamental Theory and Applications,  42, 278-284.\\nWerblin, F., Roska, T., & Chua, L.O. (1995). The Analogic Cellular Neural Network as a Bionic Eye. International Journal of Circuit Theory and Applica-tions, 23, 541-569.\\nKEy TERMS\\nBionics: The application of methods and systems \\nfound in nature to the study and design of engineering systems. The word seems to have been formed from “biology” and “electro nics” and was first used by J. \\nE. Steele in 1958.\\nChebyshev Polynomial:  An important type of \\npolynomials used in data interpolation, providing the best approximation of a continuous function under the maximum norm.\\nDynamic Range: A term used to describe the ratio \\nbetween the smallest and largest possible values of a variable quantity.\\nFPGA: Acronym that stands for Field-Program-\\nmable Gate Array, a semiconductor device invented ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e2917e91-1948-45e4-821d-9a1c8d0b8f40', embedding=None, metadata={'page_label': '85', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  Advanced Cellular Neural Networks Image Processing\\nin 1984 by R. Freeman that contains programmable \\ninterfaces and logic components called “logic blocks” used to perform the function of basic logic gates (e.g. XOR) or more complex combination functions such as decoders.\\nPiecewise Linear Function: A function f(x) that \\ncan be split into a number of linear segments, each of which is defined for a non-overlapping interval of x.\\nSpatial Convolution:  A term used to identify the \\nlinear combination of a series of discrete 2D data (a digital image) with a few coefficients or weights. In the Fourier theory, a convolution in space is equivalent to (spatial) frequency filtering.\\nTemplate: Also known as kernel, or convolution \\nkernel, is the set of coefficients used to perform a spa-tial filter operation over a digital image via the spatial convolution operator.\\nVLSI: Acronym that stands for Very Large Scale \\nIntegration. It is the process of creating integrated cir-cuits by combining thousands (nowadays hundreds of millions) of transistor-based circuits into a single chip. A typical VLSI device is the microprocessor.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='20ef3865-8f28-461b-a28a-66a874c1fbc6', embedding=None, metadata={'page_label': '86', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAgent-Based Intelligent System Modeling\\nZaiyong Tang\\nSalem State College, USA\\nXiaoyu Huang\\nUniversity of Shanghai for Science & Technology, China\\nKallol Bagchi\\nUniversity of Texas at El Paso, USA\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nAn intelligent system is a system that has, similar to a living organism, a coherent set of components and subsystems working together to engage in goal-driven activities. In general, an intelligent system is able to sense and respond to the changing environment; gather and store information in its memory; learn from earlier experiences; adapt its behaviors to meet new challenges; and achieve its pre-determined or evolving objectives. The system may start with a set of predefined stimulus-response rules. Those rules may be revised and improved through learning. Anytime the system encounters a situation, it evaluates and selects the most appropriate rules from its memory to act upon. \\nMost human organizations such as nations, \\ngovernments, universities, and business firms, can be considered as intelligent systems. In recent years, researchers have developed frameworks for building organizations around intelligence, as opposed to traditional approaches that focus on products, processes, or functions (e.g., Liang, 2002; Gupta and Sharma, 2004).  Today’s organizations must go beyond traditional goals of efficiency and effectiveness; they need to have organizational intelligence in order to adapt and survive in a continuously changing environment (Liebowitz, 1999). The intelligent behaviors of those organizations include monitoring of operations, listening and responding to stakeholders, watching the markets, gathering and analyzing data, creating and disseminating knowledge, learning, and effective decision making.\\nModeling intelligent systems has been a challenge \\nfor researchers. Intelligent systems, in particular, those involve multiple intelligent players, are complex systems where system dynamics does not follow clearly defined rules. Traditional system dynamics approaches or statistical modeling approaches rely on rather restrictive assumptions such as homogeneity of individuals in the system. Many complex systems have components or units which are also complex systems. This fact has significantly increased the difficulty of modeling intelligent systems. Agent-based modeling of complex systems such as ecological systems, stock market, and disaster recovery has recently garnered significant research interest from a wide spectrum of fields from politics, economics, sociology, mathematics, computer science, management, to information systems. Agent-based modeling is well suited for intelligent systems research as it offers a platform to study systems behavior based on individual actions and interactions. In the following, we present the concepts and illustrate how intelligent agents can be used in modeling intelligent systems.\\nWe start with basic concepts of intelligent agents. \\nThen we define agent-based modeling (ABM) and discuss strengths and weaknesses of ABM. The next section applies ABM to intelligent system modeling. We use an example of technology diffusion for illustration. Research issues and directions are discussed next, followed by conclusions.\\nINTELLIGENT AGENT\\nIntelligent agents, also known as software agents, are computer applications that autonomously sense and respond to environment in the pursuit of certain designed objectives (Wooldridge and Jennings, 1995).  Intelligent agents exhibit some level of intelligence. They can be ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0f50b74-99f6-4a20-9f29-a1cffdda059d', embedding=None, metadata={'page_label': '87', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Agent-Based Intelligent System Modeling\\nused to assist the user in performing non-repetitive tasks, \\nsuch as seeking information, shopping, scheduling, monitoring, control, negotiation, and bargaining. \\nIntelligent agents may come in various shapes and \\nforms such as knowbots, softbots, taskbots, personal agents, shopbots, information agents, etc. No matter what shape or form they have, intelligent agents exhibit one or more of the following characteristics:\\n• Autonomous: Being able to exercise control over \\ntheir own actions. \\n• Adaptive/Learning: Being able to learn and \\nadapt to their external environment.\\n• Social: Being able to communicate, bargain, \\ncollaborate, and compete with other agents on \\nbehalf of their masters (users).\\n• Mobile: Being able to migrate themselves from \\none machine/system to another in a network, such as the Web.\\n• Goal-oriented: Being able to act in accordance \\nwith built-in goals and objectives.\\n• Communicative: Being able to communicate \\nwith people or other agents thought protocols such as agent communication language (ACL).\\n• Intelligent:  Being able to exhibit intelligent \\nbehavior such as reasoning, generalizing, learning, dealing with uncertainty, using heuristics, and natural language processing.\\nAGENT-BASED MODELING\\nUsing intelligent agents and their actions and interactions in a given environment to simulate the complex dynamics of a system is referred to as agent-based modeling. ABM research is closely related to the research in complex systems, emergence, computational sociology, multi agent systems, evolutionary programming, and intelligent organizations. In ABM, system behavior results from individual behaviors and collective behaviors of the agents. Researchers of ABM are interested in how macro phenomena are emerging from micro level behaviors among a heterogeneous set of interacting agents (Holland, 1992). Every agent has its attributes and its behavior rules. When agents encounter in the agent society, each agent individually assesses the situation and makes decisions on the basis of its behavior rules. In general, individual agents do not have global awareness in the multi-agent system.\\nAgent-based modeling allows a researcher to set \\ndifferent parameters and behavior rules of individual agents. The modeler makes assumptions that are most relevant to the situation at hand, and then watches phenomena emerge from the interactions of the agents. Various hypotheses can be tested by changing agent parameters and rules. The emergent collective pattern of the agent society often leads to results that may not have been predicated. \\nOne of the main advantages of ABM over traditional \\nmathematical equation based modeling is the ability to model individual styles and attributes, rather than assuming homogeneity of the whole population. Traditional models based on analytical techniques often become intractable as the systems reach real-world level of complexity. ABM is particularly suitable for studying system dynamics that are generated from interactions of heterogeneous individuals. In recent years, ABM has been used in studying many real world systems, such as stock markets (Castiglione 2000), group selection (Pepper 2000), and workflow and information diffusion (Neri 2004). Bonabeau (2002) presents a good summary of ABM methodology and the scenarios where ABM is appropriate.\\nABM is, however, not immune from criticism. Per \\nBonabeau (2002), “an agent-based model will only be as accurate as the assumptions and data that went into it, but even approximate simulations can be very valuable”. It has also been observed that ABM relies on simplified models of rule-based human behavior that often fail to take into consideration the complexity of human cognition. Besides, it suffers from “unwrapping” problem as the solution is built into the program and thus prevents occurrence of new or unexpected events (Macy, 2002). \\nABM FOR INTELLIGENT SySTEMS\\nAn intelligent system is a system that can sense and respond to its environment in pursuing its goals and objectives. It can learn and adapt based on past experience. Examples of intelligent systems include, but not limited to, the following: biological life such as human beings, artificial intelligence applications, robots, organizations, nations, projects, and social movements.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='651e217e-a56a-4da0-93b3-102940c07fad', embedding=None, metadata={'page_label': '88', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Agent-Based Intelligent System Modeling\\nAWalter Fritz (1997) suggests that the key components \\nof an intelligent system include objectives, senses, \\nconcepts, growth of a concept, present situation, response rules, mental methods, selection, actions, reinforcement, memory and forgetting, sleeping, and patterns (high level concepts). It is apparent that traditional analytical modeling techniques are not able to model many of the components of intelligent systems, let alone the complete system dynamics. However, ABM lends itself well to such a task. All those components can be models as agents (albeit some in abstract sense). An intelligent system is thus made of inter-related and interactive agents. ABM is especially suitable for intelligent systems consist of a large number of heterogeneous participants, such as a human organization.  \\nModeling Processes\\nAgent-based modeling for intelligent systems starts with a thorough analysis of the intelligent systems. Since the system under consideration may exhibit complex behaviors, we need to identify one or a few key features to focus on. Given a scenario of the target intelligent system, we first establish a set of objectives that we aim to achieve via the simulation of the agent-based representation of the intelligent system. The objectives of the research can be expressed as a set of questions to which we seek answers (Doran, 2006).\\nA conceptual model is created to lay out the \\nrequirements for achieving the objectives. This includes defining the entities, such as agents, environment, resources, processes, and relationships. The conceptual modeling phase answers the question of what—what are needed. The design model determines how the requirements can be implemented, including defining the features and relevant behaviors of the agents (Brown, 2006).\\nDepending on the goals of a particular research, a \\nmodel may involve the use of designed or empirically grounded agents. Designed agents are those endowed with characteristics and behaviors that represent conditions for testing specific hypotheses about the intelligent systems. When the agents are empirically grounded, they are used to represent real world entities, such as individuals or processes in an organization. Empirically grounded agents are feasible only when data about the real world entities are available. Similarly, the environment within which the agents act can be designed or empirically grounded. In practice, a study may start with simple models, often with designed agents and environments, to explore certain specific dynamics of the system. \\nThe design model is refined through the calibration \\nprocess, in which design parameters are modified to improve the desired characteristics of the model. The final step in the modeling process is validation where we check the agent individual behavior, interactions, and emergent properties of the system against expected design features. Validation usually involves comparison of model outcomes, often at the macro-level, with comparable outcomes in the real world (Midgley, el at., 2007). Figure 1 shows the complete modeling process. A general tutorial on ABM is given by Macal and North (2005). \\nABM for Innovation Diffusion\\nWe present an example of using agent-based intelligent system modeling for studying the acceptance and diffusion of innovative ideas or technology. Diffusion of innovation has been studied extensively over the last few decades (Rogers, 1995). However, traditional research in innovation diffusion has been grounded on case based analysis and analytical systems modeling \\nSTART\\nSet Objectives\\nConceptual Model\\nDesign Model\\nCalibration\\nValidation\\nENDFigure 1. Agent-based modeling process', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f632b24-4f10-4b36-95af-689bf2fb7036', embedding=None, metadata={'page_label': '89', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Agent-Based Intelligent System Modeling\\n(e.g., using differential and difference equations). \\nAgent-based modeling for diffusion of innovation is relatively new. Our example is adopted from a model created by Michael Samuels (2007), implemented with a popular agent modeling system—NetLogo. \\nThe objective of innovation diffusion modeling is \\nto answer questions such as how an idea or technology is adopted in a population, how different people (e.g., innovators, early adopters, and change agents) influence each other, and under what condition an innovation will be accepted or rejected by the population.  In the conceptual modeling, we identify various factors that influence an individual’s propensity for adopting the innovation. Those factors are broadly divided into to two categories: internal influences (e.g., word-of-mouth) and external influences (e.g. mass media). Any factor that exerts its influence through individual contact is considered internal influence.  \\nIndividuals in the target population are divided \\ninto four groups: adopter, potential  (adopter), change \\nagent, and disrupter.  Adopters are those who have \\nadopted the innovation, while potentials  are those \\nwho have certain likelihood to adopt the innovation. Change agents are the champions of the innovation. They are very knowledgeable and enthusiastic about the innovation, and often play a critical role in facilitating its- diffusion.  Disrupters  are those who play an opposite \\nrole of change agents.  They are against the current \\ninnovation, oftentimes because they favor an even newer and perceived better innovation. The four groups of agents and their relationships are depicted in Figure 2. It is common, although not necessary, to assume that those four groups make up the entire population. \\nIn a traditional diffusion model, such as the Bass \\nmodel (Bass, 1996), the diffusion rate depends only on the number of adopters (and potential adopters, given fixed population size). Characteristics of individuals in the population are ignored. Even in those models where it is assumed that potential adopters have varying threshold for adopting an innovation (Abrahamson and Rosenkopf, 1997), the individuality is very limited. However, in agent-based modeling, the types of individuals and individual characteristics are essentially unbounded. For example, we can divide easily adopters into innovators, early adopters, and late adopters, etc. If necessary, various demographic and social-economic features can be bestowed to individual agents. Furthermore, both internal influence and external influence can be further attributed to more specific causes. For example, internal influence through social networks can be divided into traditional social networks that consists friends and acquaintances and virtual social networks formed online. Table 1 lists typical factors that affect the propensity of adopting an innovation.\\nAn initial study of innovation diffusion, such as the \\none in Michael Samuels (2007), can simply aggregate all internal influences into “word-of-month” and all external influences into mass media. Each potential adopter’s tendency of converting to an adopter is influenced by chance encounter with other agents. If a potential adopter meets a change agent, who is an avid promoter of the innovation, he would become more knowledgeable about the advantages of the innovation, and more likely to adopt. An encounter with a disrupter \\nEnvironment\\nDotted line:External influence\\nSolid line:   Internal influenceChange Agent\\nPotential Adopter\\nDisrupterFigure 2. Agents and influences\\nInternal influence External influence\\nWord-of-mouth Newspapers\\nTelephone Television\\nEmailLaws, policies and regulations\\nInstant message Culture\\nChat Internet/Web\\nBlog Online communities\\nSocial networks (online/offline)RSSTable1. Typical internal and external influences', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09d48a8e-f1b8-48be-99c1-cdc409875c3c', embedding=None, metadata={'page_label': '90', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Agent-Based Intelligent System Modeling\\nAcreates the opposite effect, as a disrupter favors a \\ndifferent type of innovation. \\nIn order for the simulated model to accurately reflect \\na real-world situation, the model structure and parameter values should be carefully selected. For example, we need to decide how much influence each encounter will result; what is the probability of encountering a change agent or a disrupter; how much influence is coming from the mass media, etc. We can get these values through surveys, statistical analysis of empirical data, or experiments specifically designed to elicit data from real world situations.\\nTRENDS AND RESEARCH ISSUES \\nAs illustrated through the example of modeling the diffusion of innovation in an organization, industry, or society, agent-based modeling can be used to model the adaptation of intelligent systems that consist of intelligent individuals. As most intelligent systems are complex in both structure and system dynamics, traditional modeling tools that require too many unrealistic assumptions have become less effective in modeling intelligent systems. In recent years, agent-based modeling has found a wide spectrum of applications such as in business strategic solutions, supply chain management, stock markets, power economy, social evolution, military operations, security, and ecology (North and Macal, 2007).  As ABM tools and resources become more accessible, research and applications of agent-based intelligent system modeling are expected to increase in the near future.\\nSome challenges remain, though. Using ABM \\nto model intelligent systems is a research area that draws theories from other fields, such as economics, psychology, sociology, etc., but without its own well established theoretic foundation. ABM has four key assumptions (Macy and Willer, 2002): Agents act locally with little or no central authority; agents are interdependent; agents follow simple rules, and agents are adaptive. However, some of those assumptions may not be applicable to intelligent system modeling. Central authorities, or central authoritative information such as mass media in the innovation diffusion example, may play an important role in intelligent organizations.  Not all agents are alike in an intelligent system. Some may be independent, non-adaptive, or following complex behavior rules. ABM uses a “bottom-up” approach, creating \\nemergent behaviors of an intelligent system through “actors” rather than “factors”. However, macro-level factors have direct impact on macro behaviors of the system. Macy and Willer (2002) suggest that bringing those macro-level factors back will make agent-based modeling more effective, especially in intelligent systems such as social organizations. \\nRecent intelligent systems research has developed the \\nconcept of integrating human and machine-based data, knowledge, and intelligence. Kirn (1996) postulates that the organization of the 21\\nst century will involve \\nartificial agents based system highly intertwined with human intelligence of the organization. Thus, a new challenge for agent-based intelligent system modeling is to develop models that account for interaction, aggregation, and coordination of intelligent agent and human agents. The ABM will represent not only the human players in an intelligent system, but also the intelligent agents that are developed in real-world applications in those systems. \\nCONCLUSION\\nModeling intelligent systems involving multiple intelligent players has been difficult using traditional approaches. We have reviewed recent development in agent-based modeling and suggest agent-based modeling is well suited for studying intelligent systems, especially those systems with sophisticated and heterogeneous participants. Agent-based modeling allows us to model system behaviors based on the actions and interactions of individuals in the system. Although most ABM research focuses on local rules and behaviors, it is possible that we integrate global influences in the models. ABM represents a novel approach to model intelligent systems. Combined with traditional modeling approaches (for example, micro-level simulation as proposed in MoSeS), ABM offers researchers a promising tool to solve complex and practical problems and to broaden research endeavors (Wu, 2007).', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa9b4f12-f030-4f78-9824-85717d228b2b', embedding=None, metadata={'page_label': '91', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"\\x18\\x18  Agent-Based Intelligent System Modeling\\nREFERENCES\\nAbrahamson, E. and L. Rosenkopf ( 1997). Social \\nNetwork Effects on the Extent of Innovation Diffusion: A Computer Simulation. Organization Science. 8(3), 289-309. \\nBass, F. M. (1969). A New Product Growth Model \\nfor Consumer Durables, Management Science, 13(5). 215-227.\\nBonabeau, E. (2002). Agent-based modeling: Methods \\nand techniques for simulating human systems. PNAS May 14, 2002.  99, suppl. 3, 7280-7287.\\nBrown, D.G. (2006). Agent-based models. In H. Geist, \\nEd. The Earth’s Changing Land: An Encyclopedia of Land-Use and Land-Cover Change. Westport CT: Greenwood Publishing Group. 7-13.\\nDoran J. E. (\\n\\x1800\\x18 ). Agent Design for Agent Based \\nModeling. In Agent Based Computational Modelling: \\nApplications in Demography, Social, Economic and Environmental Sciences, eds. F. C. Billari, T. Fent, A. Prskawetz, and J.Scheffran. Physica-Verlag (Springer). 215-223.\\nFilippo Castiglione (2000), ‘Diffusion and aggregation \\nin an agent based model of stock market fluctuations’, International Journal of Modern Physics C. 11(5), 1-15.\\nFritz, Walter (1997).Intelligent Systems and their \\nSocieties. First version: Jan 27, 1997 http://www.intelligent-systems.com.ar/intsyst/index.htm\\nGupta, J. N. D. and S. K. Sharma (2004). Editors. \\nIntelligent Enterprises for the 21st Century. Hershey, PA: Idea Group Publishing.\\nHolland, J.H. (1992). Complex adaptive systems. \\nDaedalus. 121(1), 17-30.\\nKirn, S. 1996. Organizational intelligence and \\ndistributed artificial intelligence. In Foundations of \\nDistributed Artificial intelligence , G. M. O’Hare and \\nN. R. Jennings, Eds. John Wiley Sixth-Generation Computer Technology Series. John Wiley & Sons, New York, NY . 505-526.\\nLiang, T. Y . (2002). The Inherent Structure and Dynamic \\nof Intelligent Human Organizations, Human Systems Management. 21(1), 9-19.Liebowitz, J. (1999). Building Organizational Intelligence: A Knowledge Primer, New York: CRC Press.\\nMacal, C. M. and North, M. J. (2005). Tutorial on \\nAgent-Based Modeling and Simulation.  Proceedings of the 37\\nth Winter Simulation Conference, Orlando, \\nFlorida. 2-15. \\nMacy, M. W. (2002). Social Simulation, In N. Smelser \\nand P. Baltes, eds., International Encyclopedia of the Social and Behavioral Sciences, Elsevier, The Netherlands. \\nMacy, M.W, and Willer, R. (2002). From Factors to \\nActors: Computational Sociology and Agent-Based Modeling. Annual Review of Sociology. 28, 143-166.\\nMcMaster, M. D. (1996). The Intelligence Advantage: \\nOrganizing for Complexity. Burlington MA: Butterworth-Heineman.\\nMidgley, D.F., Marks R.E., and Kunchamwar D. (2007). \\nThe Building and Assurance of Agent-Based Models: An Example and Challenge to the Field. Journal of Business Research. 60(8), 884-893.\\nNeri, F. (2004). Agent Based Simulation of Information \\nDiffusion in a Virtual Market Place. IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT'04). 333-336.\\nNorth, M. J. and C. M. Macal, (2007). Managing \\nBusiness Complexity: Discovering Strategic Solutions with Agent-based Modeling and Simulation.  Oxford University Press, New York.\\nPepper, J. W. (2000) An Agent-Based Model of Group \\nSelection, Santa Fe Institute. Retrieved June 16, 2007 at: http://www.santafe.edu/~jpepper/papers/ALIFE7_GS.pdf\\nRogers, E.M. (1995). Diffusion of Innovations. The \\nFree Press, New York.\\nSamuels, M.L. (2007). Innovation model. Last updated: \\n01/08/2007, http://ccl.northwestern.edu/netlogo/models/community/Innovation.\\nWooldridge, M. and  N. R. Jennings (1995). Intelligent \\nAgents: Theory and Practice, Knowledge Engineering Review. 10(2), 115-152.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aeae659b-fd18-4de1-8067-7ff4259cee2d', embedding=None, metadata={'page_label': '92', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Agent-Based Intelligent System Modeling\\nAWu, B. (2007). A Hybrid Approach for Spatial MSM. \\nNSF/ESRC Agenda Setting Workshop on Agent-Based Modeling of Complex Spatial Systems: April 14-16, 2007 \\nKEy TERMS \\nAgent Based Modeling: Using intelligent agents \\nand their actions and interactions in a given environment to simulate the complex dynamics of a system.\\nDiffusion of Innovation:  Popularized by Everett \\nRogers, it is the study of the process by which an innovation is communicated and adopted over time among the members of a social system.\\nIntelligent Agent:  An autonomous software \\nprogram that is able to learn and adapt to its environment in order to perform certain tasks delegated to it by its master.Intelligent System: A system that has a coherent \\nset of components and subsystems working together to engage in goal-driven activities.\\nIntelligent System Modeling: The process of \\nconstruction, calibration, and validation of models of intelligent systems. \\nMulti-Agent System: A distributed system with a \\ngroup of intelligent agents that communicate, bargain, compete, and cooperate with other agents and the environment to achieve goals designated by their masters. \\nOrganizational Intelligence: The ability of an \\norganization to perceive, interpret, and select the most appropriate response to the environment in order to advance its goals.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50c9c208-1754-4cf4-a553-88eacf841508', embedding=None, metadata={'page_label': '93', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAI and Ideas by Statistical Mechanics\\nLester Ingber\\nLester Ingber Research, USA\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION \\nA briefing (Allen, 2004) demonstrates the breadth and depth complexity required to address real diplomatic, information, military, economic (DIME) factors for the propagation/evolution of ideas through defined popula -\\ntions.  An open mind would conclude that it is possible that multiple approaches may be required for multiple decision makers in multiple scenarios.  However, it is in the interests of multiple decision-makers to as much as possible rely on the same generic model for actual computations.  Many users would have to trust that the coded model is faithful to process their inputs. \\nSimilar to DIME scenarios, sophisticated competi-\\ntive marketing requires assessments of responses of populations to new products. \\nMany large financial institutions are now trading at \\nspeeds barely limited by the speed of light. They co-locate their servers close to exchange floors to be able to turn quotes into orders to be executed within msecs. Clearly, trading at these speeds require automated al-gorithms for processing and making decisions.  These algorithms are based on \"technical\" information derived from price, volume and quote (Level II) information.  The next big hurdle to automated trading is to turn \"fundamental\" information into technical indicators, e.g., to include new political and economic news into such algorithms. \\nBACKGROUND \\nThe concept of “ memes” is an example of an approach \\nto deal with DIME factors (Situngkir, 2004).  The meme approach, using a reductionist philosophy of evolution among genes, is reasonably contrasted to approaches emphasizing the need to include relatively global influ-ences of evolution (Thurtle, 2006). \\nThere are multiple other alternative works being \\nconducted world-wide that must be at least kept in mind while developing and testing models of evolu-tion/propagation of ideas in defined populations: A study on a simple algebraic model of opinion formation concluded that the only final opinions are extremal ones (Aletti et al., 2006).  A study of the influence on chaos on opinion formation, using a simple algebraic model, concluded that contrarian opinion could persist and be crucial in close elections, albeit the authors were careful to note that most real populations prob-ably do not support chaos (Borghesi & Galam, 2006). A limited review of work in social networks illustrates that there are about as many phenomena to be explored as there are disciplines ready to apply their network models (Sen, 2006). \\nStatistical Mechanics of Neocortical Interactions (SMNI) \\nA class of AI algorithms that has not yet been developed in this context takes advantage of information known about real neocortex. It seems appropriate to base an approach for propagation of ideas on the only system so far demonstrated to develop and nurture ideas, i.e., the neocortical brain. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory (STM) and electroencephalography (EEG) indicators, is the proposed bottom-up model. Ideas by Statistical  Mechanics (ISM) is a generic program to model evo-lution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions (Ingber, 2006). ISM develops subsets of macrocolumnar activity of multivariate stochastic de-scriptions of defined populations, with macrocolumns defined by their local parameters within specific regions and with parameterized endogenous inter-regional and exogenous external connectivities.  Parameters of subsets of macrocolumns will be fit to patterns repre-senting ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Fitting such nonlinear systems requires the use of sampling techniques. \\nThe author\\'s approach uses guidance from his sta-\\ntistical mechanics of neocortical interactions (SMNI), ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ca4ace3-23a2-4aca-be25-c05fc4140b17', embedding=None, metadata={'page_label': '94', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"  \\x18\\x18AI and Ideas by Statistical Mechanics\\nAdeveloped in a series of about 30 published papers \\nfrom 1981-2001 (Ingber, 1983; Ingber, 1985; Ingber, 1992; Ingber, 1994; Ingber, 1995; Ingber, 1997).  These papers also address long-standing  issues of informa-tion measured by electroencephalography (EEG) as arising from bottom-up local interactions of clusters of thousands to tens of thousands of neurons interact-ing via short-ranged fibers), or top-down influences of global interactions (mediated by long-ranged myelin-ated fibers). SMNI does this by including both local and global interactions as being necessary to develop neocortical circuitry. \\nStatistical Mechanics of Financial Markets (SMFM) \\nTools of financial risk management, developed to process correlated multivariate systems with differ-ing non-Gaussian distributions using modern copula analysis enables bona fide correlations and uncertain -\\nties of success and failure to be calculated. Since 1984, the author has published about 20 papers developing a Statistical Mechanics of Financial Markets (SMFM), many available at http://www.ingber.com. These are relevant to ISM, to properly deal with real-world dis-tributions that arise in such varied contexts. \\nGaussian copulas are developed in a project Trad-\\ning in Risk Dimensions (TRD) (Ingber, 2006). Other copula distributions are possible, e.g., Student-t distri-butions.  These alternative distributions can be quite slow because inverse transformations typically are not as quick as for the present distribution. Copulas are cited as an important component of risk management not yet widely used by risk management practitioners (Blanco, 2005). \\nSampling Tools\\nComputational approaches developed to process dif-ferent approaches to modeling phenomena must not be confused with the models of these phenomena. For example, the meme approach lends it self well to a computational scheme in the spirit of genetic algorithms (GA). The cost/objective function that describes the phenomena of course could be processed by any other sampling technique such as simulated annealing (SA).  One comparison (Ingber & Rosen, 1992) demonstrated the superiority of SA over GA on cost/objective func-tions used in a GA database. That study used Very Fast Simulated Annealing (VFSR), created by the author for military simulation studies (Ingber, 1989), which has evolved into Adaptive Simulated Annealing (ASA) (Ingber, 1993).  However, it is the author's experience that the Art and Science of sampling complex systems requires tuning expertise of the researcher as well as good codes, and GA or SA likely would do as well on cost functions for this study. \\nIf there are not analytic or relatively standard math \\nfunctions for the transformations required, then these transformations must be performed explicitly numeri-cally in code such as TRD. Then, the ASA_PARALLEL OPTIONS already existing in ASA (developed as part of the1994 National Science Foundation Parallelizing ASA and PATHINT Project (PAPP)) would be very useful to speed up real time calculations (Ingber, 1993).  Below, only a few topics relevant to ISM are discussed.  More details are in a previous report (Ingber, 2006). \\nSMNI AND SMFM APPLIED TO ARTIFICIAL INTELLIGENCE \\nNeocortex has evolved to use minicolumns of neurons interacting via short-ranged interactions in macrocol-umns, and interacting via long-ranged interactions across regions of macrocolumns. This common ar-chitecture processes patterns of information within and among different regions of sensory, motor, as-sociative cortex, etc. Therefore, the premise of this approach is that this is a good model to describe and analyze evolution/propagation of ideas among defined populations. \\nRelevant to this study is that a spatial-temporal \\nlattice-field short-time conditional multiplicative-noise (nonlinear in drifts and diffusions) multivariate Gaussian-Markovian probability distribution is de-veloped faithful to neocortical function/physiology.  Such probability distributions are a basic input into the approach used here. The SMNI model was the first physical application of a nonlinear multivariate calculus developed by other mathematical physicists in the late 1970s to define a statistical mechanics of multivariate nonlinear nonequilibrium systems (Graham, 1977; Langouche et al., 1982). \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='685a85fc-ee20-4dfb-9410-2285b22a1299', embedding=None, metadata={'page_label': '95', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  AI and Ideas by Statistical Mechanics\\nSMNI Tests on STM and EEG \\nSMNI builds from synaptic interactions to minicolum-\\nnar, macrocolumnar, and regional interactions in neo-cortex.  Since 1981, a series of SMNI papers has been developed model columns and regions of neocortex, spanning mm to cm of tissue.  Most of these papers have dealt explicitly with calculating properties of STM and scalp EEG in order to test the basic formulation of this approach (Ingber, 1983; Ingber, 1985; Ingber & Nunez, 1995). \\nThe SMNI modeling of local mesocolumnar \\ninteractions (convergence and divergence between minicolumnar and macrocolumnar interactions) was tested on STM phenomena. The SMNI modeling of macrocolumnar interactions across regions was tested on EEG phenomena. \\nFigure 1. Illustrated are three biophysical scales of neocortical interactions: (a)-(a*)-(a\\') microscopic neurons; (b)-(b\\') mesocolumnar domains; (c)-(c\\') macroscopic regions (Ingber, 1983). SMNI has developed appropriate conditional probability distributions at each level, aggregating up from the smallest levels of interactions. In (a*) synaptic inter-neuronal interactions, averaged over by mesocolumns, are phenomenologically described by the mean and variance of a distribution Ψ. Similarly, in (a) intraneuronal transmissions are phenomenologically \\ndescribed by the mean and variance of\\uf020 Γ. Mesocolumnar averaged excitatory (E) and inhibitory (I) neuronal \\nfirings M are represented in (a\\').  In (b) the vertical organization of minicolumns is sketched together with their horizontal stratification, yielding a physiological entity, the mesocolumn.  In (b\\') the overlap of interacting mesocolumns at locations r and r′ from times t and t + t is sketched. In (c) macroscopic regions of neocortex \\nare depicted as arising from many mesocolumnar domains. (c\\') sketches how regions may be coupled by long-ranged interactions. SMNI Description of STM\\nSMNI studies have detailed that maximal numbers of attractors lie within the physical firing space of both excitatory and inhibitory minicolumnar firings, consis -\\ntent with experimentally observed capacities of audi-tory and visual STM, when a \"centering\" mechanism is enforced by shifting background noise in synaptic interactions, consistent with experimental observations under conditions of selective attention (Ingber, 1985; Ingber, 1994). \\nThese calculations were further supported by high-\\nresolution evolution of the short-time conditional-prob-ability propagator using PATHINT (Ingber & Nunez, 1995). SMNI correctly calculated the stability and duration of STM, the primacy versus recency rule, ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='719a2158-8fcc-4ca1-a823-b62a0239fd06', embedding=None, metadata={'page_label': '96', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18AI and Ideas by Statistical Mechanics\\nArandom access to memories within tenths of a second \\nas observed, and the observed 7±2 capacity rule of auditory memory and the observed 4±2 capacity rule of visual memory. \\nSMNI also calculates how STM patterns (e.g., \\nfrom a given region or even aggregated from multiple regions) may be encoded by dynamic modification of synaptic parameters (within experimentally observed ranges) into long-term memory patterns (LTM) (Ing-ber, 1983). \\nSMNI Description of EEG \\nUsing the power of this formal structure, sets of EEG and evoked potential data from a separate NIH study, collected to investigate genetic predispositions to al-coholism, were fitted to an SMNI model on a lattice of regional electrodes to extract brain \"signatures\" of STM (Ingber, 1997). Each electrode site was represented by an SMNI distribution of independent stochastic macrocolumnar-scaled firing variables, interconnected by long-ranged circuitry with delays appropriate to long-fiber communication in neocor -\\ntex. The global optimization algorithm ASA was used to perform maximum likelihood fits of Lagrangians defined by path integrals of multivariate conditional probabilities. Canonical momenta indicators (CMI) were thereby derived for individual\\'s EEG data.  The CMI give better signal recognition than the raw data, and were used to advantage as correlates of behavioral states. In-sample data was used for training (Ingber, 1997), and out-of-sample data was used for testing these fits. The architecture of ISM is modeled using scales similar to those used for local STM and global EEG connectivity. \\nGeneric Mesoscopic Neural Networks \\nSMNI was applied to a parallelized generic mesoscopic neural networks (MNN) (Ingber, 1992), adding com-putational power to a similar paradigm proposed for target recognition. \\n\"Learning\" takes place by presenting the MNN with \\ndata, and parametrizing the data in terms of the firings, or multivariate firings.  The \"weights,\" or coefficients of functions of firings appearing in the drifts and dif -\\nfusions, are fit to incoming data, considering the joint \"effective\" Lagrangian (including the logarithm of the prefactor in the probability distribution) as a dynamic cost function. This program of fitting coefficients in Lagrangian uses methods of ASA. \"Prediction\" takes advantage of a mathematically equivalent representa-tion of the Lagrangian path-integral algorithm, i.e., a set of coupled Langevin rate-equations.  A coarse deterministic estimate to \"predict\" the evolution can be applied using the most probable path, but PATHINT has been used.  PATHINT, even when parallelized, typically can be too slow for \"predicting\" evolution of these systems. However, PATHTREE is much faster. \\nArchitecture for Selected ISM Model\\nThe primary objective is to deliver a computer model that contains the following features: (1) A multivariable space will be defined to accommodate populations.  (2) A cost function over the population variables in (1) will be defined to explicitly define a pattern that can be identified as an Idea.  A very important issue is for this project is to develop cost functions, not only how to fit or process them.  (3) Subsets of the popula -\\ntion will be used to fit parameters — e.g, coefficients of variables, connectivities to patterns, etc. — to an Idea, using the cost function in (2). (4) Connectivity of the population in (3) will be made to the rest of the population.  Investigations will be made to determine what endogenous connectivity is required to stop or promote the propagation of the Idea into other regions of the population. (5) External forces, e.g., acting only on specific regions of the population, will be introduced, to determine how these exogenous forces may stop or promote the propagation of an Idea. \\nApplication of SMNI Model\\nThe approach is to develop subsets of Ideas/macroco-lumnar activity of multivariate stochastic descriptions of \\nFigure 2. Scales of interactions among minicolumns are represented, within macrocolumns, across macro-columns, and across regions of macrocolumns ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b225f9bf-e66d-4779-8005-d3317e91f8d9', embedding=None, metadata={'page_label': '97', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  AI and Ideas by Statistical Mechanics\\ndefined populations (of a reasonable but small popula -\\ntion samples, e.g., of 100-1000), with macrocolumns \\ndefined by their local parameters within specific regions (larger samples of populations) and with parameterized long-ranged inter-regional  and external connectivities.  Parameters of a given subset of macrocolumns will be fit using ASA to patterns representing Ideas, akin to acquiring hard-wired long-term (LTM) patterns.  Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread  of these Ideas, by determining the degree of fits and overlaps of probability distributions relative to the seeded macrocolumns. \\nThat is, the same Ideas/patterns may be represented \\nin other than the seeded macrocolumns by local conflu-ence of macrocolumnar and long-ranged firings, akin to STM, or by different hard-wired parameter LTM sets that can support the same local firings in other regions (possible in nonlinear systems). SMNI also calculates how STM can be dynamically encoded into LTM (Ingber, 1983). \\nSmall populations in regions will be sampled to \\ndetermine if the propagated Idea(s) exists in its pattern space where it did exist prior to its interactions with the seeded population.  SMNI derives nonlinear functions as arguments of probability distributions, leading to multiple STM, e.g., 7±2 for auditory memory capac-ity.  Some investigation will be made into nonlinear functional forms other than those derived for SMNI, e.g., to have capacities of tens or hundreds of patterns for ISM. \\nApplication of TRD Analysis\\nThis approach includes application of methods of port-folio risk analysis to such statistical systems, correct-ing two kinds of errors committed in multivariate risk analyses: (E1) Although the distributions of variables being considered are not Gaussian (or not tested to see how close they are to Gaussian), standard statistical calculations appropriate only to Gaussian distribu-tions are employed.  (E2) Either correlations among the variables are ignored, or the mistakes committed in (E1) — incorrectly assuming variables are Gaussian — are compounded by calculating correlations as if all variables were Gaussian. \\nIt should be understood that any sampling algorithm \\nprocessing a huge number of states can find many multiple optima.  ASA\\'s MULTI_MIN OPTIONS are used to save multiple optima during sampling.  Some algorithms might label these states as \"mutations\" of optimal states.  It is important to be able to include them in final decisions, e.g., to apply additional metrics of performance specific to applications. Experience with risk-managing portfolios shows that all criteria are not best considered by lumping them all into one cost function, but rather good judgment should be applied to multiple stages of pre-processing and post-processing  when performing such sampling, e.g., adding additional metrics of performance. \\nFUTURE TRENDS \\nGiven financial and political motivations to merge in -\\nformation discussed in the Introduction, it is inevitable that many AI algorithms will be developed, and many current AI algorithms will be enhanced, to address these issues. \\nCONCLUSION \\nIt seems appropriate to base an approach for propa-gation of generic ideas on the only system so far demonstrated to develop and nurture ideas, i.e., the neocortical brain. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, Ideas by Statistical Mechanics (ISM) (Ingber, 2006) is the proposed model.  ISM develops subsets of macrocolumnar activity of multivariate stochastic descriptions of defined populations, with macrocolumns defined by their local parameters within specific regions and with parameterized endogenous inter-regional and exogenous external connectivities. Tools of financial risk management, developed to process correlated multivariate systems with differ-ing non-Gaussian distributions using modern copula analysis, importance-sampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00e423bc-9d1f-4b53-9bcc-27ffa7ac05a4', embedding=None, metadata={'page_label': '98', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18AI and Ideas by Statistical Mechanics\\nAREFERENCES\\nAletti, G.,  Naldi, G. & Toscani, G.  (2006) First-order \\ncontinuous models of opinion formation. \\nReport.  U Milano.  [Url http://lanl.arxiv.org/abs/cond-\\nmat/0605092] \\nAllen, J.  (2004) Commander\\'s automated decision \\nsupport tools.  Report.  DARPA.  [URL http://www.darpa.mil/ato/solicit/IBC/allen.ppt] \\nBlanco, C.  (2005) Financial Risk Management: Beyond \\nNormality, V olatility and Correlations. \\nFinancial Economics Network, Waltham, MA.  [URL \\nhttp://www.fenews.com/fen46/front-sr/blanco/blanco.html] \\nBorghesi, C. & Galam, S.  (2006) Chaotic, staggered and \\npolarized dynamics in opinion forming: the contrarian effect.  Report.  Service de Physique de l\\'Etat Condens.  [Url http://lanl.arxiv.org/abs/physics/0605150] \\nGraham, R.  (1977) Covariant formulation of non-\\nequilibrium statistical thermodynamics. Zeitschrift fu¨r Physik.  B26, 397-405. \\nIngber, L.  (1983) Statistical mechanics of neocorti-\\ncal interactions. Dynamics of synaptic modification.  Physical Review A.  28, 395-416.  [URL http://www.ingber.com/smni83_dynamics.pdf] \\nIngber, L.  (1985) Statistical mechanics of neocortical \\ninteractions: Stability and duration of the 7+-2 rule of short-term-memory capacity.  Physical Review A.  31, 1183-1186.  [URL http://www.ingber.com/smni85_stm.pdf] \\nIngber, L.  (1989) Very fast simulated re-annealing.  \\nMathematical Computer Modelling.  12(8), 967-973.  [URL http://www.ingber.com/asa89_vfsr.pdf] \\nIngber, L.  (1992) Generic mesoscopic neural networks \\nbased on statistical mechanics of neocortical interac-tions.  Physical Review A.  45(4), R2183-R2186.  [URL http://www.ingber.com/smni92_mnn.pdf] \\nIngber, L.  (1993) Adaptive Simulated Annealing \\n(ASA).  Global optimization C-code.  Caltech Alumni Association.  [URL http://www.ingber.com/#ASA-CODE] \\nIngber, L.  (1994) Statistical mechanics of neocorti-\\ncal interactions: Path-integral evolution of short-term memory.  Physical Review E.  49(5B), 4652-4664.  [URL http://www.ingber.com/smni94_stm.pdf] \\nIngber, L.  (1995) Statistical mechanics of multiple \\nscales of neocortical interactions, In: Neocortical Dynamics and Human EEG Rhythms, ed. P.L. Nunez.  Oxford University \\nPress, 628-681.  [ISBN 0-19-505728-7.  URL http://\\nwww.ingber.com/smni95_scales.pdf] \\nIngber, L.  (1997) Statistical mechanics of neocorti-\\ncal interactions: Applications of canonical momenta indicators to electroencephalography.  Physical Re-view E.  55(4), 4578-4593. [URL http://www.ingber.com/smni97_cmi.pdf] \\nIngber, L.  (2006) Ideas by statistical mechanics (ISM).  \\nReport 2006:ISM.  Lester Ingber Research.  [URL http://www.ingber.com/smni06_ism.pdf] \\nIngber, L. & Nunez, P.L.  (1995) Statistical mechanics of \\nneocortical interactions: High resolution path-integral calculation of short-term memory.  Physical Review E.  51(5), 5074-5083.  [URL http://www.ingber.com/smni95_stm.pdf] \\nIngber, L. & Rosen, B.  (1992) Genetic algorithms \\nand very fast simulated reannealing: A comparison.  Mathematical Computer Modelling.  16(11), 87-100.  [URL http://www.ingber.com/asa92_saga.pdf] \\nLangouche, F.,  Roekaerts, D. & Tirapegui, E.  (1982) \\nFunctional Integration and Semiclassical Expansions.  Reidel, Dordrecht, The Netherlands. \\nSen, P.  (2006) Complexities of social networks: A \\nphysicist\\'s perspective.  Report.  U Calcutta. [Url http://lanl.arxiv.org/abs/physics/0605072] \\nSitungkir, H.  (2004) On selfish memes: Culture as \\ncomplex adaptive system.  Journal Social Complexity.  2(1), 20-32.  [URL http://cogprints.org/3471/] \\nThurtle, P.S.  (2006) \"The G Files\": Linking \"The Self-\\nish Gene\" And \"The Thinking Reed\". \\nStanford Presidential Lectures and Symposia in the \\nHumanities and Arts.  Standford U. [URL http://prelec-tur.stanford.edu/lecturers/gould/commentary/thurtle.html] ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b40c34e4-0bfd-48b4-953d-0e17c22d1ad4', embedding=None, metadata={'page_label': '99', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  AI and Ideas by Statistical Mechanics\\nKEy TERMS\\n \\nCopula Analysis: This transforms non-Gaussian  \\nprobability distributions to a common appropriate space (usually a Gaussian space) where it makes sense to calculate correlations as second moments. \\nDIME: Represents diplomatic, information, mili-\\ntary, and economic aspects of information that must be merged into coherent pattern. \\nGlobal Optimization: Refers to a collection of \\nalgorithms used to statistically sample a space of parameters or variables to optimize a system, but also often used to sample a huge space for information.  There are many variants, including simulated an-nealing, genetic algorithms, ant colony optimization, hill-climbing, etc. \\nISM: An anacronym for Ideas by Statistical Me-\\nchanics in the context of the noun defined as: A belief (or system of beliefs) accepted as authoritative by some group or school. A doctrine or theory; especially, a wild or visionary theory.  A distinctive doctrine, theory, system, or practice. Meme: Alludes to a technology originally defined \\nto explain social evolution, which has been refined to mean a gene-like analytic tool to study cultural evolution. \\nMemory: This may have many forms and mecha-\\nnisms. Here, two major processes of neocortical memory are used for AI technologies, short-term memory (STM) and long-term  memory (LTM). \\nSimulated Annealing (SA): A class of algorithms \\nfor sampling a huge space, which has a mathematical proof of convergence to global optimal minima.  Most SA algorithms applied to most systems do not fully take advantage of this proof, but the proof often is useful to give confidence that the system will avoid getting stuck for a long time in local optimal regions. \\nStatistical Mechanics: A branch of mathematical \\nphysics dealing with systems with a large number of states. Applications of nonequilibrium nonlinear statisti-cal mechanics are now common in many fields, ranging from physical and biological sciences, to finance, to computer science, etc. \\n  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a7f5935-91c4-41c4-b718-ee3badc6429b', embedding=None, metadata={'page_label': '100', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAI Methods for Analyzing Microarray Data\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nBiological systems can be viewed as information man-\\nagement systems, with a basic instruction set stored in each cell’s DNA as “genes.” For most genes, their information is enabled when they are transcribed into RNA which is subsequently translated into the proteins that form much of a cell’s machinery. Although details of the process for individual genes are known, more complex interactions between elements are yet to be discovered. What we do know is that diseases can result if there are changes in the genes themselves, in the proteins they encode, or if RNAs or proteins are made at the wrong time or in the wrong quantities.\\nRecent advances in biotechnology led to the de-\\nvelopment of DNA microarrays, which quantitatively measure the expression of thousands of genes simul-taneously and provide a snapshot of a cell’s response to a particular condition. Finding patterns of gene ex-pression that provide insight into biological endpoints offers great opportunities for revolutionizing diagnostic and prognostic medicine and providing mechanistic insight in data-driven research in the life sciences, an area with a great need for advances, given the urgency associated with diseases. However, microarray data analysis presents a number of challenges, from noisy data to the curse of dimensionality (large number of features, small number of instances) to problems with no clear solutions (e.g. real world mappings of genes \\nto traits or diseases that are not yet known). \\nFinding patterns of gene expression in microarray \\ndata poses problems of class discovery, comparison, prediction, and network analysis which are often ap-proached with  AI methods. Many of these methods have been successfully applied to microarray data analysis in a variety of applications ranging from clustering of yeast gene expression patterns (Eisen et al. , 1998) to \\nclassification of different types of leukemia (Golub et al., \\n1999). Unsupervised learning methods ( e.g. hierarchical \\nclustering) explore clusters in data and have been used for class discovery of distinct forms of diffuse large B-cell lymphoma (Alizadeh et al. , 2000). Supervised \\nlearning methods ( e.g. artificial neural networks) utilize \\na previously determined mapping between biological samples and classes ( i.e. labels) to generate models for \\nclass prediction. A k-nearest neighbor (k-NN) approach was used to train a gene expression classifier of differ -\\nent forms of brain tumors and its predictions were able to distinguish biopsy samples with different prognosis suggesting that microarray profiles can predict clini-cal outcome and direct treatment (Nutt et al., 2003). \\nBayesian networks constructed from microarray data hold promise for elucidating the underlying biological mechanisms of disease (Friedman et al., 2000).\\nBACKGROUND\\nCells dynamically respond to their environment by changing the set and concentrations of active genes by altering the associated RNA expression. Thus “gene expression” is one of the main determinants of a cell’s state, or phenotype. For example, we can investigate the differences between a normal cell and a cancer cell by examining their relative gene expression profiles. \\nMicroarrays quantify gene expression levels in vari-\\nous conditions (such as disease vs. normal) or across \\ntime points. For n genes and m instances (biological Amira DjebbariNational Research Council Canada, Canada\\nAedín C. Culhane\\nHarvard School of Public Health, USA\\nAlice J. Armstrong\\nThe George Washington University, USA\\nJohn Quackenbush\\nHarvard School of Public Health, USA', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67d85e2d-c0c4-463b-8fef-172ebedb967e', embedding=None, metadata={'page_label': '101', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  AI Methods for Analyzing Microarray Data\\nsamples), microarray measurements are stored in an \\nn by m matrix where each row is a gene, each column \\nis a sample and each element in the matrix is the ex-pression level of a gene in a biological sample, where samples are instances and genes are features describing those instances. Microarray data is available through many public online repositories (Table 1). In addition, the Kent-Ridge repository (http://sdmc.i2r.a-star.edu.sg/rp/) contains pre-formatted data ready to use with the well-known machine learning tool Weka (Witten & Frank, 2000).\\nMicroarray data presents some unique challenges for \\nAI such as a severe case of the curse of dimensionality due to the scarcity of biological samples (instances). Microarray studies typically measure tens of thousands of genes in only tens of samples. This low case to variable ratio increases the risk of detecting spurious relationships. This problem is exacerbated because microarray data contains multiple sources of within-class variability, both technical and biological. The high levels of variance and low sample size make feature selection difficult. Testing thousands of genes creates a multiple testing problem, which can result in under-estimating the number of false positives. Given data with these limitations, constructing models becomes under-determined and therefore prone to over-fitting . \\nFrom biology, it is also clear that genes do not act \\nindependently. Genes interact in the form of pathways or gene regulatory networks. For this reason, we need models that can be interpreted in the context of path-ways. Researchers have successfully applied AI meth-ods to microarray data preprocessing, clustering, feature selection, classification, and network analysis.MINING MICROARRAy DATA:  CURRENT TECHNIQUES, CHALLENGES AND OPPORTUNITIES FOR AI\\nData Preprocessing\\nAfter obtaining microarray data, normalization is per-\\nformed to account for systematic measurement biases and to facilitate between-sample comparisons (Quack-enbush, 2002). Microarray data may contain missing values that may be replaced by mean replacement or k-NN imputation (Troyanskaya et al., 2001). \\nFeature Selection\\nThe goal of feature selection is to find genes (features) \\nthat best distinguish groups of instances ( e.g. disease \\nvs. normal) to reduce the dimensionality of the dataset. Several statistical methods including t-test, significance analysis of microarrays (SAM) (Tusher et al., 2001), \\nand analysis of variance (ANOV A) have been applied to select features from microarray data. \\nIn classification experiments, feature selection \\nmethods generally aim to identify relevant gene subsets to construct a classifier with good performance (Inza et al., 2004). Features are considered to be relevant when they can affect the class; the strongly relevant are indispensable to prediction and the weakly relevant may only sometimes contribute to prediction.\\nFilter methods evaluate feature subsets regardless \\nof the specific learning algorithm used. The statistical methods for feature selection discussed above as well as rankers like information gain rankers are filters for the features to be included. These methods ignore the fact that there may be redundant features (features that are highly correlated with each other and as such one can be used to replace the other) and so do not seek to find a set of features which could perform similarly Name of the repository URL\\nArrayExpress at the European Bioinformatics Institute http://www.ebi.ac.uk/arrayexpress/\\nGene Expression Omnibus at the National Institutes of \\nHealthhttp://www.ncbi.nlm.nih.gov/geo/\\nStanford microarray database http://smd.stanford.edu/\\nOncomine http://www.oncomine.org/main/index.jspTable 1. Some public online repositories of microarray data', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35f04814-060e-472c-8894-8453d9db72ad', embedding=None, metadata={'page_label': '102', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18AI Methods for Analyzing Microarray Data\\nAwith fewer variables while retaining the same predic-\\ntive power (Guyon & Elisseeff, 2003). For this reason multivariate methods are more appropriate. \\nAs an alternative, wrappers consider the learning \\nalgorithm as a black-box and use prediction accuracy to evaluate feature subsets (Kohavi & John, 1997). Wrap-pers are more direct than filter methods but depend on the particular learning algorithm used. The computational complexity associated with wrappers is prohibitive due to curse of dimensionality, so typically filters are used with forward selection (starting with an empty set and adding features one by one) instead of backward elimination (starting with all features and removing them one by one). Dimension reduction approaches are also used for multivariate feature selection.\\nDimension Reduction Approaches\\nPrincipal component analysis (PCA) is widely used for dimension reduction in machine learning (Wall et al., \\n2003). The idea behind PCA is quite intuitive: correlated objects can be combined to reduce data “dimensional-ity”. Relationships between gene expression profiles in a data matrix can be expressed as a linear combination such that colinear variables are regressed onto a new set of coordinates. PCA, its underlying method Single Value Decomposition (SVD), related approaches such as correspondence analysis (COA), and multidimensional scaling (MDS) have been applied to microarray data and are reviewed by Brazma & Culhane (2005). Studies have reported that COA or other dual scaling dimension reduction approaches such as spectral map analysis may be more appropriate than PCA for decomposition of microarray data (Wouters et al., 2003). \\nWhile PCA considers the variance of the whole \\ndataset, clustering approaches examine the pairwise distance between instances or features. Therefore, these methods are complementary and are often both used in exploratory data analysis. However, difficulties in interpreting the results in terms of discrete genes limit the application of these methods.\\nClustering\\nWhat we see as one disease is often a collection of disease subtypes. Class discovery aims to discover these subtypes by finding groups of instances with similar expression patterns. Hierarchical clustering is an agglomerative method which starts with a singleton and groups similar data points using some distance measure such that two data points that are most simi-lar are grouped together in a cluster by making them children of a parent node in the tree. This process is repeated in a bottom-up fashion until all data points belong to a single cluster (corresponding to the root of the tree). \\nHierarchical and other clustering approaches, \\nincluding K-means, have been applied to microarray data (Causton et al., 2003). Hierarchical clustering \\nwas applied to study gene expression in samples from patients with diffuse large B-cell lymphoma (DLBCL) resulting in the discovery of two subtypes of the dis-ease. These groups were found by analyzing microar-ray data from biopsy samples of patients who had not been previously treated. These patients continued to be studied after chemotherapy, and researchers found that the two newly discovered disease subtypes had different survival rates, confirming the hypothesis that the subtypes had significantly different pathologies (Alizadeh et al., 2000).\\nWhile clustering simply groups the given data based \\non pair-wise distances, when information is known a \\npriori about some or all of the data i.e. labels, a super-\\nvised approach can be used to obtain a classifier that can predict the label of new instances.\\nClassification (Supervised Learning)\\nThe large dimensionality of microarray data means that all classification methods are susceptible to over-fitting. Several supervised approaches have been applied to microarray data including Artificial Neural Networks (ANNs), Support Vector Machines (SVMs), and k-NNs among others (Hastie et al., 2001). \\nA very challenging and clinically relevant prob-\\nlem is the accurate diagnosis of the primary origin of metastatic tumors. Bloom et al.  (2004) applied ANNs \\nto the microarray data of 21 tumor types with 88% accuracy to predict the primary site of origin of meta-static cancers with unknown origin. A classification of 84% was obtained on an independent test set with important implications for diagnosing cancer origin and directing therapy. \\nIn a comparison of different SVM approaches, \\nmulticategory SVMs were reported to outperform other popular machine learning algorithms such as k-NNs and ANNs (Statnikov et al. , 2005) when applied to 11 \\npublicly available microarray datasets related to cancer. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='354f6d5e-efec-4b0f-9d53-711c8eef053e', embedding=None, metadata={'page_label': '103', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  AI Methods for Analyzing Microarray Data\\nIt is worth noting that feature selection can significantly \\nimprove classification performance. \\nCross-Validation\\nCross-validation (CV) is appropriate in microarray stud-ies which are often limited by the number of instances (e.g. patient samples). In k-fold CV , the training set is divided into k subsets of equal size. In each iteration \\nk-1 subsets are used for training and one subset is used for testing. This process is repeated k times and \\nthe mean accuracy is reported. Unfortunately, some published studies have applied CV only partially, by applying CV on the creation of the prediction rule while excluding feature selection. This introduces a bias in the estimated error rates and over-estimates the classification accuracy (Simon et al., 2003). As a \\nconsequence, results from many studies are contro-versial due to methodological flaws (Dupuy & Simon, 2007). Therefore, models must be evaluated carefully to prevent selection bias (Ambroise & McLachlan, 2002). Nested CV is recommended, with an inner CV loop to perform the tuning of the parameters and an outer CV to compute an estimate of the error (Varma & Simon, 2006). \\nSeveral studies which have examined similar bio-\\nlogical problems have reported poor overlap in gene expression signatures. Brenton et al. (2005) compared \\ntwo gene lists predictive of breast cancer prognosis and found only 3 genes in common. Even though the intersection of specific gene lists is poor, the highly correlated nature of microarray data means that many gene lists may have similar prediction accuracy (Ein-Dor et al., 2004). Gene signatures identified from dif -\\nferent breast cancer studies with few genes in common were shown to have comparable success in predicting patient survival (Buyse et al., 2006). \\nCommonly used supervised learning algorithms \\nyield black box models prompting the need for interpre-table models that provide insights about the underlying biological mechanism that produced the data. \\nNetwork Analysis\\nBayesian networks (BNs), derived from an alliance between graph theory and probability theory, can capture dependencies among many variables (Pearl, 1988, Heckerman, 1996). Friedman et al. (2000) introduced a multinomial \\nmodel framework for BNs to reverse-engineer networks and showed that this method differs from clustering in that it can discover gene interactions other than cor-relation when applied to yeast gene expression data. Spirtes et al. (2002) highlight some of the difficulties of \\napplying this approach to microarray data. Nevertheless, many extensions of this research direction have been explored. Correlation is not necessarily a good predictor of interactions, and weak interactions are essential to understand disease progression. Identifying the biologi-cally meaningful interactions from the spurious ones is challenging, and BNs are particularly well-suited for modeling stochastic biological processes. \\nThe exponential growth of data produced by mi-\\ncroarray technology as well as other high-throughput data (e.g. protein-protein interactions) call for novel AI approaches as the paradigm shifts from a reductionist to a mechanistic systems view in the life sciences.\\nFUTURE TRENDS\\nUncovering the underlying biological mechanisms that generate these data is harder than prediction and has the potential to have far reaching implications for understanding disease etiologies. Time series analysis (Bar-Joseph, 2004) is a first step to understanding the dynamics of gene regulation, but, eventually, we need to use the technology not only to observe gene expression data but also to direct intervention experiments (Pe’er et al. , 2001, Yoo et al. , 2002) and develop methods to \\ninvestigate the fundamental problem of distinguishing correlation from causation. \\nCONCLUSION\\nWe have reviewed AI methods for pre-processing, clustering, feature selection, classification and mecha -\\nnistic analysis of microarray data. The clusters, gene lists, molecular fingerprints and network hypotheses produced by these approaches have already shown impact; from discovering new disease subtypes and biological markers, predicting clinical outcome for directing treatment as well as unraveling gene networks. From the AI perspective, this field offers challenging problems and may have a tremendous impact on biol-ogy and medicine. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='038f5a28-32a1-4d91-aaba-062742454df6', embedding=None, metadata={'page_label': '104', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18AI Methods for Analyzing Microarray Data\\nAREFERENCES\\nAlizadeh A.A., Eisen M.B., Davis R.E., Ma C., Lossos \\nI.S., Rosenwald A., et al. (2000). Distinct types of diffuse \\nlarge B-cell lymphoma identified by gene expression profiling. Nature, 403(6769), 503-11.\\nAmbroise C., & McLachlan G.J. (2002). Selection bias \\nin gene extraction on the basis of microarray gene-ex-pression data. Proceedings of the National Academy \\nof Sciences,  99(10), 6562-6.\\nBar-Joseph Z. (2004). Analyzing time series gene ex-pression data. Bioinformatics, 20 (16), 2493-503.\\nBloom G., Yang I.V ., Boulware D., Kwong K.Y ., Cop-pola D., Eschrich S., et al. (2004). Multi-platform, \\nmulti-site, microarray-based human tumor classifica-tion. American Journal of Pathology, 164(1), 9-16.\\nBrenton J.D., Carey L.A., Ahmed A.A., & Caldas C. \\n(2005). Molecular classification and molecular fore -\\ncasting of breast cancer: ready for clinical application? Journal of Clinical Oncology, 23(29), 7350-60. \\nBrazma A., & Culhane AC. (2005). Algorithms for \\ngene expression analysis. In  Jorde LB., Little PFR, Dunn MJ., Subramaniam S. (Eds.) Encyclopedia of \\nGenetics, Genomics, Proteomics and Bioinformatics., (3148 -3159) London: John Wiley & Sons. \\nBuyse, M., Loi S., Van’t Veer L., Viale G., Delorenzi \\nM., Glas A.M., et al. (2006). Validation and clinical \\nutility of a 70-gene prognostic signature for women with node-negative breast cancer. Journal of the National \\nCancer Institute, 98 , 1183-92.\\nCauston H.C., Quackenbush J., & Brazma A. (2003) Mi-croarray Gene Expression Data Analysis: A Beginner’s Guide. Oxford: Blackwell Science Limited. \\nDupuy A., & Simon RM. (2007). Critical review of \\npublished microarray studies for cancer outcome and guidelines on statistical analysis and reporting. Journal \\nof the National Cancer Institute, 99 (2), 147-57.\\nEin-Dor L., Kela I., Getz G., Givol D., & Domany E. (2004). Outcome signature genes in breast cancer: is there a unique set? Bioinformatics, 21 (2), 171-8.\\nEisen M.B., Spellman P.T., Brown P.O., & Botstein D. (1998). Cluster analysis and display of genome-wide expression patterns. Proceedings of the National \\nAcademy of Sciences,  95, 14863-14868.Friedman N., Linial M., Nachman I., & Pe’er D. (2000). Using Bayesian networks to analyze expression data. Journal of Computational Biology, 7(3-4), 601-20.\\nGolub T. R., Slonim D. K., Tamayo P., Huard C., \\nGaasenbeek M., Mesirov J. P., et al. (1999). Molecular \\nClassification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring. Science, \\n286 (5439), 531. \\nGuyon, I., & Elisseff, A. (2003). An introduction to \\nvariable and feature selection. Journal of Machine \\nLearning Research, 3, 1157-1182.\\nHastie T., Tibshirani R., & Friedman J. (2001). The \\nElements of Statistical Learning: Data Mining, Infer-ence, and Prediction. New York: Springer Series in Statistics.\\nHeckerman D. (1996). A Tutorial on Learning with \\nBayesian Networks. Technical Report MSR-TR-95-06. Microsoft Research.\\nInza I., Larrañaga P., Blanco R., & Cerrolaza A.J. \\n(2004). Filter versus wrapper gene selection approaches in DNA microarray domains. Artificial Intelligence in \\nMedicine, special issue in “Data mining in genomics and proteomics”, 31(2), 91-103.\\nKohavi R., & John G.H. (1997). Wrappers for feature \\nsubset selection, Artificial Intelligence, 97(1-2), 273-\\n324.\\nNutt C.L., Mani D.R., Betensky R.A., Tamayo P., \\nCairncross J.G., Ladd C., et al. (2003). Gene Expression-\\nbased Classification of Malignant Gliomas Correlates Better with Survival than Histological Classification. Cancer Research,  63, 1602-1607.\\nPe’er D, Regev A, Elidan G, & Friedman N. (2001). Inferring subnetworks from perturbed expression pro-files. Bioinformatics, 17 S1 , S215-24.\\nPearl J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, San Mateo: Morgan Kaufmann Publishers.\\nQuackenbush J. (2002). Microarray data normalization \\nand transformation, Nature Genetics, 32 , 496–501.\\nQuackenbush J. (2006). Microarray Analysis and Tumor Classification. The New England Journal of Medicine, \\n354(23), 2463-72.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ead24ea7-8910-43a2-9860-d00bb3de3f9c', embedding=None, metadata={'page_label': '105', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  AI Methods for Analyzing Microarray Data\\nSimon R., Radmacher M.D., Dobbin K., & McShane \\nL.M. (2003). Pitfalls in the use of DNA microarray data for diagnostic and prognostic classification. Journal of \\nthe National Cancer Institute, 95 (1), 14-8.\\nSpirtes, P., Glymour, C., Scheines, R. Kauffman, S., Aimale, V ., & Wimberly, F. (2001). Constructing Bayes-ian Network Models of Gene Expression Networks from Microarray Data. Proceedings of the Atlantic \\nSymposium on Computational Biology, Genome In-formation Systems and Technology.\\nStatnikov A., Aliferis C.F., Tsamardinos I., Hardin \\nD., & Levy S. (2005). A comprehensive evaluation of multicategory classification methodsfor microarray gene expression cancer diagnosis. Bioinformatics, \\n21(5), 631-643\\nTroyanskaya O., Cantor M., Sherlock G., Brown P., \\nHastie T., Tibshirani R., et al. (2001). Missing value \\nestimation methods for DNA microarrays. Bioinformat-\\nics, 17(6), 520-5.\\nTusher V .G., Tibshirani R., & Chu G. (2001). Signifi -\\ncance analysis of microarrays applied to the ionizing \\nradiation response. Proceedings of the National Acad-\\nemy of Sciences, 98 (9), 5116-5121.\\nVarma, S., & Simon, R. (2006). Bias in error estimation when using cross-validation for model selection. BMC \\nBioinformatics,  7, 91\\nWitten, I. H. & Frank, E. (2000). Data Mining: Practi-cal Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann Publishers Inc.\\nWall, M., Rechtsteiner, A., & Rocha, L. (2003). Singular \\nvalue decomposition and principal component analy-sis. In D.P. Berrar, W. Dubitzky, M. Granzow (Eds.) A Practical Approach to Microarray Data Analysis. (91-109). Norwell: Kluwer.\\nWouters, L., Gohlmann, H.W., Bijnens, L., Kass, \\nS.U., Molenberghs, G., & Lewi, P.J. (2003). Graphi-cal exploration of gene expression data: a comparative study of three multivariate methods. Biometrics, 59, \\n1131-1139Yoo C., Thorsson V ., & Cooper G.F. (2002). Discovery of causal relationships in a gene-regulation pathway from a mixture of experimental and observational DNA microarray data. Biocomputing: Proceedings of \\nthe Pacific Symposium, 7 , 498-509\\nKEy TERMS\\nCurse of Dimensionality: A situation where the \\nnumber of features (genes) is much larger than the number of instances (biological samples) which is known in statistics as p >> n problem.\\nFeature Selection: A problem of finding a subset (or \\nsubsets) of features so as to improve the performance of learning algorithms. \\nMicroarray:  A microarray is an experimental assay \\nwhich measures the abundances of mRNA (intermedi-ary between DNA and proteins) corresponding to gene expression levels in biological samples.\\nMultiple testing problem: A problem that occurs \\nwhen a large number of hypotheses are tested simul-taneously using a user-defined α cut off p-value which may lead to rejecting a non-negligible number of null hypotheses by chance.  \\nOver-Fitting: A situation where a model learns \\nspurious relationships and as a result can predict training data labels but not generalize to predict future data.\\nSupervised Learning: A learning algorithm that \\nis given a training set consisting of feature vectors as-sociated with class labels and whose goal is to learn a classifier that can predict the class labels of future instances.\\nUnsupervised Learning: A learning algorithm that \\ntries to identify clusters based on similarity between features or between instances or both but without tak-ing into account any prior knowledge. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='596a2c6b-d60f-45c5-b55a-5a8ff14c77cd', embedding=None, metadata={'page_label': '106', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAn AI Walk from Pharmacokinetics to  \\nMarketing\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nThis work is intended for providing a review of real-\\nlife practical applications of Artificial Intelligence (AI) \\nmethods. We focus on the use of Machine Learning (ML) methods applied to rather real problems than synthetic problems with standard and controlled environment. In particular, we will describe the following problems in next sections:\\n• Optimization of Erythropoietin (EPO) dosages \\nin anaemic patients undergoing Chronic Renal \\nFailure (CRF). \\n• Optimization of a recommender system for citizen \\nweb portal users.\\n• Optimization of a marketing campaign.\\nThe choice of these problems is due to their \\nrelevance and their heterogeneity. This heterogeneity shows the capabilities and versatility of ML methods to solve real-life problems in very different fields of knowledge. The following methods will be mentioned during this work: \\n• Artificial Neural Networks (ANNs): Multilayer \\nPerceptron (MLP), Finite Impulse Response (FIR) \\nNeural Network, Elman Network, Self-Oganizing Maps (SOMs) and Adaptive Resonance Theory (ART).\\n• Other clustering algorithms: K-Means, Expec-\\ntation-Maximization (EM) algorithm, Fuzzy C-Means (FCM), Hierarchical Clustering Algo-rithms (HCA).• Generalized Auto-Regressive Conditional Het-\\neroskedasticity (GARCH).\\n• Support Vector Regression (SVR).• Collaborative filtering techniques.• Reinforcement Learning (RL) methods.\\nBACKGROUND\\nThe aim of this communication is to emphasize the capabilities of ML methods to deliver practical and effective solutions in difficult real-world applications. In order to make the work easy to read we focus on each of the three separate domains, namely, Pharmacokinetics (PK), Web Recommender Systems and Marketing.\\nPharmacokinetics\\nClinical decision-making support systems have used Artificial Intelligence (AI) methods since the end of the fifties. Nevertheless, it was only during the nineties that decision support systems were routinely used in clinical practice on a significant scale. In particular, ANNs have been widely used in medical applications the last two decades (Lisboa, 2002). One of the first relevant studies involving ANNs and Therapeutic Drug Monitoring was (Gray, Ash, Jacobi, & Michel, 1991). In this work, an ANN-based drug interaction warning system was developed with a computerized real-time entry medical records system. A reference work in this field is found in (Brier, Zurada, & Aronoff, 1995), in which the capabilities of ANNs and NONMEN are benchmarked. José D. Martín-GuerreroUniversity of Valencia, Spain\\nEmilio Soria-Olivas\\nUniversity of Valencia, Spain\\nPaulo J.G. Lisboa\\nLiverpool John Moores University, UK\\nAntonio J. Serrano-López\\nUniversity of Valencia, Spain', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9adf397-af33-44ab-a5a8-c1813b32c696', embedding=None, metadata={'page_label': '107', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  An AI Walk from Pharmacokinetics to Marketing\\nFocusing on problems that are closer to the real-\\nlife application that will be described in next section, \\nthere are also a number of recent works involving the use of ML for drug delivery in kidney disease. For instance, a comparison of renal-related adverse drug reactions between rofecoxib and celecoxib, based on the WHO/Uppsala Monitoring Centre safety database, was carried out by (Zhao, Reynolds, Lejkowith, Whelton, & Arellano, 2001). Disproportionality in the association between a particular drug and renal-related adverse drug reactions was evaluated using a Bayesian confidence propagation neural network method. A study of prediction of cyclosporine dosage in patients after kidney transplantation using neural networks and kernel-based methods was carried out in (Camps et al., 2003). In (Gaweda, Jacobs, Brier, & Zurada, 2003), a pharmacodynamic population analysis in CRF patients using ANNs was performed. Such models allow for adjusting the dosing regime. Finally, in (Martín et al., 2003) , the use of neural networks was proposed for the optimization of EPO dosage in patients undergoing anaemia connected with CRF. \\nWeb Recommender Systems\\nRecommender systems are widely used in web sites including Google. The main goal of these systems is to recommend objects which a user might be interested in. Two main approaches have been used: content-based and collaborative filtering (Zukerman & Albrecht, 2001), although other kinds of techniques have also been proposed (Burke, 2002). \\nCollaborative recommenders aggregate ratings \\nof recommendations of objects, find user similarities based on their ratings, and finally provide new recommendations based on inter-user comparisons. Some of the most relevant systems using this technique are GroupLens/NetPerceptions and Recommender. The main advantage of collaborative techniques is that they are independent from any machine-readable representation of the objects, and that they work well for complex objects where subjective judgements are responsible for much of the variation in preferences.\\nContent-based learning is used when a user’s past \\nbehaviour is a reliable indicator of his/her future behaviour. It is particularly suitable for situations in which users tend to exhibit idiosyncratic behaviour. However, this approach requires a system to collect relatively large amounts of data from each user in order to enable the formulation of a statistical model. Examples of systems of this kind are text recommendation systems like the newsgroup filtering system, NewsWeeder, which uses words from its texts as features. \\nMarketing\\nThe latest marketing trends are more concerned about maintaining current customers and optimizing their behaviour than getting new ones. For this reason, relational marketing focuses on what a company must do to achieve this objective. The relationships between a company and its costumers follow a sequence of action-response system, where the customers can modify their behaviour in accordance with the marketing actions developed by the company. \\nThe development of a good and individualized \\npolicy is not easy because there are many variables to take into account. Applications of this kind can be viewed as a Markov chain problem, in which a company decides what action to take once the customer properties in the current state (time t), are known. \\nReinforcement Learning (RL) can be used to solve this task since previous applications have demonstrated its suitability in this area. In (Sun, 2003), RL was applied to analyse mailing by studying how an action in time t influences actions in following times. In (Abe et al., 2002) and (Pednault, Abe & Zadrozny., 2002), several RL algorithms were benchmarked in mailing problems. In (Abe, 2004), RL was used to optimize cross channel marketing.\\nAI CONTRIBUTIONS IN REAL-LIFE  APPLICATIONS\\nPrevious section showed a review of related work. In this section, we will focus on showing authors’ experience in using AI to solve real-life problems. In order to show up the versatility of AI methods, we will focus on particular applications from three different fields of knowledge, the same that were reviewed in previous section.\\nPharmacokinetics \\nAlthough  we have also worked with other pharmacokinetic problems, in this work, we focus on maybe the most relevant problem, which is the ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fdac24c0-aaf2-4637-9f5e-61c0aac85f69', embedding=None, metadata={'page_label': '108', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18An AI Walk from Pharmacokinetics to Marketing\\nAoptimization of EPO dosages in patients within a \\nhaemodialysis program. Patients who suffer from CRF tend to suffer from an associated anaemia, as well. EPO is the treatment of choice for this kind of anaemia. The use of this drug has greatly reduced cardiovascular problems and the necessity of multiple transfusions. However, EPO is expensive, making the already costly CRF program even more so. Moreover, there are significant risks associated with EPO such as thrombo-embolisms and vascular problems, if Haemoglobin (Hb) levels are too high or they increase too fast.  Consequently, optimizing dosage is critical to ensure adequate pharmacotherapy as well as a reasonable treatment cost.\\nPopulation models, widely used by Pharmacoki-\\nnetics’ researchers, are not suitable for this problem since the response to the treatment with EPO is highly dependent on the patient. The same dosages may have very different responses in different patients, most notably the so-called EPO-resistant patients, who do not respond to EPO treatment, even after receiving high dosages. Therefore, it is preferable to focus on an individualized treatment.\\nOur first approach to this problem was based on \\npredicting the Hb level given a certain administered dose of EPO. Although the final goal is to individualize EPO doses, we did not predict EPO dose but Hb level. The reason is that EPO predictors would model physician’s protocol whereas Hb predictors model body’s response to the treatment, hence being a more “objective” approach. In particular, the following models were used: GARCH (Hamilton, 1994), MLP, FIR neural network, Elman’s recurrent neural network and SVR (Haykin, 1999). Accurate prediction models were obtained, especially when using ANNs and SVR. Dynamic neural networks (i.e., FIR and recurrent) did not outperform notably the static MLP probably due to the short length of the time series (Martín et al., 2003). An easy-to-use software application was developed to be used by clinicians, in which after filling in patients’ data and a certain EPO dose, the predicted Hb level for next month was shown. \\nAlthough prediction models were accurate, we \\nrealized that this prediction approach had a major flaw. Despite obtaining accurate models, we had not yet achieved a straightforward way to transfer the extracted knowledge to daily clinical practice, because clinicians had to “play” with different doses to analyse the best solution to attain a certain Hb level. It would be better to have an automatic model that suggests the actions to be made in order to attain the targeted range of Hb, rather than this “indirect” approach. This reflection made us research on new models, and we came up with the use of RL (Sutton & Barto, 1998). We are currently working on this topic but we have already achieved promising results, finding policies (sequence of actions) that appear to be better than those followed in the hospital, i.e., there are a higher number of patients within the desired target of Hb at the end of the treatment (Martín et al., 2006a).\\nWeb Recommender Systems\\nA completely different application is described in this subsection, namely, the development of web recommender systems. The authors proposed a new approach to develop recommender systems based on collaborative filtering, but also including an analysis of the feasibility of the recommender by using a prediction stage (Martín et al., 2006b).\\nThe very basic idea was to use clustering algorithms \\nin order to find groups of similar users. The following clustering algorithms were taken into account: K-Means, FCM, HCA, EM algorithm, SOMs and ART. New users were assigned to one of the groups found by these clustering algorithms, and then they were recommended with web services that were usually accessed by other users of his/her same group, but had not yet been accessed by these new users (in order to maximize the usefulness of the approach). Using controlled data sets, the study concluded that ART and SOMs showed a very good behaviour with data sets of very different characteristics, whereas HCA and EM showed an acceptable behaviour provided that the dimensionality of the data set was not too high and the overlap was slight. Algorithms based on K-Means achieved the most limited success in the acceptance of offered recommendations.\\nEven though the use of  RL was only slightly \\nstudied, it seems to be a suitable choice for this problem, since the internal dynamics of the problem is easily tackled by RL, and moreover the interference between the recommendation interface and the user can be minimized with an adequate definition of the rewards (Hernández, Gaudioso, & Boticario, 2004).', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad072f02-4f9c-4ea2-8eb0-e4a583df5744', embedding=None, metadata={'page_label': '109', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  An AI Walk from Pharmacokinetics to Marketing\\nMarketing\\nThe last application that will be mentioned in this \\ncommunication is related to marketing. One way to increase the loyalty of customers is by offering them the opportunity to obtain some gifts as the result of their purchases from a certain company. The company can give virtual credits to anyone who buys certain \\narticles, typically those that the company is interested in promoting. After a certain number of purchases, the customers can exchange their virtual credits for the gifts offered by the company. The problem is to establish the appropriate number of virtual credits for each promoted item. In accordance with the company policy, it is expected that the higher the credit assignment, the higher the amount of purchases. However, the company’s profits are lower since the marketing campaign adds an extra cost to the company. The goal is to achieve a trade-off by establishing an optimal policy.\\nWe proposed a RL approach to optimize this \\nmarketing campaign. This particular application, whose characteristics are described below, is much more difficult than the other RL approaches to marketing mentioned in the Background Section. This is basically because there are many more different actions that can be taken. The information used for the study corresponds to five months of the campaign, involving 1,264,862 transactions, 1,004 articles and 3,573 customers. \\nRL can deal with intrinsic dynamics, and besides, it \\nhas the attractive advantage that is able to maximize the so-called long-term reward. This is especially relevant in this application since the company is interested in maximizing the profits at the end of the campaign, and a customer who do not produce much profits in the first months of the campaign, may however make many profitable transactions in the future . \\nOur first results showed that profits using a policy \\nbased on RL instead of the policy followed by the company so far, could even double long-term profits at the end of the campaign (Gómez et al., 2005).\\nCONCLUSION AND FUTURE TRENDS\\nThis paper has shown the capabilities and versatility of different AI methods to be applied to real-life problems, illustrated with three specific applications in different domains. Clearly, the methodology is generic and applies equally well to many other fields, provided that the information contained in the data is sufficiently rich to require non-linear modelling and is capable of supporting a predictive performance that is of practical value.\\nAs a next future trend, it should be emphasized \\nthat AI methods are increasingly popular for business applications in recent years, challenging classical business models. \\nIn the particular case of  RL, the commercial potential \\nof this powerful methodology has been significantly underestimated, as it is applied almost exclusively to Robotics. We feel that it is a methodology still to be exploited in many real applications, as we have shown in this paper.\\nREFERENCES\\nAbe, N., Pednault, E., Wang, H., Zadrozny, B., Wei, F., & Apte, C. (2002). Empirical comparison of various reinforcement learning strategies for sequential targeted marketing. Proceedings of the ICDM 2002 , 315-321.\\nAbe, N., Verma, N., Schroko, R. & Apte, C. (2004). Cross-channel optimized marketing by reinforcement learning. Proceedings of the KDD 2004 , 767-772.\\nBrier, M. E., Zurada, J. M., & Aronoff, G. R. (1995). Neural network predicted peak and trough gentamicin concentrations. Pharmaceutical Research , 12 (3), \\n406-412.\\nBurke, R. (2002). Hybrid recommender systems: Survey \\nand experiments. User Modeling and User-Adapted \\nInteraction , 12, 331-370.\\nCamps, G., Porta, B., Soria, E., Martín, J. D., Serrano, A. J., Pérez, J. J., & Jiménez, N. V . (2003). Prediction of cyclosporine dosage in patients after kidney transplantation using neural networks. IEEE \\nTransactions on Biomedical Engineering , 50 (4), \\n442-448.\\nGaweda, A. E., Jacobs, A. A., Brier, M. E., & Zurada, \\nJ. M. (2003). Pharmacodynamic population analysis in chronic renal failure using artificial neural networks – a comparative study. Neural Networks , 16 (5-6), \\n841-845.\\nGómez, G., Martín, J. D., Soria, E., Palomares, A., \\nBalaguer, E., Casariego, N.,, &  Paglialunga, D. (2005). An approach based on reinforcement learning and ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c7cdec79-1e7a-4660-8810-73020ca377fd', embedding=None, metadata={'page_label': '110', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18An AI Walk from Pharmacokinetics to Marketing\\nASeelf-Organizing Maps to design a marketing campaign. \\nProceedings of the 2nd International Conference on \\nMachine Intelligence ACIDCA-ICMI 2005, 259-265.\\nGray, D. L., Ash, S. R., Jacobi, J. & Michel, A. N. \\n(1991). The training and use of an artificial neural network to monitor use of medication in treatment of complex patients. Journal of Clinical Engineering, 16 \\n(4), 331-336.\\nHamilton, J. D. (1994). Time Series Analysis, Princeton \\nUniversity Press, Princeton NJ, USA.Haykin, S. (1999). Neural Networks (2nd ed.). Prentice \\nHall, Englewood Cliffs, NJ, USA.Hernández, F., Gaudioso, E. & Boticario, J. G. (2004) \\nA reinforcement approach to achieve unobstrusive and interactive recommendation systems for web-based communities. Proceedings of Adaptive Hypermedia \\n2004, 409-412.  \\nLisboa, P. J. G. (2002). A review of evidence of health \\nbenefit from artificial neural networks in medical intervention. Neural Networks, 15 (1), 11-39. \\nMartín, J. D., Soria, E., Camps, G., Serrano, A. J., Pérez, J. J., & Jiménez, N. V . (2003). Use of neural networks for dosage indidualisation of erythropoietin in patients with secondary anemia to chronic renal failure. Computers \\nin Biology and Medicine,  33 (4), 361-373.\\nMartín, J. D., Soria, E., Chorro, V ., Climente. M., & Jiménez, N. V . (2006a). Reinforcement Learning for anemia management in hemodialysis patients treated with erythropoietic stimulating factors. Proceedings \\nof the Workshop “Planning, Learning and Monitoring with uncertainty and dynamic worlds”, European Conference on Artificial Intelligence 2006, 19-24. \\nMartín, J. D., Palomares, A., Balaguer, E., Soria, E., Gómez, J., & Soriano, A. (2006b) Studying the feasibility of a recommender in a citizen web portal based on user modeling and clustering algorithms. Expert Systems with Aplications , 30 (2), 299-312.\\nPednault, E., Abe, N., & Zadrozny, B. (2002). Sequential cost-sensitive decision making with reinforcement learning. Proceedings of the Eighth ACM SIGKDD \\nInternational Conference on Knowledge Discovery and Data Mining 2002, 259-268. \\nSun, P. (2003). Constructing learning models from \\ndata: The dynamic catalog mailing problem.  Ph. D. \\nDissertation, Tsinghua University, China.Sutton, R. S., & Barto, A. G. (1998). Reinforcement \\nLearning: An Introduction. MIT Press, Cambridge, \\nMA, USA.\\nZhao, S. Z., Reynolds, M. W., Leikowith, J., Whelton, \\nA., & Arellano, F. M. (2001). A comparison of renal-related adverse drug reactions between rofecoxib and celecoxib, based on World Health Organization/Uppsala Monitoring Centre safety database. Clinical \\nTherapeutics, 23 (9), 1478-1491.\\nZukerman, I., & Albrecht, D. (2001). Predictive \\nstatistical models for user modeling. User Modeling \\nand User-Adapted Interaction , 11, 5-18.\\nKEy TERMS\\nAgent:  In RL terms, it is the responsible of \\nmaking decisions according to observations of its environment.\\n \\nEnvironment: In RL terms, it is every external \\ncondition to the agent.\\nExploration-Explotation Dilemma: It is a classical \\nRL dilemma, in which a trade-off solution must be achieved. Exploration means random search of new actions in order to achieve a likely (but yet unknown) better reward than all the known ones, while explotation is focused on exploiting the current knowledge for the maximization of the reward ( greedy approach).\\nLife-Time Value: It is a measure widely used in \\nmarketing applications that offers the long-term result that has to be maximized.\\nReward: In RL terms, the immediate reward is \\nthe value returned by the environment to the agent depending on the taken action. The long-term reward is the sum of all the immediate rewards throughout a complete decision process.\\nSensitivity: Similar measure that offers the ratio \\nof positives that are correctly classified by the model. (Refer to Specificity.)\\nSpecificity: Success rate measure in a classification \\nproblem. If there are two classes (namely, positive and negative), specificity measures the ratio of negatives that are correctly classified by the model.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99043693-2d8c-453d-88c5-0e2b9529d7e6', embedding=None, metadata={'page_label': '111', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"\\x18\\x18\\nAlgorithms for Association Rule Mining\\nVasudha Bhatnagar\\nUniversity of Delhi, India\\nAnamika Gupta\\nUniversity of Delhi, India\\nNaveen Kumar\\nUniversity of Delhi, India\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nAssociation Rule Mining (ARM) is one of the important data mining tasks that has been extensively researched by data-mining community and has found wide applica-tions in industry. An Association Rule is a pattern that implies co-occurrence of events or items in a database. Knowledge of such relationships in a database can be employed in strategic decision making in both com-mercial and scientific domains.\\nA typical application of ARM is market basket \\nanalysis where associations between the different items are discovered to analyze the customer’s buying habits. The discovery of such associations can help to develop better marketing strategies. ARM has been extensively used in other applications like spatial-temporal, health care, bioinformatics, web data etc (Hipp J., Güntzer U., Nakhaeizadeh G. 2000). \\nAn association rule is an implication of the form \\nX → Y where X and Y are independent sets of attri-\\nbutes/items. An association rule indicates that if a set of items X occurs in a transaction record then the set of \\nitems Y also occurs in the same record. X is called the \\nantecedent of the rule and Y is called the consequent of \\nthe rule. Processing massive datasets for discovering co-occurring items and generating interesting rules in reasonable time is the objective of all ARM algorithms.  The task of discovering co-occurring sets of items cannot be easily accomplished using SQL, as a little reflection will reveal. Use of ‘Count’ aggregate query requires the condition to be specified in the where clause, which finds the frequency of only one set of items at a time. In order to find out all sets of co-oc -\\ncurring items in a database with n items, the number \\nof queries that need to be written is exponential in n. \\nThis is the prime motivation for designing algorithms for efficient discovery of co-occurring sets of items, which are required to find the association rules.\\nIn this article we focus on the algorithms for asso-\\nciation rule mining (ARM) and the scalability issues in ARM. We assume familiarity of the reader with the motivation and applications of association rule mining\\nBACKGROUND\\nLet I = {i1, i2,…, in} denote a set of items and D denote \\na database of N transactions. A typical transaction T∈D \\nmay contain a subset X of the entire set of items I and \\nis associated with a unique identifier TID. An item-set \\nis a set of one or more items i.e. X is an item-set if \\nX ⊆ I. A k-item-set is an item-set of cardinality k. A \\ntransaction is said to contain an item-set X if X ⊆ T. \\nSupport of an item set X, also called Coverage is the \\nfraction of transactions that contain X.  It denotes the \\nprobability that a transaction contains X. \\nNX containingns transactio of NoX P X Support.) ( ) (= =\\nAn item-set having support greater than the user \\nspecified support threshold (ms) is known as frequent \\nitem-set.\\nAn association rule is an implication of the form X \\n→Y [Support, Confidence] where X ⊂ I, Y ⊂ I and X∩Y \\n=∅, where Support and Confidence are rule evaluation \\nmetrics. Support of a rule X → Y  in D is ‘ S'’ if S% of \\ntransactions in D contain X ∪ Y. It is computed as:\\n \\nNY X containing n transactio of NoY X P Y X Support∪= ∪ = →.) ( ) (\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='707548a7-87c9-40b5-b127-e3e4fccc47ae', embedding=None, metadata={'page_label': '112', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"  \\x18\\x18Algorithms for Association Rule Mining\\nASupport indicates the prevalence of a rule. In a \\ntypical market basket analysis application, rules with \\nvery low support values represent rare events and are likely to be uninteresting or unprofitable. Confidence \\nof a rule measures its strength and provides an indica-tion of the reliability of prediction made by the rule. A rule X → Y  has a confidence ‘C'‘ in D if C % of \\ntransactions in D that contain X, also contain Y. Con-\\nfidence is computed, as the conditional probability of Y occuring in a transaction, given X is present in the \\nsame transaction, i.e.\\n) () (\\n) () () ( ) (X SupportY X Support\\nX PY X P\\nXYP Y X Confidence∪=∪= = →\\nA rule generated from frequent item-sets is strong \\nif its confidence is greater than the user specified confidence threshold (mc). Fig. 1 shows an example database of five transactions and shows the computa-tion of support and confidence of a rule.\\nThe objective of Association Rule Mining algo-\\nrithms is to discover the set of strong rules from a given database as per the user specified ms and mc thresholds. \\nAlgorithms for ARM essentially perform two distinct tasks: (1) Discover frequent item-sets. (2) Generate strong rules from frequent item-sets.\\nThe first task requires counting of item-sets in \\nthe database and filtering against the user specified threshold (ms). The second task of generating rules from frequent item-sets is a straightforward process of generating subsets and checking for the strength. We describe below the general approaches for finding frequent item-sets in association rule mining algorithms. The second task is trivial as explained in the last sec-tion of the article.APPROACHES FOR GENERATING FREQUENT ITEM-SETS \\nIf we apply a brute force approach to discover frequent item-sets, the algorithm needs to maintain counters for all 2\\nn - 1 item-sets. For large values of n that are common \\nin the datasets being targeted for mining, maintaining such large number of counters is a daunting task. Even if we assume availability of such large memory, indexing of these counters also presents a challenge. Data mining researchers have developed numerous algorithms for efficient discovery of frequent item-sets.\\nThe earlier algorithms for ARM discovered all \\nfrequent item-sets. Later it was shown by three inde-pendent groups of researchers (Pasquier N., Bastide Y ., Taouil R. & Lakhal L. 1999), (Zaki M.J. 2000), (Stumme G., 1999), that it is sufficient to discover frequent closed item-sets (FCI) instead of all frequent item-sets (FI). FCI are the item-sets whose support is not equal to the support of any of its proper superset. FCI is a reduced, complete and loss less representa-tion of frequent item-sets. Since FCI are much less in number than FI, computational expense for ARM is drastically reduced. \\nFigure 2 summarizes different approaches used for \\nARM. We briefly describe these approaches.\\nDiscovery of Frequent Item-Sets\\nLevel-Wise Approach \\nLevel wise algorithms start with finding the item-sets of \\ncardinality one and gradually work up to the frequent item-sets of higher cardinality. These algorithms use anti-monotonic property of frequent item-sets accord-\\nFigure 1. Computation of support and confidence of a rule in an example database\\n Let ms=40%, mc=70%  \\nConsider the association rule  B ,  \\nsupport ( ) = 3/5 = 60% \\nconfidence( ) = support( BD)/support( B) \\n=  3/4 = 75%  \\nThe rule is a strong rule. TID  Items \\n1  BCD \\n2  BCDE \\n3  AC \\n4  BDE \\n5  AB \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6f9a7d8-7378-4103-b90f-a0d9e9a82462', embedding=None, metadata={'page_label': '113', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Algorithms for Association Rule Mining\\ning to which, no superset of an infrequent item-set can \\nbe frequent.\\nAgarwal et al. (Agarwal, R., Imielinski T., & Swami \\nA. 1993), (Agarwal, R., & Swami A., 1994) proposed Apriori algorithm, which is the most popular iterative algorithm in this category. It starts, with finding the frequent item-sets of size one and goes up level by level, finding candidate item-sets of size k by joining \\nitem-sets of size k-1. Two item-sets, each of size k-1 \\njoin to form an item-set of size k if and only if they have \\nfirst k-2 items common. At each level the algorithm \\nprunes the candidate item-sets using anti-monotonic property and subsequently scans the database to find the support of pruned candidate item-sets. The process continues till the set of frequent item-sets is non-empty. Since each iteration requires a database scan, maximum number of database scans required is same as the size of maximal item-set. Fig. 3 and Fig 4 gives the pseudo code of Apriori algorithm and a running example respectively. \\nTwo of the major bottlenecks in Apriori algorithm \\nare i) number of passes and ii) number of candidates generated. The first is likely to cause I/O bottleneck and the second causes heavy load on memory and CPU usage. Researchers have proposed solutions to these problems with considerable success. Although detailed discussion of these solutions is beyond the scope of this article, a brief mention is necessary.\\nHash techniques reduce the number of candidates \\nby making a hash table and discarding a bucket if it has support less than the ms. Thus at each level memory \\nrequirement is reduced because of smaller candidate set. The reduction is most significant at lower levels. Maintaining a list of transaction ids for each candidate set reduces the database access. Dynamic Item-set Counting algorithm reduces the number of scans by counting candidate sets of different cardinality in a single scan (Brin S., Motwani R., Ullman J.D., & Tsur S. 1997). Pincer Search algorithm uses a bi-directional strategy to prune the candidate set from top (maximal) and bottom (1-itemset) (Lin D. & Kedem Z.M. 1998). Partitioning and Sampling strategies have also been proposed to speed up the counting task. An excellent comparison of Apriori algorithm and its variants has been given in (Hipp J., Güntzer U., Nakhaeizadeh G. 2000).\\nTree Based Algorithms\\nTree based algorithms have been proposed to overcome the problem of multiple database scans. These algo-rithms compress (sometimes lossy) the database into a tree data structure and reduce the number of database scans appreciably. Subsequently the tree is used to mine for support of all frequent item-sets.\\nSet-Enumeration tree used in Max Miner algorithm \\n(Bayardo R.J. 1998) orders the candidate sets while searching for maximal frequent item-sets. The data structure facilitates quick identification of long frequent item-sets based on the information gathered during each pass. The algorithm is particularly suitable for dense databases with maximal item-sets of high cardinality. \\nHan et. al. (Han, J., Pei, J., & Yin, Y . 2000) pro-\\nposed Frequent Pattern (FP)-growth algorithm which performs a database scan and finds frequent item-sets of cardinality one. It arranges all frequent item-sets in a table (header) in the descending order of their sup-ports. During the second database scan, the algorithm constructs in-memory data structure called FP-Tree by inserting each transaction after rearranging it in descending order of the support. A node in FP-Tree stores a single attribute so that each path in the tree ARM Algorithms  \\nFrequent Item-sets  Frequent Closed Item-sets \\nLevel-wise   Tree Based  Level-wise   Tree Based  La  \\n Figure 2. Approaches for ARM algorithms', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d7c43c6-15ec-4a7b-8aae-766403fbc590', embedding=None, metadata={'page_label': '114', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Algorithms for Association Rule Mining\\nAFigure 3. Apriori algorithm\\nrepresents and counts the corresponding record in the \\ndatabase. A link from the header connects all the nodes of an item. This structural information is used while mining the FP-Tree. FP-Growth algorithm recursively generates sub-trees from FP-Trees corresponding to each frequent item-set.\\nCoenen et. al. (Coenen F., Leng P., & Ahmed \\nS. 2004) proposed Total Support Tree (T-Tree) and Partial Support Tree (P-Tree) data structures which offer significant advantage in terms of storage and execution. These data structures are compressed set enumeration trees and are constructed after one scan of the database and stores all the item-sets as distinct records in database.\\nDiscovery of Frequent Closed Item-Sets\\nLevel Wise Approach\\nPasquier et. al. (Pasquier N., Bastide Y ., Taouil R. \\n& Lakhal L. 1999) proposed Close method to find Frequent Closed Item-sets (FCI). This method finds closures based on Galois closure operators and com-putes the generators. Galois closure operator h(X) for some X ⊆ I is defined as the intersection of transactions \\nin D containing item-set X. An item-set X is a closed \\nitem-set if and only if  h(X) = X. One of the smallest \\narbitrarily chosen item-set p, such that h(p) = X is \\nknown as generator of X.\\nClose method is based on Apriori algorithm. It \\nstarts from 1- item-sets, finds the closure based on Galois closure operator, goes up level by level com-puting generators and their closures (i.e. FCI) at each level. At each level, candidate generator item-sets of size k are found by joining generator item-sets of size \\nk-1 using the combinatorial procedure used in Apriori algorithm. The candidate generators are pruned using two strategies i) remove candidate generators whose all subsets are not frequent ii) remove the candidate generators if closure of one of its subsets is superset of the generator. Subsequently algorithm finds the support of pruned candidate generator. Each iteration requires \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23fe2f29-8a97-4da1-a754-112ee4236b57', embedding=None, metadata={'page_label': '115', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  Algorithms for Association Rule Mining\\n Figure 4. Running example of apriori algorithm for finding frequent itemsets (ms = 40%)\\none pass over the database to construct the set of FCI \\nand count their support.\\nTree Based Approach\\nWang et. al. (Wang J., Han J. & Pei J. 2003) proposed Closet+ algorithm to compute FCI and their supports using FP-tree structure. The algorithm is based on divide and conquers strategy and computes the local frequent items of a certain prefix by building and scanning its projected database. Concept Lattice Based Approach\\nConcept lattice  is a core structure of Formal Concept \\nAnalysis (FCA). FCA is a branch of mathematics based \\non Concept and Concept hierarchies. Concept (A,B) is \\ndefined as a pair of set of objects A (known as extent ) \\nand set of attributes B (known as intent ) such that set \\nof all attributes belonging to extent  A is same as B \\nand set of all objects containing attributes of intent  \\nB is same as A. In other words, no object other than \\nobjects of set A contains all attributes of B and no at-\\ntribute other than attributes in set B is contained in all \\nobjects of set A. Concept lattice  is a complete lattice of \\nall Concepts. Stumme G., (1999) discovered that intent  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0b7bb0b-fd17-4e9c-9e12-d0f6e25203cf', embedding=None, metadata={'page_label': '116', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Algorithms for Association Rule Mining\\nA\\nB of the Concept (A,B) represents the closed item-set, \\nwhich implies that all algorithms for finding Concepts \\ncan be used to find closed item-sets. Kuznetsov S.O., & Obiedkov S.A. (2002) provides a comparison of performance of various algorithms for concepts. The naïve method to compute Concepts, proposed by Ganter is given in Exhibit A.\\nThis method generates all the Concepts  i.e. all closed \\nitem-sets. Closed item-sets generated using this method in example 1 are {A},{B} ,{C},{A,B},{A,C},{B,D},{B,\\nC,D}, {B,D,E}, {B,C,D,E}. Frequent Closed item-sets \\nare {A} ,{B},{C},{B,D},{B,C,D},{B,D,E}. \\nConcept lattice  for frequent closed item-sets is \\ngiven in Figure 5. \\nGenerating Association Rules \\nOnce all frequent item-sets are known, association rules can be generated in a straightforward manner by find -\\ning all subsets of an item-sets and testing the strength (Han J., & Kamber M., 2006). The pseudo code for this algorithm is given in Exhibit B. \\nBased on the above algorithm, strong  rules generated \\nfrom frequent item-set BCD in Example 1 are:\\nBC → D, conf=100%CD → B, conf=100%where mc = 70%\\nThere are two ways to find association rules from \\nfrequent closed item-sets:i) compute frequent item-sets from FCI and then \\nfind the association rules \\nii) generate rules directly using FCI. \\nClose method uses the first approach, which gen-\\nerates lot of redundant rules while method proposed \\nby Zaki (Zaki M.J., 2000), (Zaki, M.J., & Hsiao C., J., 2005) uses the second approach and derives rules directly from the Concept lattice . The association rules \\nthus derived are non-redundant rules. For example, set \\nof strong rules generated using Close method in Ex-\\nample 1 is {BC → D,CD →B,D →B,E → B,E →D,E → BD, BE →D,DE →B}. For the same example, set of non-redundant strong rules generated using Concept \\nLattice  approach is {D →B, E → BD, BC → D, CD →\\nB}. We can observe here that all rules can be derived from the reduced non-redundant set of rules.\\nScalability issues in Association Rule MiningScalability issues in ARM have motivated de-\\nvelopment of incremental and parallel algorithms. Incremental algorithms for ARM preserve the counts of selective item-sets and reuse this knowledge later to discover frequent item-sets from augmented database. Fast update algorithm (FUP) is the earliest algorithm based on this idea. Later different algorithms are presented based on sampling (Hipp J., Guntzer U., & Nakhaeizadeh G., 2000). \\nParallel algorithms partition either the dataset for \\ncounting or the set of counters, across different ma-add extent {all transactions} in the list of extents  \\nFor each item i ∈ I \\n  for each set  X in the list of extents  \\nfind X ∩ {set of transactions containing i} \\ninclude in the list of extents if not included earlier  \\n  EndFor  \\n EndFor \\n   Exhibit A.\\nFigure 5. Concept lattice\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76726b7e-60f7-4e52-9315-d2c054ef1daf', embedding=None, metadata={'page_label': '117', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Algorithms for Association Rule Mining\\nchines to achieve scalability (Hipp J., Guntzer U., & \\nNakhaeizadeh G., 2000). Algorithms, which partition the dataset exchange counters while the algorithms, which partition the counters, exchange datasets incur-ring high communication cost.\\nFUTURE TRENDS\\nDiscovery of Frequent Closed Item-sets (FCI) is a big lead in ARM algorithms. With the current growth rate of databases and increasing applications of ARM in various scientific and commercial applications we envisage tremendous scope for research in parallel, incremental and distributed algorithms for FCI. Use of lattice structure for FCI offers promise of scalability. On line mining on streaming datasets using FCI approach is an interesting direction to work on. \\nCONCLUSION\\nThe article presents the basic approach for Association Rule Mining, focusing on some common algorithms for finding frequent item-sets and frequent closed item-sets. Various approaches have been discussed to find such item-sets. Formal Concept Analysis approach for finding frequent closed item-sets is also discussed. Generation of rules from frequent items-sets and fre-quent closed item-sets is briefly discussed. The article addresses the scalability issues involved in various algorithms.\\nREFERENCES\\nAgarwal, R., Imielinski T., & Swami A., (1993), Min-ing Association Rules Between Sets of Items in Large Databases, Proceedings of the 1993 ACM International Conference on Management of Data, 207-216, Wash-ington, D.C.\\nAgrawal R., & Srikant R., (1994), Fast Algorithms \\nfor Mining Association Rules in Large Databases, Proceedings of the Twentieth International Conference on VLDB, pp. 487-499, Santiago, Chile\\nBayardo R.J. (1998), E fficiently Mining Long Patterns \\nFrom Databases, Proceedings of the ACM Interna-\\ntional Conference on Management of Data\\n.\\nBrin S., Motwani R., Ullman J. D., & Tsur S., (1997), \\nDynamic Item-set Counting and Implication Rules for Market Basket Data. ACM Special Interest Group on Management of Data, 26(2):255\\nCoenen F., Leng P., & Ahmed S., (2004) Data Structure \\nfor Association Rule Mining: T-Trees and P-Trees, IEEE TKDE, V ol. 16, No. 6\\nHan, J., Pei, J., & Yin, Y ., (2000), Mining Frequent \\nPatterns Without Candidate Generation, Proceedings of the ACM International Conference on Management of Data, ACM Press, 1-12.\\nHan, J., & Kamber, M., (2006), Data Mining: Con-\\ncepts and Techniques, 2nd ed. Morgan Kaufmann Publishers.\\nHipp, J., Güntzer, U., & Nakhaeizadeh, G., (2000), \\nAlgorithms for Association Rule Mining: A General Survey and Comparison, SIGKDD Explorations.\\nKuznetsov, S.O., & Obiedkov, S.A., (2002), Comparing \\nPerformance of Algorithms For Generating Concept Lattices, Journal of Experimentation and Theoretical Artificial Intelligence.\\nLin, D., & Kedem, Z. M., (1998), Pincer Search: A New \\nAlgorithm for Discovering the Maximum Frequent For each frequent item-set  I,  \\ngenerate all non-empty subsets of I \\nFor every non-empty subset  s of I,  \\nOutput the rule s → (I-s)  if support( I) / support ( s) >= mc \\n  EndFor \\n EndFor \\n Exhibit B.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7a6e2a9-0cd6-41ec-bcd1-117ac60747e6', embedding=None, metadata={'page_label': '118', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Algorithms for Association Rule Mining\\nASets. Proceedings of the 6th International Conference \\non Extending Database Technology (EDBT), Valencia, \\nSpain.\\nPasquier, N., Bastide, Y ., Taouil, R., & Lakhal, L., \\n(1999), Efficient Mining of Association Rules Us-ing Closed Item-set Lattices, Information Systems, 24(1):25-46\\nStumme, G., (1999), Conceptual Knowledge Discovery \\nwith Frequent Concept Lattices, FB4-Preprint 2043, TU Darmstadt\\nWang, J., Han, J., & Pei, J., (2003), Closet+: Searching \\nfor the Best Strategies for Mining Frequent Closed Item-sets, Proceedings of the 9\\nth ACM SIGKDD International \\nConference on Knowledge Discovery and Data Mining, Pages 236-245, New York, USA, ACM Press.\\nZaki, M. J., (2000), Generating Non-Redundant Asso-\\nciation Rules, Proceedings of the International Confer-ence on Knowledge Discovery and Data Mining.\\nZaki, M.J., & Hsiao C.,J.,(2005), Efficient algorithms \\nfor mining closed item-sets and their lattice structure. IEEE Transactions on Knowledge and Data Engineer-ing, 17(4): 462-478.\\nKEy TERMS\\nAssociation Rule: An Association rule is an impli-\\ncation of the form X→Y where X ⊂ I, Y⊂ I and X∩Y \\n=∅, I denotes the set of items.\\nData Mining: Extraction of interesting, non-trivial, \\nimplicit, previously unknown and potentially useful information or patterns from data in large databases. \\nFormal Concept: A formal context K = (G,M,I) \\nconsists of two sets G (objects) and M (attributes) \\nand a relation I between G and M. For a set A⊆G of \\nobjects \\nA’={meM | gIm for all geA} (the set of all attributes \\ncommon to the objects in A). Correspondingly, for a \\nset B of attributes we define B’ = {g eG | gIm for all meB} (the set of objects com-\\nmon to the attributes in B). \\nA formal concept of the context (G,M,I) is a pair (A,B) \\nwith A⊆G,B⊆M, \\nA’=B and B’=AA is called the extent and B is the intent of the concept \\n(A,B). \\nFrequent Closed Item-Set: An item-set X is a closed \\nitem-set if there exists no item-set X’ such that: \\ni. X’ is a proper superset of X,\\nii. Every transaction containing X also contains \\nX’. \\nA closed item-set X is frequent if its support exceeds \\nthe given support threshold. \\nGalois Connection: Let D = ( O,I,R) be a data \\nmining context where O and I are finite sets of objects \\n(transactions) and items respectively. R ⊆ O x I is a \\nbinary relation between objects and items. For O ⊆ O, \\nand I ⊆ I, we define as shown in Exhibit C.\\nf(O) associates with O the items common to all \\nobjects o ∈ O and g(I) associates with I the objects \\nrelated to all items i ∈ I. The couple of applications \\n(f,g) is a Galois connection between the power set of O (i.e. 2\\nO) and the power set of I (i.e. 2I).\\nThe operators h = f o g in 2I and h’ = g o f in 2o are \\nGalois closure operators. An item-set C ⊆ I  from D is \\na closed item-set iff h(C) = C.\\nGenerator Item-Set: A generator p of a closed \\nitem-set c is one of the smallest item-sets such that h(p) = c.\\nNon-Redundant  Association Rules: Let R\\ni denote \\nthe rule X1i→X2i, where X1,X2 ⊆ I. Rule R1 is more general \\nthan rule R2 provided R2 can be generated by adding \\nadditional items to either the antecedent or consequent of R\\n1. Rules having the same support and confidence as \\nf(O): 2O → 2I  \\nf(O) = (i∈ I  | ∀o ∈ O, (o,i) ∈ R} g(I): 2I  → 2O \\ng( I) = (o ∈ O | ∀i ∈ I, (o,i) ∈ R} \\n Exhibit C.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b118b9ca-acfc-4376-be17-e66ce362d17b', embedding=None, metadata={'page_label': '119', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Algorithms for Association Rule Mining\\nmore general rules are the redundant association rules. \\nRemaining rules are non-redundant rules.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='979cbad6-2c04-4b84-bce2-35371ecb835e', embedding=None, metadata={'page_label': '120', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAmbient Intelligence\\nFariba Sadri\\nImperial College London, UK \\nKostas Stathis\\nRoyal Holloway, University of London, UK\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nIn recent years much research and development effort has been directed towards the broad field of ambient \\nintelligence (AmI), and this trend is set to continue for the foreseeable future. AmI aims at seamlessly integrat-ing services within smart infrastructures to be used at home, at work, in the car, on the move, and generally in most environments inhabited by people. It is a relatively new paradigm rooted in ubiquitous computing, which calls for the integration and convergence of multiple disciplines, such as sensor networks, portable devices, intelligent systems, human-computer and social interac-tions, as well as many techniques within artificial intel-ligence, such as planning, contextual reasoning, speech recognition, language translation, learning, adaptability, and temporal and hypothetical reasoning.\\nThe term AmI was coined by the European Com-\\nmission, when in 2001 one of its Programme Advisory Groups launched the AmI challenge (Ducatel et al., 2001), later updated in 2003 (Ducatel et al., 2003). But although the term AmI originated from Europe, the goals of the work have been adopted worldwide, see for ex-ample (The Aware Home, 2007), (The Oxygen Project, 2007), and (The Sony Interaction Lab, 2007). \\nThe foundations of AmI infrastructures are based on \\nthe impressive progress we are witnessing in wireless technologies, sensor networks, display capabilities, processing speeds and mobile services.  These devel-opments help provide much useful (row) information for AmI applications. Further progress is needed in taking full advantage of such information in order to provide the degree of intelligence, flexibility and naturalness envisaged. This is where artificial intel-\\nligence  and multi-agent techniques have important \\nroles to play.\\nIn this paper we will review the progress that has \\nbeen made in intelligent systems, discuss the role of artificial intelligence and agent technologies  and focus \\non the application of AmI for independent living.\\nBACKGROUND\\nAmbient intelligence is a vision of the information society where normal working and living environments are surrounded by embedded intelligent devices that \\ncan merge unobtrusively into the background and work through intuitive interfaces. Such devices, each specialised in one or more capabilities, are intended to work together within an infrastructure of intelligent \\nsystems, to provide a multitude of services aimed at generally improving safety and security and improving quality of life in ordinary living, travelling and work-ing environments.\\nThe European Commission identified four AmI sce -\\nnarios (Ducatel et al. 2001, 2003) in order to stimulate imagination and initiate and structure research in this area. We summarise two of these to provide the flavour of AmI visions.\\nAmI Scenarios:\\nDimitrios is taking a coffee break and prefers not to be disturbed. He is wearing on his clothes or body a voice activated digital avatar of himself, known as Digital Me (D-Me). D-Me is both a learning device, learning about Dimitrios and his environment, and an acting device offering communication, process-ing and decision-making functionalities. During the coffee break D-Me answers the incoming calls and emails of Dimitrios. It does so smoothly in the necessary languages, with a re-production of Dimitrios’ voice and accent. Then D-Me receives a call from Dimitrios’ wife, recognises its urgency and passes it on to Demetrios. At the same time it catches a message from an older person’s D-Me, 1.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0b7f02a-1b20-4e9c-b0bb-e999be56b8d7', embedding=None, metadata={'page_label': '121', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Ambient Intelligence\\nlocated nearby. This person has left home without \\nhis medication and would like to find out where to access similar drugs. He has asked his D-Me, in natural language, to investigate this. Dimitrios happens to suffer from a similar health problem and uses the same drugs. His D-Me processes the incoming request for information, and decides neither to reveal Dimitrios’ identity nor offer direct help, but to provide the elderly person’s D-Me with a list of the closest medicine shops and potential contact with a self-help group.Carmen plans her journey to work. It asks AmI, by voice command, to find her someone with whom she can share a lift to work in half an hour. She then plans the dinner party she is to give that evening. She wishes to bake a cake, and her e-fridge flashes a recipe on the e-fridge screen and highlights the ingredients that are missing. Carmen completes her shopping list on the screen and asks for it to be delivered to the nearest distribution point in her neighbourhood. All goods are smart tagged, so she can check the progress of her virtual shopping from any enabled device anywhere, and make alterations. Carmen makes her journey to work, in a car with dynamic traffic guidance facilities and traffic sys-tems that dynamically adjust speed limits depend-ing on congestion and pollution levels. When she returns home the AmI welcomes her and suggests that on the next day she should telework, as a big demonstration is planned in downtown.2.The demands that drive AmI and provide opportuni-\\nties are for improvement of safety and quality of life, enhancements of productivity and quality of products and services, including public services such as hospitals, schools, military and police, and industrial innovation. AmI is intended to facilitate human contact and com-munity and cultural enhancement, and ultimately it should inspire trust and confidence. \\nSome of the technologies required for AmI are \\nsummarised in Figure 1.\\nAmI work builds on ubiquitous computing and sen-\\nsor network and mobile technologies. To provide the intelligence and naturalness required, it is our view that significant contributions can come from advances in artificial intelligence and agent technologies. Artificial \\nintelligence  has a long history of research on plan-\\nning, scheduling, temporal reasoning, fault diagnosis, hypothetical reasoning, and reasoning with incomplete and uncertain information. All of these are techniques that can contribute to AmI where actions and decisions have to be taken in real time, often with dynamic and uncertain knowledge about the environment and the user. Agent technology research has concentrated on agent architectures that combine several, often cogni-tive, capabilities, including reactivity and adaptability, as well as the formation of agent societies through communication, norms and protocols. \\nRecent work has attempted to exploit these tech-\\nniques for AmI. In (Augusto and Nugent 2004) the use of temporal reasoning combined with active data-\\nFigure 1. Components of Ambient Intelligence\\nCOMPONENTS\\nAMBIENT\\n\\uf0a7Very unobtrusive hardw are\\n\\uf0a7E m bedded system s  \\n\\uf0a7D ynam ic distributed netw orks  \\n\\uf0a7S eam less m obile/fixed ubiquitous  \\ncom m unication \\n\\uf0a7S ensor t echnology\\n\\uf0a7I/O devices\\n\\uf0a7A daptive s oftw areINTELLIGENCE\\uf0a7C om putational intelligence\\n\\uf0a7C ontextual a w areness\\n \\n\\uf0a7N atural interaction \\n\\uf0a7A daptability  \\n\\uf0a7R obustness  \\n\\uf0a7S ecurity  \\n\\uf0a7F ault tolerance\\nSOFTWARE PLATFORM', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='390cac17-7440-4dcf-b9ca-295178b82a98', embedding=None, metadata={'page_label': '122', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Ambient Intelligence\\nAbases are explored in the context of smart homes. In \\n(Sadri 2007) the use of temporal reasoning together with agents is explored to deal with similar scenarios, where information observed in a home environment is evaluated, deviations from normal behaviour and risky situations are recognised and compensating actions are recommended.\\nThe relationship of AmI to cognitive agents is \\nmotivated by (Stathis and Toni 2004) who argue that computational logic elevates the level of the system to that of a user. They advocate the KGP agent model  \\n(Kakas, et al 2004) to investigate how to assist a trav-eller to act independently and safely in an unknown environment using a personal communicator. (Augusto et al 2006) address the process of taking decisions in the presence of conflicting options. (Li and Ji 2005) offer a new probabilistic framework based on Bayesian Networks for dealing with ambiguous and uncertain sensory observations and users’ changing states, in order to provide correct assistance. \\n(Amigoni et al 2005) address the goal-oriented as-\\npect of AmI applications, and in particular the planning problem within AmI. They conclude that a combination of centralised and distributed planning capabilities are required, due to the distributed nature of AmI and the participation of heterogeneous agents, with different capabilities. They offer an approach based on the Hi-erarchical Task Networks taking the perspective of a multi-agent paradigm for AmI.\\nThe paradigm of embedded agents for AmI en-\\nvironments with a focus on developing learning and adaptation techniques for the agents is discussed in (Hagras et al 2004, and Hagras and Callaghan 2005). Each agent is equipped with sensors and effectors and uses a learning system based on fuzzy logic. A real AmI environment in the form of an “intelligent dormitory” is used for experimentation.\\nPrivacy and security in the context of AmI appli-\\ncations at home, at work, and in the health, shopping and mobility domains are discussed in (Friedewald et al 2007). For such applications they consider security threats such as surveillance of users, identity theft and malicious attacks, as well as the potential of the digital divide amongst communities and social pressures. AMBIENT INTELLIGENCE FOR  INDEPENDENT LIVING\\nOne major use of AmI is to support services for in-\\ndependent living, to prolong the time people can live decently in their own homes by increasing their autonomy and self-confidence. This may involve the elimination of monotonous everyday activities, moni-toring and caring for the elderly, provision of security, or saving resources. The aim of such AmI applications is to help:\\nmaintain safety of a person by monitoring his en-vironment and recognizing and anticipating risks, and taking appropriate actions,provide assistance in daily activities and require-ments, for example, by reminding and advising about medication and nutrition, andimprove quality of life, for example by providing personalized information about entertainment and social activities.\\nThis area has attracted a great deal of attention in \\nrecent years, because of increased longevity and the aging population in many parts of the world. For such an AmI system to be useful and accepted it needs to be versatile, adaptable, capable of dealing with changing environments and situations, transparent and easy, and even pleasant, to interact with.\\nWe believe that it would be promising to explore \\nan approach based on providing an agent architec-\\nture consisting of a society of heterogeneous, intel-ligent, embedded agents, each specialised in one or \\nmore functionalities. The agents should be capable of sharing information through communication, and their dialogues and behaviour should be governed by context-dependent and dynamic norms.\\nThe basic capabilities for intelligent agents in-\\nclude:\\nSensing: to allow the agent observe the environ-mentReactivity : to provide context-dependent dynamic \\nbehaviour and the ability to adapt to changes in the environmentPlanning: to provide goal-directed behaviour Goal Decision: to allow dynamic decisions about which goals have higher priorities•\\n••\\n•\\n•\\n•\\n•', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fc48c03-0486-4419-9655-e614ed516539', embedding=None, metadata={'page_label': '123', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Ambient Intelligence\\nAction execution: to allow the agent to affect the \\nenvironment.\\nAll of these functionalities also require reasoning \\nabout spatio-temporal constraints reflecting the envi-ronment in which an AmI system operates. \\nMost of these functionalities have been integrated \\nin the KGP model (Kakas et al, 2004), whose archi-\\ntecture is shown in Figure 2 and implemented in the PROSOCS system (Bracciali et al, 2006). The use of reactivity  for communication and dialogue policies \\nhas also been discussed in, for example, (Sadri et al, 2003). The inclusion of normative behaviour has been discussed in (Sadri et al, 2006) where we also consider how to choose amongst different types of goals, depend-ing on the governing norms. For a general discussion on the importance of norms in artificial societies see (Pitt, 2005). \\nKGP agents are situated in the environment via \\ntheir physical capabilities . Information received from \\nthe environment (including other agents) updates the agents state and provides input to its dynamic cycle \\ntheory,  which, in turn, determines the next steps in terms \\nof its transitions, using its reasoning capabilities .\\nFUTURE TRENDS\\nAs most other information and communication tech-nologies, AmI is not likely to be good or bad on its own, but its value will be judged from the different • ways the technology will be used to improve people’s \\nlives. In this section we discuss new opportunities and challenges for the integration of AmI with what people do in ordinary settings. We abstract away from hardware trends and we focus on areas that are software related and are likely to play an important role in the adoption of AmI technologies.\\nA focal point is the observation that people discover \\nand understand the world through visual and conver-sational interactions. As a result, in the coming years we expect to see the design of AmI systems to focus in ways that will allow humans to interact in natural ways, using their common skills such as speaking, gesturing, glancing. This kind of natural interaction  (Leibe et al \\n2000) will complement existing interfaces and will require that AmI systems be capable of representing virtual objects, possibly in 3D, as well as capture people’s moves in the environment and identify which of these moves are directed to virtual objects.\\nWe also expect to see new research directed towards \\nprocessing of sensor data with different information (Massaro and Friedman 1990) and different kind of formats such as audio, video, and RFID. Efficient techniques to index, search, and structure these data and ways to transform them to the higher-level semantic information required by cognitive agents will be an important area for future work. Similarly, the reverse of this process is likely to be of equal importance, namely, how to translate high-level information to the lower-level signals required by actuators that are situated in the environment.\\nGiven that sensors and actuators will provide the \\nlink with the physical environment, we also anticipate further research to address the general linking of AmI systems to already existing computing infrastructures such as the semantic web. This work will create hybrid \\nenvironments that will need to combine useful informa-tion from existing wired technologies with information from wireless ones (Stathis et al 2007). To enable the creation of such environments we imagine the need to build new frameworks and middleware to facilitate integration of heterogeneous AmI systems and make the interoperation more flexible.\\nAnother important issue is how the human experi-\\nence in AmI will be managed in a way that will be as unobtrusive as possible. In this we foresee that develop-ments in cognitive systems will play a very important role. Although there will be many areas of cognitive system behaviour that will need to be addressed, we \\nFigure 2. The architecture of a KGP agent', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d36c31a-b1ca-4b48-a43a-ab912b5f47d8', embedding=None, metadata={'page_label': '124', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Ambient Intelligence\\nAanticipate that development of agent models that adapt \\nand learn (Sutton and Barto 1998), to be of great im-\\nportance. The challenge here will be how to integrate the output of these adaptive and learning capabilities to the reasoning and decision processes of the agent. The resulting cognitive behaviour must differentiate between newly learned concepts and existing ones, as well as discriminate between normal behaviour and exceptions.\\nWe expect that AmI will emerge with the formation \\nof user communities who live and work in a particular locality (Stathis et al 2006). The issue then becomes how to manage all the information that is provided and captured as the system evolves. We foresee research to address issues such as semantic annotations of content, and partitioning and ownership of information.\\nLinking in local communities with smart homes, \\ne-healthcare, mobile commerce, and transportation systems will eventually give rise to a global AmI sys-tem. For applications in such a system to be embraced by people we will need to see specific human factors studies to decide how unobtrusive, acceptable and desirable the actions of the AmI environment seem to people who use them. Some human factors studies should focus on issues of presentation of objects and agents in a 3D setting, as well as on the important is-sues of privacy, trust and security. \\nTo make possible the customization of system in-\\nteractions to different classes of users, it is required to acquire and store information about these users. Thus for people to trust AmI interactions in the future we must ensure that the omnipresent intelligent environ-ment maintains privacy in an ethical manner. Ethical or, better, normative behaviour cannot only be ensured at the cognitive level (Sadri et al 2006), but also at the lower, implementation level of the AmI platform. In this context, ensuring that communicated information is encrypted, certified, and follows transparent security policies will be required to build systems less vulner-able to malicious attacks. Finally, we also envisage changes to business models that would characterise AmI interactions (Hax and Wielde 2001). \\nCONCLUSION \\nThe successful adoption of AmI is predicated on the suitable combination of ubiquitous computing, artificial intelligence and agent technologies. A useful class of applications that can test such a combination is AmI supporting independent living. For such applications we have identified the trends that are likely to play an important role in the future.\\nREFERENCES\\nAugusto, J.C., Liu, J., Chen L. (2006). Using ambient intelligence for disaster management. In the Proceedings of the 10th International Conference on Knowledge-based Intelligent Information and Engineering Systems (KES 2006), Springer Verlag.\\nAugusto, J.C., Nugent, C. D. (2004). The use of temporal \\nreasoning and management of complex events in smart homes. In Proceedings of the European Conference on Artificial Systems (ECAI), 778-782.\\nBracciali, A., Endriss, U., Demetriou, N., Kakas, \\nA.C., Lu, L., Stathis, K. (2006). Crafting the mind of PROSOCS agents. Applied Artificial Intelligence \\n20(2-4), 105-131. \\nDucatel, K., Bogdanowicz, M., Scapolo, F., Leijten, \\nJ., Burgelman J.-C. (2001). Scenarios for ambient in-telligence in 2010. IST Advisory Group Final Report, European Commission.\\nDucatel, K., Bogdanowicz, M., Scapolo, F., Leijten, J., \\nBurgelman J.-C. (2003). Ambient intelligence : from vision to reality. IST Advisory Group Draft Report, European Commission.\\nDutton, W. H. (1999). Society on the line: information \\npolitics in the digital age, Oxford, Oxford University Press.\\nFriedewald M., Vildijiounaite, E., Punie, Y . Wright, \\nD. (2007). Privacy, identity and security in ambient intelligence: a scenario analysis. Telematics and In-formatics, 24, 15-29.\\nHagras, H., Callaghan, V ., Colley, M., Clarke, G., \\nPounds-Cornish, A., Duman, H. (2004). Creating an ambient intelligence environment using embedded agents. IEEE Intelligent Systems, 19(6), 12-20. \\nHagras, H. and Callaghan, V . (2005). An intelligent \\nfuzzy agent approach for realizing ambient intelligence in intelligent inhabited environments. IEEE Transac-tions on Systems, Man, and Cybernetics - Part A: Systems and Humans, 35(1), 55-65. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80bf740b-89ad-4f42-8411-3585b4841401', embedding=None, metadata={'page_label': '125', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180  Ambient Intelligence\\nHax, A., and Wilde, D, II. (2001). The Delta Model \\n– discovering new sources of profitability in a ne -\\ntworked economy. European Management Journal. 9, 379-391.\\nKakas, A., Mancarella, P., Sadri, F. Stathis, K. Toni, \\nF. (2004). The KGP model of agency. In Proceedings of European Conference on Artificial Intelligence, 33-37. \\nLi, X. and Ji, Q. (2005). Active affective state de-\\ntection and user assistance with dynamic bayesian networks. IEEE Transactions on Systems, Man, and Cybernetics: Special Issue on Ambient Intelligence, 35(1), 93-105.\\nLeibe, B., Starner, T., Ribarsky, W., Wartell, Z., Krum, \\nD., Singletary, B., and Hodges, L. (2000).  The Per-ceptive Workbench: towards spontaneous and natural interaction in semi-immersive virtual environments. IEEE Virtual Reality Conference (VR’2000). 13-20.\\nMassaro, D. W., and D. Friedman. (1990). Models \\nof integration given multiple sources of information. Psychological Review. 97, 225-252.\\nPitt, J. (2005) The open agent society as a platform for \\nthe user-friendly information society. AI Soc. 19(2), 123-158.\\nSadri, F., Stathis, K., and Toni, F. (2006). Normative \\nKGP agents. Journal of Computational and Mathema-tical Organizational Theory. 12(2-3), 101-126.\\nSadri, F. (2007). Ambient intelligence for care of the \\nelderly in their homes. In Proceedings of the 2nd Work-shop on Artificial Intelligent Techniques for Ambient Intelligence (AITAmI ‘07), 62-67.\\nSadri, F., Toni, F., Torroni, P. (2003).  Minimally intrusive \\nnegotiating agents for resource sharing. In Proceedings \\nof the 8\\nth International Joint Conference on Artificial \\nIntelligence (IJCAI 03),  796-801.\\nStathis, K., de Bruijn, O., Spence, R. and Purcell, P. \\n(2006) Ambient intelligence: human-agent interactions in a networked community. In Purcell, P. (ed) Networked Neighbourhoods: The Connected Community in Con-text (Springer), 279-304. Stathis, K., Kafetzoglou, S., Papavasiliou, S., and Bro-muri, S. (2007). Sensor network grids: agent environ-ments combined with QoS in wireless sensor networks. In Proceedings of the 3rd International Conference on Autonomic and Autonomous Systems, IEEE. 47-52.\\nStathis, K. And Toni, F. (2004). Ambient intelligence \\nusing KGP agents. Workshop at the Second European Symposium on Ambient Intelligence, Lecture Notes in Compuer Science 3295, 351-362.\\nSutton, R. S. and Barto, G. A. (1998). Reinforcement \\nlearning: an introduction. MIT Press.\\nThe Aware Home Initiative (2007), http://www.\\ncc.gatech.edu/fce/house/house.html.\\nThe Oxygen Project (2007), http://www.oxygen.lcs.\\nmit.edu.\\nThe Sony Interaction Lab (2007), http://www.sonycsl.\\nco.jp/IL/index.html.\\nTERMS AND DEFINITIONS\\nArtificial Societies: Complex systems consisting \\nof a, possibly large,  set of agents whose interaction are constrained by norms and the roles the agents are responsible to play. \\nCognitive Agents: Software agents endowed with \\nhigh-level mental attitudes, such as beliefs, goals and plans.\\nContext Awareness: Refers to the idea that comput-\\ners can both sense and react according to the state of the environment they are situated. Devices may have information about the circumstances under which they are able to operate and react accordingly.\\nNatural Interaction: The investigation of the re-\\nlationships between humans and machines aiming to create interactive artifacts that respect and exploit the \\nnatural dynamics through which people\\n communicate \\nand discover the real world.\\nSmart Homes: Homes equipped with intelligent \\nsensors and devices within a communications infra-\\nstructure that allows the various systems and devices to communicate with each other for monitoring and maintenance purposes. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f26fb59-8d9f-4a47-bd59-a2c451f3022a', embedding=None, metadata={'page_label': '126', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Ambient Intelligence\\nAUbiquitous Computing:  A model of human-com-\\nputer interaction in which information processing is \\nintegrated into everyday objects and activities. Unlike the desktop paradigm, in which a single user chooses to interact with a single device for a specialized purpose, with ubiquitous computing a user interacts with many computational devices and systems simultaneously, in the course of ordinary activities, and may not neces-sarily even be aware that is doing so.\\nWireless Sensor Networks: Wireless networks \\nconsisting of spatially distributed autonomous devices using sensors to cooperatively monitor physical or environmental conditions, such as temperature, sound, vibration, pressure, motion or pollutants, at different locations.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd249d4d-d966-4bbc-ad13-b332c941c763', embedding=None, metadata={'page_label': '127', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAmbient Intelligence Environments\\nCarlos Ramos\\nPolytechnic of Porto, Portugal\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nThe trend in the direction of hardware cost reduction and miniaturization allows including computing de-vices in several objects and environments (embedded systems). Ambient Intelligence (AmI) deals with a new world where computing devices are spread everywhere (ubiquity), allowing the human being to interact in physical world environments in an intelligent and un-obtrusive way. These environments should be aware of the needs of people, customizing requirements and forecasting behaviours.\\nAmI environments may be so diverse, such as \\nhomes, offices, meeting rooms, schools, hospitals, control centers, transports, touristic attractions, stores, sport installations, and music devices.\\nAmbient Intelligence involves many different disci-\\nplines, like automation (sensors, control, and actuators), human-machine interaction and computer graphics, communication, ubiquitous computing, embedded systems, and, obviously, Artificial Intelligence. In the aims of Artificial Intelligence, research envisages to include more intelligence in the AmI environments, allowing a better support to the human being and the access to the essential knowledge to make better deci-sions when interacting with these environments.\\nBACKGROUND\\nAmbient Intelligence (AmI) is a concept developed by the European Commission’s IST Advisory Group ISTAG (ISTAG, 2001)(ISTAG, 2002). ISTAG believes that it is necessary to take a holistic view of Ambient Intelligence, considering not just the technology, but the whole of the innovation supply-chain from sci-ence to end-user, and also the various features of the academic, industrial and administrative environment that facilitate or hinder realisation of the AmI vision (ISTAG, 2003). Due to the great amount of technolo-gies involved in the Ambient Intelligence concept we may find several works that appeared even before the ISTAG vision pointing in the direction of Ambient Intelligence trends.\\nIn what concerns Artificial Intelligence (AI), Ambi -\\nent Intelligence is a new meaningful step in the evolution of AI (Ramos, 2007). AI has closely walked side-by-side with the evolution of Computer Science and Engineer-ing. The building of the first artificial neural models and hardware, with the Walter Pitts and Warren McCullock work (Pitts & McCullock, 1943) and Marvin Minsky and Dean Edmonds SNARC system correspond to the first step. Computer-based Intelligent Systems, like the MYCIN Expert System (Shortliffe, 1976) or network-based Intelligent Systems, like AUTHORIZER’s AS-SISTANT (Rothi, 1990) used by American Express for authorizing transactions consulting several Data Bases are the kind of systems of the second step of AI. From the 80’s Intelligent Agents and Multi-Agent Systems have established the third step, leading more recently to Ontologies and Semantic Web. From hardware to the computer, from the computer to the local network, from the local network to the Internet, and from the Internet to the Web, Artificial Intelligence was on the state of the art of computing, most of times a little bit ahead of the technology limits.\\nNow the centre is no more in the hardware, or in \\nthe computer, or even in the network. Intelligence must be provided to our daily-used environments. We are aware of the push in the direction of Intelligent Homes, Intelligent Vehicles, Intelligent Transportation Systems, Intelligent Manufacturing Systems, even Intelligent Cities. This is the reason why Ambient Intelligence concept is so important nowadays (Ramos, 2007).\\nAmbient Intelligence is not possible without Artifi-\\ncial Intelligence. On the other hand, AI researchers must be aware of the need to integrate their techniques with other scientific communities’ techniques (e.g. Automa-tion, Computer Graphics, Communications).  Ambient Intelligence is a tremendous challenge, needing the better effort of different scientific communities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='efbfc607-8305-4971-98e5-64ce764e9769', embedding=None, metadata={'page_label': '128', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Ambient Intelligence Environments\\nAThere is a miscellaneous of concepts and tech-\\nnologies related with Ambient Intelligence. Ubiquitous \\nComputing, Pervasive Computing, Embedded Sys-tems, and Context Awareness are the most common. However these concepts are different from Ambient Intelligence. \\nThe concept of Ubiquitous Computing (UbiComp) \\nwas introduced by Mark Weiser during his tenure as Chief Technologist of the Palo Alto Research Center (PARC) (Weiser, 1991). Ubiquitous Computing means that we have access to computing devices anywhere in an integrated and coherent way. Ubiquitous Computing was mainly driven by Communications and Comput-ing devices scientific communities but now is involv -\\ning other research areas. Ambient Intelligence differs from Ubiquitous Computing because sometimes the environment where Ambient Intelligence is considered is simply local. Another difference is that Ambient Intelligence makes more emphasis on intelligence than Ubiquitous Computing. However, ubiquity is a real need today and Ambient Intelligence systems are considering this feature.\\nA concept that sometimes is seen as a synonymous \\nof Ubiquitous Computing is Pervasive Computing. According to Teresa Dillon, Ubiquitous Computing is best considered as the underlying framework, the embedded systems, networks and displays which are invisible and everywhere, allowing us to ‘plug-and-play’ devices and tools,  On the other hand, Pervasive Computing, is related with all the physical parts of our lives; mobile phone, hand-held computer or smart jacket (Dillon, 2006).\\nEmbedded Systems mean that electronic and \\ncomputing devices are embedded in current objects or goods. Today goods like cars are equipped with mi-croprocessors; the same is true for washing machines, refrigerators, and toys. Embedded Systems community is more driven by electronics and automation scientific communities. Current efforts go in the direction to in-clude electronic and computing devices in the most usual and simple objects we use, like furniture or mirrors. Ambient Intelligence differs from Embedded Systems since computing devices may be clearly visible in AmI scenarios. However, there is a clear trend to involve more embedded systems in Ambient Intelligence.\\nContext Awareness means that the system is aware \\nof the current situation we are dealing with. An example is the automatic detection of the current situation in a Control Centre. Are we in presence of a normal situation or are we dealing with a critical situation, or even an emergency? In this Control Centre the intelligent alarm processor will exhibit different outputs according to the identified situation (Vale, Moura, Fernandes, Marques, Rosado, Ramos, 1997). Automobile Industry is also investing in Context Aware systems, like near-accident detection. Human-Computer Interaction scientific com -\\nmunity is paying lots of attention to Context Awareness. Context Awareness is one of the most desired concepts to include in Ambient Intelligence, the identification of the context is important for deciding to act in an intelligent way. \\nThere are different views of the importance of other \\nconcepts and technologies in the Ambient Intelligence field. Usually these differences are derived from the basic scientific community of the authors. ISTAG see the technology research requirements from different points of view (Components, Integration, System, and User/Person). In (ISTAG, 2003) the following ambient components are mentioned: smart materials; MEMS and sensor technologies; embedded systems; ubiquitous communications; I/O device technology; adaptive soft-ware. In the same document ISTAG refers the following intelligence components: media management and han-dling; natural interaction; computational intelligence; context awareness; and emotional computing.\\nRecently Ambient Intelligence is receiving a \\nsignificant attention from Artificial Intelligence Com-munity. We may refer the Ambient Intelligence Work-shops organized by Juan Augusto and Daniel Shapiro at ECAI’2006 (European Conference on Artificial Intelligence) and IJCAI’2007 (International Joint Conference on Artificial Intelligence) and the Special Issue on Ambient Intelligence, coordinated by Carlos Ramos, Juan Augusto and Daniel Shapiro to appear in the March/April’2008 issue of the IEEE Intelligent Systems magazine.\\nAMBIENT INTELLIGENT PROTOTyPES AND SySTEMS\\nHere we will analyse some examples of Ambient Intel-ligence prototypes and systems, divided by the area of application. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='be55a06f-d002-4fc6-9d5d-309f9b28611c', embedding=None, metadata={'page_label': '129', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Ambient Intelligence Environments\\nAmI at Home\\nDomotics is a consolidated area of activity. After the \\nfirst experiences using Domotics at homes there was a trend to refer the Intelligent Home concept. However, Domotics is too centred in the automation, giving to the user the capability to control the house devices from everywhere. We are still far from the real Ambient Intelligence in homes, at least at the commercial level. In (Wichert, Hellschimidt, 2006) there is an interesting example in the aims of EMBASSI project, by gesture a woman is commanding the TV to be brighter, however the TV is already at the brightest level, so the lights reduce the level and the windows close, showing an example of context awareness in the environment.\\nSeveral organizations are doing experiments to \\nachieve the Intelligent Home concept. Some examples are HomeLab from Philips, MIT House_ n, Georgia \\nTech Aware Home, Microsoft Concept Home, and e2 Home from Electrolux and Ericsson. \\nAmI in Vehicles and Transports\\nSince the first experiences with NA VLAB 1 (Thorpe, Herbert, Kanade, Shafer, 1988) Carnegie Mellon Uni-versity has developed several prototypes for Autono-mous Vehicle Driving and Assistance. The last one, NA VLAB 11, is an autonomous Jeep. Most of the car industry companies are doing research in the area of Intelligent Vehicles for several tasks like car parking assistance or pre-collision detection.\\nAnother example of AmI application is related \\nwith Transports, namely in connection with Intelligent Transportation Systems (ITS). The ITS Joint Program of the US Department of Transportation identified several areas of applications, namely: arterial management; freeway management; transit management; incident management; emergence management; electronic pay-ment; traveller information; information management; crash prevention and safety; roadway operations and management; road weather management; commercial vehicle operations; and intermodal freight. In all these application areas Ambient Intelligence can be used.\\nAmI in Elderly and Health Care\\nSeveral studies point to the aging of population dur-ing the next decades. While being a good result of increasing of life expectation, this also implies some problems. The percentage of population with health problems will increase and it will be very difficult to Hospitals to maintain all patients. Our society is faced with the responsibility to care for these people in the best possible social and economical ways. So, there is a clear interest to create Ambient Intelligence devices and environments allowing the patients to be followed in their own homes or during their day-by-day life. \\nThe medical control support devices may be em-\\nbedded in clothes, like T-shirts, collecting vital-sign information from sensors (e. g. blood pressure, tem-perature). Patients will be monitored at long distance. The surrounding environment, for example the patient home, may be aware of the results from the clinical data and even perform emergency calls to order an ambulance service. \\nFor instance, we may refer the IST Vivago® system \\n(IST International Security Technology Oy, Helsinki, Finland), an active social alarm system, which combines intelligent social alarms with continuous remote moni-toring of the user’s activity profile (Särelä, Korhonen, Lötjönen, Sola, Myllymäki, 2003).\\nAmI in Tourism and Cultural Heritage\\nTourism and Cultural Heritage are good application areas for Ambient Intelligence. Tourism is a grow-ing industry. In the past tourists were satisfied with pre-defined tours, equal for all the people. However there is a trend in the customization and the same tour can be conceived to adapt to tourists according their preferences. \\nImmersive tour post is an example of such experi-\\nence (Park, Nam, Shi, Golub, Van Loan, 2006). MEGA is an user-friend virtual-guide to assist visitors in the Parco Archeologico della Valle del Temple in Agrigento, an archaeological area with ancient Greek temples in Agrigento, located in Sicily, Italy (Pilato, Augello, Santangelo, Gentile, Gaglio, 2006). DALICA has been used for constructing and updating the user profile of visitors of Villa Adriana in Tivoli, near Rome, Italy (Constantini, Inverardi, Mostarda, Tocchio, Tsintza, 2007).\\nAmI at Work\\nThe human being spends considerable time in work-ing places like offices, meeting rooms, manufacturing plants, control centres. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='af7edacf-8f36-4c45-8c37-b0f6b4081852', embedding=None, metadata={'page_label': '130', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Ambient Intelligence Environments\\nASPARSE is a project initially created for helping \\nPower Systems Control Centre Operators in the diagno-\\nsis and restoration of incidents (Vale, Moura, Fernandes, Marques, Rosado, Ramos, 1997). It is a good example of context awareness since the developed system is aware of the on-going situation, acting in different ways according the normal or critical situation of the power system. This system is evolving for an Ambient Intelligence framework applied to Control Centres. \\nDecision Making is one of the most important \\nactivities of the human being. Nowadays decisions imply to consider many different points of view, so decisions are commonly taken by formal or informal groups of persons. Groups exchange ideas or engage in a process of argumentation and counter-argumenta-tion, negotiate, cooperate, collaborate or even discuss techniques and/or methodologies for problem solving. Group Decision Making is a social activity in which the discussion and results consider a combination of rational and emotional aspects. ArgEmotionAgents is a project in the area of the application of Ambient Intelligence in the group argumentation and decision support considering emotional aspects and running in the Laboratory of Ambient Intelligence for Decision Support (LAID), seen in Figure 1 (Marreiros, Santos, Ramos, Neves, Novais, Machado, Bulas-Cruz, 2007), a kind of an Intelligent Decision Room. This work has also a part involving ubiquity support.\\nAmI in Sports\\nSports involve high-level athletes and many more prac-titioners. Many sports are done without any help of the associated devices, opening here a clear opportunity for Ambient Intelligence to create sports assistance devices and environments.FlyMaster NA V+ is a free-flight on-board pilot As -\\nsistant (e.g. gliding, paragliding), using the FlyMaster F1 module with access to GPS and sensorial informa-tion. FlyMaster Avionics S.A., a spin-off, was created to commercialize these products (see figure 2).\\nAMBIENT INTELLIGENCE PLATFORMS\\nSome companies and academic institutions are invest-ing in the creation of Ambient Intelligence generation platforms.\\nThe Endeavour project is developed by the California \\nUniversity in Berkeley (http://endeavour.cs.berkeley.edu/). The project aims to specify, design, and imple-ment prototypes at a planet scale, self organized and involving an adaptive “Information Utility”. \\nOxygen enables pervasive human centred comput-\\ning through a combination of specific user and system technologies (http://www.oxygen.lcs.mit.edu/). This project provides speech and vision technologies en-abling us to communicate with Oxygen as if we were interacting with another person, saving much time and effort (Rudolph, 2001). \\nThe Portolano project was developed in the Uni-\\nversity of Washington and seeks to create a testbed for research into the emerging field of invisible computing (http://portolano.cs.washington.edu/). The invisible computing is possible with devices so highly optimized to particular tasks that they bend into the world and require little technical knowledge from the users (Esler, Hightower, Anderson, Borrielo, 1999). \\nThe EasyLiving project of Microsoft Research \\nVision Group corresponds to a prototype architecture and associated technologies for building intelligent environments (Brumitt, Meyers, Krumm, Kern, Shafer, \\nFigure 1. Ambient Intelligence for decision support, LAID Laboratory', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6cab2c82-273e-46ff-a541-906ce43ccb43', embedding=None, metadata={'page_label': '131', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Ambient Intelligence Environments\\n2000). EasyLiving goal is to facilitate the interaction \\nof people with other people, with computer, and with devices (http://research.microsoft.com/easyliving/). \\nFUTURE TRENDS\\nAmbient Intelligence deals with a futuristic notion for our lives. Most of the practical experiences concerning Ambient Intelligence are still in a very incipient phase, due to the recent existence of this concept. Today, it is not clear the separation between the computer and the environments. However, for new generations things will be more transparent, and environments with Ambient Intelligence will be more widely accepted. \\nIn the area of transport, AmI will cover several \\naspects. The first will be related with the vehicle itself. Several performances start to be available, like the au-tomatic identification of the situation (e.g. pre-collision identification, identification of the driver conditions). Other aspects will be related with the traffic information. Today, GPS devices are generalized, but they deal with static information. Joining on-line traffic conditions will enable the driver to avoid roads with accidents. Technology is giving good steps in the direction of automatic vehicle driving. But in the near future the developed systems will be seen more like driver as-sistants in spite of autonomous driving systems.\\nAnother area where AmI will experience a strong \\ndevelopment will be the area of Health Care, especially in the Elderly Care. Patients will receive this support to allow a more autonomous life in their homes. However automatic acquisition of vital signals (e.g. blood pres-sure, temperature) will allow to do automatic emergency calls when the patient health is in significant trouble. The person monitoring will also be done in his/her home, trying to detect differences in expected situa-tions and habits.\\nThe home support will achieve the normal personal \\nand family life. Intelligent Homes will be a reality. The home residents will pay less attention to normal home management aspects, for example, how many bottles of red wine are available for the week meals or if the specific ingredients for a cake are all available.\\nAmI for job support are also expected. Decision \\nSupport Systems will be oriented to on-the-job envi-ronments. This will be clear in offices, meeting rooms, call centres, control centres, and plants.  \\nCONCLUSION \\nThis article presents the state of the art in which con-cerns Ambient Intelligence field. After the history of the concept, we established some related concepts definitions and illustrated with some examples. There is a long way to follow in order to achieve the Ambi-ent Intelligence concept, however in the future, this concept will be referred as one of the landmarks in the Artificial Intelligence development.\\nFigure 2. FlyMaster Pilot Assistant device, from FlyMaster Avionics S.A. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6767411-71f3-4371-a094-b1068b5f76cf', embedding=None, metadata={'page_label': '132', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18Ambient Intelligence Environments\\nAREFERENCES\\nBrumitt, B., Meyers, B., Krumm, J., Kern, A., Shafer, \\nS. (2000). EasyLiving: Technologies for Intelligent Environments. Lecture Notes in Computer Science, vol. 1927, pp. 97-119.\\nConstantini, S., Inverardi, P., Mostarda, L., Tocchio, \\nA., Tsintza, P. (2007). User Profile Agents for Cultural Heritage fruition. Artificial and Ambient Intelligence. Proc. of the Artificial Intelligence and Simulation of Behaviour Annual Convention, pp. 30-33.\\nDillon, T. (2006). Pervasive and Ubiquitous Comput-\\ning. Futurelab. Available at http://www.futurelab.org.uk/viewpoint/art71.htm.\\nISTAG (2001), Scenarios for Ambient Intelligence in \\n2010, European Commission Report.ISTAG (2002). Strategic Orientations & Priorities for \\nIST in FP6, European Commission Report.ISTAG (2003). Ambient Intelligence: from vision to \\nreality , European Commission Report.\\nMarreiros, G., Santos, R., Ramos, C., Neves, J., No-\\nvais, P., Machado, J., Bulas-Cruz, J. (2007). Ambient Intelligence in Emotion Based Ubiquitous Decision Making. Proc. Artificial Intelligence Techniques for Ambient Intelligence, IJCAI’07 – Twentieth Inter-national Joint Conference on Artificial Intelligence. Hyderabad, India.\\nMcCulloch, W.S., & Pitts, W. (1943). A Logical Cal-\\nculus of Ideas Immanent in Nervous Activity. Bulletin \\nof Mathematical Biophysics . (5) 115-133.\\nPark, D., Nam, T., Shi, C., Golub, G., Van Loan, C. (2006). Designing an immersive tour experience system for cultural tour sites. ACM Press. New York, NY . pp. 1193-1198. \\nPilato, G., Augello, A., Santangelo, A., Gentile, A., \\nGaglio S. (2006). An intelligent multimodal site-guide for the Parco Archeologico della Valle del Temple in Agrigento. Proc. of the First Workshop in Intelligent Technologies for Cultural HeritageExploitation. Euro-pean Conference on Artificial Intelligence.Esler, M., Hightower, J., Anderson, T., Borrielo, J. (1999). Next century challenges: data-centric network-ing for invisible computing: the Portolano project at the University of Washington. Proceedings of the 5th annual ACM/IEEE international conference on Mobile computing and networking, pp. 256-262.\\nRamos, C. (2007). Ambient Intelligence – a State of \\nthe Art from Artificial Intelligence perspective. Pro-ceedings of EPIA’2007 – the Portuguese Conference on Artificial Intelligence.\\nRothi J., Yen D.(1990). Why American Express Gam-\\nbled on an Expert Data Base. Information Strategy: The Executive´s Journal, 6(3), pp. 16-22.\\nRudolph, L. (2001). Project Oxygen: Pervasive, Human-\\nCentric Computing - An Initial Experience. Lecture Notes in Computer Science, vol. 2068.\\nSärelä A., Korhonen I., Lötjönen L., Sola M., Myl-\\nlymäki M. (2003), IST Vivago® - an intelligent social and remote wellness monitoring system for the elderly. In: Proceedings of the 4\\nth Annual IEEE EMBS Special \\nTopic Conference on Information Technology Applica-tions in Biomedicine, pp. 362-365. \\nShortliffe, E. (1976). Computer-Based Medical Con-\\nsultations: MYCIN; Elsevier - North Holland.Thorpe, C.,  Hebert, M.H.,  Kanade, T., Shafer, S.A. \\n(1988), Vision and navigation for the Carnegie-Mellon Navlab, IEEE Transactions on Pattern Analysis and Machine Intelligence,  10(3), 362-373.\\nVale, Z., Moura, A., Fernandes, M., Marques, A., Ro-\\nsado, A., Ramos, C. (1997). SPARSE: An Intelligent Alarm Processor and Operator Assistant, IEEE Expert- Special Track on AI Applications in the Electric Power Industry, 12(3), pp. 86- 93, 1997.\\nWeiser, M. (1991), The Computer for the Twenty-\\nFirst Century. Scientific American. September 1991. \\npp. 94-104.\\nWichert R., Hellenschmidt M. (2006). Intelligent \\nSystems. Ambient Intelligence solutions for Intelligent \\nEnvioronments. Thematic Brochure of INI-Graphics-Net, pp. 12-13, n.1, 2006.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a1ceca1c-6e1d-4a99-b73c-424b3ab00d88', embedding=None, metadata={'page_label': '133', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18  Ambient Intelligence Environments\\nTERMS AND DEFINITIONS\\nAmbient Intelligence:  Ambient Intelligence (AmI) \\ndeals with a new world where computing devices are \\nspread everywhere, allowing the human being to interact in physical world environments in an intelligent and unobtrusive way. These environments should be aware of the needs of people, customizing requirements and forecasting behaviours.\\nContext Awareness:  Context Awareness means \\nthat the system is aware of the current situation we are dealing with.\\nEmbedded Systems:  Embedded Systems means \\nthat electronic and computing devices are embedded in current objects or goods.\\nIntelligent Decision Room: A decision-making \\nspace, eg a meeting room or a control center, equipped with intelligent devices and/or systems to support deci-sion-making processes.\\nIntelligent Home: A home equipped with several \\nelectronic and interactive devices to help residents to manage conventional home decisions.\\nIntelligent Transportation Systems: Intelligent \\nSystems applied to the area of Transports, namely to traffic and travelling issues.\\nIntelligent Vehicles: A vehicle equipped with sen-\\nsors and decision support components.\\nPervasive Computing:  Pervasive Computing is \\nrelated with all the physical parts of our lives, the user may have not notion of the computing devices and details related with these physical parts.\\nUbiquitous Computing: Ubiquitous Computing \\nmeans that we have access to computing devices any-where in an integrated and coherent way.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3d9daad-ea57-430a-a045-e2a78e2083fe', embedding=None, metadata={'page_label': '134', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\nAAnalytics for Noisy Unstructured Text Data I\\nShourya Roy\\nIBM Research, India Research Lab, India\\nL. Venkata Subramaniam\\nIBM Research, India Research Lab, India\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nAccdrnig to rscheearch at Cmabrigde Uinervtisy, it deosn’t mttaer in what oredr the ltteers in a wrod are, the olny iprmoetnt tihng is that the frist and lsat ltteer be at the rghit pclae. Tihs is bcuseae the human mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe.\\n1\\nUnfortunately computing systems are not yet as \\nsmart as the human mind. Over the last couple of years a significant number of researchers have been focus-sing on noisy text analytics. Noisy text data is found in informal settings (online chat, SMS, e-mails, message boards, among others) and in text produced through automated speech recognition or optical character recognition systems. Noise can possibly degrade the performance of other information processing algo-rithms such as classification, clustering, summarization and information extraction. We will identify some of the key research areas for noisy text and give a brief overview of the state of the art. These areas will be, (i) classification of noisy text, (ii) correcting noisy text, (iii) information extraction from noisy text. We will cover the first one in this chapter and the later two in the next chapter.\\nWe define noise in text as any kind of difference \\nin the surface form of an electronic text from the in-tended, correct or original text. We see such noisy text  \\neveryday in various forms. Each of them has unique characteristics and hence requires special handling. We introduce some such forms of noisy textual data in this section.\\nOnline Noisy Documents: E-mails, chat logs, scrap-\\nbook entries, newsgroup postings, threads in discussion fora, blogs, etc., fall under this category. People are typically less careful about the sanity of written content in such informal modes of communication. These are characterized by frequent misspellings, commonly and not so commonly used abbreviations, incomplete sentences, missing punctuations and so on. Almost always noisy documents are human interpretable, if not by everyone, at least by intended readers.\\nSMS: Short Message Services are becoming more \\nand more common. Language usage over SMS text sig-nificantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language (Choudhury et. al., 2007) .\\nText Generated by ASR Devices: ASR  is the \\nprocess of converting a speech signal to a sequence of words. An ASR system takes speech signal such as monologs, discussions between people, telephonic conversations, etc. as input and produces a string a words, typically not demarcated by punctuations as  transcripts. An ASR system consists of an acoustic model, a language model and a decoding algorithm. The acoustic model is trained on speech data and their corresponding manual transcripts. The language model is trained on a large monolingual corpus. ASR convert audio into text by searching the acoustic model and language model space using the decoding algorithm. Most conversations at contact centers today between agents and customers are recorded. To do any process-ing of this data to obtain customer intelligence it is necessary to convert the audio into text. \\nText Generated by OCR Devices:  Optical character \\nrecognition, or ‘OCR’, is a technology that allows digital images of typed or handwritten text to be transferred into an editable text document. It takes the picture of text and translates the text into Unicode or ASCII. . For handwritten optical character recognition, the rate of recognition is 80% to 90% with clean handwriting. \\nCall Logs in Contact Centers: Today’s contact cen-\\nters (also known as call centers, BPOs, KPOs) produce huge amounts of unstructured data in the form of call logs apart from emails, call transcriptions, SMS, chat ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3d3d1857-1ab7-42b4-b101-637c8c97a277', embedding=None, metadata={'page_label': '135', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x1800  Analytics for Noisy Unstructured Text Data I\\ntranscripts etc. Agents are expected to summarize an \\ninteraction as soon as they are done with it and before picking up the next one. As the agents work under im-mense time pressure hence the summary logs are very poorly written and sometimes even difficult for human interpretation. Analysis of such call logs are important to identify problem areas, agent performance, evolving problems etc.\\n In this chapter we will be focussing on automatic \\nclassification of noisy text. Automatic text classifica -\\ntion refers to segregating documents into different topics depending on content. For example, categorizing customer emails according to topics such as billing problem, address change, product enquiry etc. It has important applications in the field of email categori-zation, building and maintaining web directories e.g. DMoz, spam filter, automatic call and email routing in contact center, pornographic material filter and so on. \\nNOISy TEXT CATEGORIZATION\\nThe text classification task is one of the learning models for a given set of classes and applying these models to new unseen documents for class assignment. This is an important component in many knowledge extraction tasks; real time sorting of email or files into folder hierarchies, topic identification to support topic-specific processing operations, structured search and/or browsing, or finding documents corresponding to long-term standing interests or more dynamic task-based interests. Two types of classifiers are generally commonly found viz. statistical classifiers and rule \\nbased classifiers. \\nIn statistical techniques a model is typically trained \\non a corpus of labelled data and once trained the system can be used for automatic assignment of unseen data. A survey of text classification can be found in the work by Aas & Eikvil (Aas & Eikvil, 1999). Given a train-ing document collection D ={d\\n1, d2, ….., dM} with true \\nclasses {y1, y2, ….., yM} the task is to learn a model. \\nThis model is used for categorizing a new unlabelled document d\\nu. Typically words appearing in the text are \\nused as features.  Other applications including search rely heavily on taking the markup or link structure of documents into account but classifiers only depend on the content of the documents or the collection of words present in the documents. Once features are extracted from documents, each document is converted into a document vector. Documents are represented in a vec-tor space; each dimension of this space represents a single feature and the importance of that feature in that document gives the exact distance from the origin. The simplest representation of document vectors uses the binary event model, where if a feature j \\n∈V appears in \\ndocument di, then the jth component of di is 1 otherwise \\nit is 0. One of the most popular statistical classification techniques is naive Bayes (McCallum, 1998). In the naive Bayes technique the probability of a document d\\ni belonging to class c is computed as:\\n) | Pr(d c = \\n) Pr() , Pr(\\ndd c\\n= \\n) Pr() | Pr( ) Pr(\\ndc d c\\n∞ ) | Pr( ) Pr(c d c\\n∞ ) | (c d P\\njj ∏\\nThe final approximation of the above equation refers \\nto the naive part of such a model, i.e., the assumption of word independence which means the features are assumed to be conditionally independent, given the class variable.\\nRule-based learning systems have been adopted in \\nthe document classification problem since it has con-siderable appeal. They perform well at finding simple axis-parallel frontiers. A typical rule-based classifica-tion scheme for a category, say C, has the form:\\nAssign category C if antecedent or\\nDo no assign category C if antecedent or\\nThe antecedent in the premise of a rule usually \\ninvolves some kind of feature value comparison. A rule is said to cover a document or a document is said to satisfy a rule if all the feature value comparisons in the antecedent of the rule are true for the document. One of the well known works in the rule based text classification domain is RIPPER. Like a standard separate-and-conquer algorithm, it builds a rule set incrementally. When a rule is found, all documents covered by the rule are discarded including positive ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2893d1fe-9d21-41e7-b4ab-4c0cf1318225', embedding=None, metadata={'page_label': '136', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x180\\x18Analytics for Noisy Unstructured Text Data I\\nAand negative documents. The rule is then added to the \\nrule set. The remaining documents are used to build other rules in the next iteration.    \\nIn both statistical as well as rule based text clas-\\nsification techniques, the content of the text is the sole determiner of the category to be assigned. However noise in the text distorts the content and hence read-ers can expect the categorization performance to get affected by noise in the text. Classifiers are essentially trained to identify correlation between extracted features (words) with different categories which can be later utilized to categorize new documents. For example, words like exciting offer get a free laptop  might have \\nstronger correlation with category spam emails than \\nnon-spam emails. Noise in text distorts this feature space excitinng ofer get frree lap top  will be new set \\nof features and the categorizer will not be able to re-late it to the spam emails category. The feature space \\nexplodes as the same feature can appear in different forms due to spelling errors, poor recognition, wrong transcription, etc. In the remaining part of this section we will give an overview how people have approached the problem of categorizing noisy text.\\nCategorization of OCRed Documents\\nElectronically recognized handwritten documents and documents generated from OCR process are typical examples of noisy text because of the errors introduced by the recognition process. Vinciarelli (Vinciarelli, 2004) has studied the characteristics of noise present in such data and its effects on categorization accuracy. A subset of documents from the Reuters-21578 text classification dataset were taken and noise was intro -\\nduced using two methods: first a subset of documents were manually written and recognized using an offline handwriting recognition system. In the second the OCR based extraction process was simulated by randomly changing a certain percentage of characters. According to them for recall values up to 60-70 percent depending on the sources, the categorization system is robust to noise even when the Term Error Rate is higher than 40 percent. It was also observed that the results from the handwritten data appeared to be lower than those obtained from OCR simulations. Generic systems for text categorization based on statistical analysis of representative text corpora have been proposed (Bayer et. al., 1998). Features are extracted from training texts by selecting substrings from actual word forms and applying statistical information and general linguistic knowledge followed by dimensionality reduction by linear transformation.  The actual categorization system is based on minimum least-squares approach.  The system is evaluated on the tasks of categorizing abstracts of paper-based German technical reports and business letters concerning complaints. Approximately 80% classification accuracy is obtained and it is seen that the system is very robust against recognition or typing errors.\\nIssues with categorizing OCRed documents are also \\ndiscussed by many other authors (Brooks & Teahan, 2007), (Hoch, 1994) and (Taghva et. al., 2001).\\nCategorization of ASRed Documents\\nAutomatic Speech Recognition (ASR) is simply the process of converting an acoustic signal to a sequence of words. Researchers have proposed different techniques for speech recognition tasks based on Hidden Markov model (HMM), neural networks, Dynamic time warp-ing (DTW) (Trentin & Gori, 2001). The performance of an ASR system is typically measured in terms of Word Error Rate (WER), which is derived from the Levenshtein distance, working at word level instead of character.  WER can be computed as\\nWER = \\nNI D S+ +\\nwhere S is the number of substitutions, D is the number \\nof the deletions, I is the number of the insertions, and N is the number of words in the reference. Bahl et.al.  (Bahl et. al. 1995) have built an ASR system and dem-onstrated its capability on benchmark datasets.\\nASR systems give rise to word substitutions, dele-\\ntions and insertions, while OCR systems produce es-sentially word substitutions. Moreover, ASR systems are constrained by a lexicon and can give as output only words belonging to it, while OCR systems can work without a lexicon (this corresponds to the possibility of transcribing any character string) and can output sequences of symbols not necessarily corresponding to actual words. Such differences are expected to have strong influence on performance of systems designed for categorizing ASRed documents in comparison to categorization of OCRed documents. A lot of work on automatic call type classification for the purpose of ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8d0b0c5-b473-4d86-bcbc-90f5b09f4d46', embedding=None, metadata={'page_label': '137', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180\\x18  Analytics for Noisy Unstructured Text Data I\\ncategorizing calls (Tang et al., 2003), call routing (Kuo \\nand Lee, 2003; Haffner et al., 2003), obtaining call log summaries (Douglas et al., 2005), agent assisting and monitoring (Mishne et al., 2005) has appeared in the past.Here calls are classified based on the transcription from an ASR system. One interesting work on seeing effect of ASR noise on text classification was done on a subset of benchmark text classification dataset Re-uters-21578\\n2 (Agarwal et. al., 2007). They read out and \\nautomatically transcribed 200 documents and applied a text classifier trained on clean Reuters-21578 training corpus\\n3. Surprisingly, in spite of high degree of noise, \\nthey did not observe much degradation in accuracy.\\nEffect of Spelling Errors on Categorization\\nSpelling errors are an integral part of written text—elec-tronic as well as non-electronic. Every reader reading this book must have been scolded by their teacher in school for spelling words wrongly! In this era of electronic text people have become less careful while writing resulting poorly written text containing ab-breviations, short forms, acronyms, wrong spellings. Such electronic text documents including email, chat log, postings, SMSs are sometimes difficult to interpret even for human beings. It goes without saying that text analytics on such noisy data is a non trivial task.\\nWrong spellings can affect automatic classification \\nperformance in multiple ways depending on the nature of the classification technique being used. In the case of statistical techniques, spelling differences distort the feature space. If training as well as the test data corpus are noisy, while learning the model the classifier will treat variants of the same words as different features. As a result the observed joint probability distribution will be different from the actual distribution. If the proportion of wrongly spelt words is high then the distortion can be significant and will hurt the accuracy of the resultant classifier. However, if the classifier is trained on a clean corpus and the test documents are noisy, then wrongly spelt words will be treated as unseen words and will not help in classification. In an unlikely situation a wrongly spelt word present in a test document may become a different valid feature and worse, may become a valid indicative feature of a different class. A standard technique in the text clas-sification process is feature selection  which happens \\nafter feature extraction  and before training.  Feature selection typically employs some statistical measures over the training corpus and ranks features in order of the amount of information (correlation) they have with respect to the class labels of the classification task at hand.  After the feature set has been ranked, the top few features are retained (typically order of hundreds or a few thousand) and the others are discarded. Feature selection should be able to eliminate wrongly spelt words present in the training data provided (i) the proportion of wrongly spelt words is not very large and (ii) there is no regular pattern in spelling errors\\n4.  \\nHowever it has been observed, even at high degree of spelling errors the classification accuracy does not suffer much (Agarwal et al., 2007).\\nRule based classification techniques also get nega-\\ntively affected by spelling errors. If the training data contains spelling errors then some of the rules may not get the required statistical significance. Due to spelling errors present in the test data a valid rule may not fire and worse, an invalid rule may fire leading to a wrong categorization. Suppose RIPPER has learnt a rule set like:\\nAssign category “sports” IF\\n(the document contains {\\\\it sports}) OR  (the document contains {\\\\it exercise} AND {\\\\it out-door}) OR  (the document contains {\\\\it exercise} but not {\\\\it home-work} {\\\\it exam}) OR  (the document contains {\\\\it play} AND {\\\\it rule}) OR  ……\\nA hypothetical test document containing repeated \\noccurrences of exercise,  but each time wrongly spelt as \\nexarcise, will not be categorized to the sports category \\nand hence lead to misclassification.\\nCONCLUSION \\nIn this chapter we have looked at noisy text analytics. This topic is gaining in importance as more and more noisy data gets generated and needs processing. In particular we have looked at techniques for correcting noisy text and for doing classification. We have pre -\\nsented a survey of existing techniques in the area and have shown that even though it is a difficult problem it is possible to address it with a combination of new and existing techniques.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='298968eb-6970-4220-a635-6c459c31eb23', embedding=None, metadata={'page_label': '138', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x180\\x18Analytics for Noisy Unstructured Text Data I\\nAREFERENCES\\nK. Aas & L. Eikvil (1999). Text Categorisation: A Sur-\\nvey. Technical report, Norwegian Computing Center.\\nS. Agarwal, S. Godbole, D. Punjani & S. Roy (2007). \\nHow Much Noise is too Much: A Study in Automatic Text Classification. In Proceedings of the IEEE Inter -\\nnational Conference on Data Mining series (ICDM), Nebraska, Omaha (To Appear). \\nL. R. Bahl, S. Balakrishnan-Aiyer, J. Bellegarda, M. \\nFranz, P. Gopalakrishnan, D. Nahamoo, M. Novak, M. Padmanabhan, M. Picheny, and S. Roukos. Performance of the IBM large vocabulary continuous speech recog-nition system on the ARPA wall street journal task. In Proc. ICASSP ’95, pages 41–44, Detroit, MI, 1995.\\nT. Bayer, U. Kressel, H. Mogg-Schneider, &  Renz \\n(1998). Categorizing Paper Documents. Computer Vision and Image Understanding, 70(3) (299-306).\\nR. Brooks & L. J. Teahan (2007). A Practical Implemen-\\ntation of Automatic Text Categorization and Correction of the Conversion of Noisy OCR Documents into Braille and Large Print. Proceedings of Workshop on Analytics for Noisy Unstructured Text Data (at IJCAI 2007). Jan, Hyderabad, India.\\nS. Douglas, D. Agarwal, T. Alonso, R. M. Bell, M. \\nGilbert, D. F. Swayne and C. V olinsky. 2005. Mining Customer Care Dialogs for “Daily News”. IEEE Trans. on Speech and Audio Processing, 13(5):652–660.\\nP. Haffner, G. Tur & J. H. Wright (2003). Optimizing \\nSVMs for Complex Call Classification. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing.\\nR. Hoch (1994). Using IR Techniques for Text Classi-\\nfication in Document Analysis. In Proceedings of 17th ACM SIGIR Conference on Research and Development in Information Retrieval, (31-40).\\nH.-K J. Kuo and C.-H. Lee. 2003. Discriminative Trai-\\nning of Natural Language Call Routers. IEEE Trans. on Speech and Audio Processing, 11(1):24–35.\\nA. McCallum and K. Nigam. A comparison of event \\nmodels for naive Bayes text classification. In AAAI/ICML-98 Workshop on Learning for Text Categori-zation, 1998. G. Mishne, D. Carmel, R. Hoory, A. Roytman and A. Soffer. 2005. Automatic Analysis of Call-center Conversations. Conference on Information and Know-ledge Management. October 31-November 5, Bremen, Germany.\\nK. Taghva, T. Narkter, J. Borsack, Lumos. S., A. Condit, \\n&  Young (2001). Evaluating Text Categorization in the Presence of OCR Errors. In Proceedings of IS&T SPIE 2001 International Symposium on Electronic Imaging Science and Technology, (68-74).\\nM. Tang, B. Pellom and K. Hacioglu. 2003. Calltype \\nClassification and Unsupervised Training for the Call Center Domain. Automatic Speech Recognition and UnderstandingWorkshop. November 30-December 4, St. Thomas, U S Virgin Islands.\\nE. Trentin  & M. Gori  (2001). A Survey of Hybrid \\nANN/HMM Models for Automatic Speech Recogni-tion. Neurocomputing journal. V olume 37. (91-126) \\nA. Vinciarelli (2005). Noisy Text Categorization. IEEE \\nTransactions on Pattern Analysis and Machine Intelli-gence, V ol. 27, no. 12.  (1882 – 1295).\\n Vlachos (2006). Active Annotation. In Proceedings of \\nthe EACL 2006 Workshop on Adaptive Text Extraction and Mining, Trento, Italy.\\nKEy TERMS\\nAutomatic Speech Recognition:  Machine recogni-\\ntion and conversion of spoken words into text.\\nData Mining:  The application of analytical methods \\nand tools to data for the purpose of identifying patterns, relationships or obtaining systems that perform useful tasks such as classification, prediction, estimation, or affinity grouping.\\nInformation Extraction: Automatic extraction of \\nstructured knowledge from unstructured documents.\\nNoisy Text: Text with any kind of difference in the \\nsurface form, from the intended, correct or original text.\\nOptical Character Recognition: Translation of \\nimages of handwritten or typewritten text (usually captured by a scanner) into machine-editable text.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='159e6e46-448f-4edd-b841-09d135830464', embedding=None, metadata={'page_label': '139', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180\\x18  Analytics for Noisy Unstructured Text Data I\\nRule Induction:  Process of learning, from cases \\nor instances, if-then rule relationships that consist of \\nan antecedent (if-part, defining the preconditions or coverage of the rule) and a consequent (then-part, stating a classification, prediction, or other expres-sion of a property that holds for cases defined in the antecedent).\\nText Analytics:  The process of extracting useful and \\nstructured knowledge from unstructured documents to find useful associations and insights.\\nText Classification (or Text Categorization): Is \\nthe task of learning models for a given set of classes and applying these models to new unseen documents for class assignment. \\nENDNOTES\\n1 According to http://www.mrc-cbu.cam.ac.uk/\\n%7Emattd/Cmabrigde/, this is an internet hoax. However we found it interesting and hence in-cluded here.\\n2 http://www.daviddlewis.com/resources/testcol-\\nlections/\\n3 This dataset is available from http://kdd.ics.uci.\\nedu/databases/reuters_transcribed/reuters_tran-scribed.html\\n4 Note: this assumption may not hold true in the \\ncase of cognitive errors', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dcca4780-16f6-4799-82f4-65984af16b51', embedding=None, metadata={'page_label': '140', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180\\x18\\nAAnalytics for Noisy Unstructured Text \\nData II\\nL. Venkata Subramaniam\\nIBM Research, India Research Lab, India\\nShourya Roy\\nIBM Research, India Research Lab, India\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nThe importance of text mining applications is growing proportionally with the exponential growth of electronic text. Along with the growth of internet many other sources of electronic text have become really popular. With increasing penetration of internet, many forms of communication and interaction such as email, chat, newsgroups, blogs, discussion groups, scraps etc. have become increasingly popular. These generate huge amount of noisy text data everyday. Apart from these the other big contributors in the pool of electronic text documents are call centres and customer relationship management organizations in the form of call logs, call transcriptions, problem tickets, complaint emails etc., electronic text generated by Optical Character Recognition (OCR) process from hand written and printed documents and mobile text such as Short Mes-sage Service (SMS). Though the nature of each of these documents is different but there is a common thread between all of these—presence of noise.\\nAn example of information extraction is the extrac-\\ntion of instances of corporate mergers, more formally MergerBetween(company1,company2,date) , from an \\nonline news sentence such as: “Yesterday, New-York \\nbased Foo Inc. announced their acquisition of Bar Corp.” Opinion(product1,good) , from a blog post such \\nas: “I absolutely liked the texture of SheetK quilts.” \\nAt superficial level, there are two ways for informa-\\ntion extraction from noisy text. The first one is cleaning text by removing noise and then applying existing state of the art techniques for information extraction. There in lies the importance of techniques for automatically correcting noisy text. In this chapter, first we will review some work in the area of noisy text correction. The sec-ond approach is to devise extraction techniques which are robust with respect to noise. Later in this chapter, we will see how the task of information extraction is \\naffected by noise.  \\nNOISy TEXT CORRECTION\\nBefore moving on to techniques for processing noisy \\ntext we will briefly introduce methods for correcting noisy text. One of the most common forms of noise in text is wrong spelling. Kukich provides a comprehen-sive survey of techniques pertaining to detecting and correcting spelling errors (Kukich, 1992). According to this survey, three types of nonword misspellings are typically found viz. typographic such as teh, speel, \\ncognitive such as recieve , conspeeracy and phonetic \\nsuch as abiss, nacherly. A distinction must be made \\nbetween automatically detecting  such errors and auto-\\nmatically correcting  those errors. The latter is a much \\nharder problem. Most of the recent work in this area is about correcting spelling mistakes automatically. Golding and Roth (Golding & Roth, 1999) proposed a combination of a variant of Winnow, a multiplicative \\nweight-update algorithm and weighted majority voting for context sensitive spelling correction. Mangu and Brill (Mangu & Brill, 1997) have shown that a small set of human understandable rules is more meaningful than a large set of opaque features and weights. Hybrid methods capturing the context using trigrams of the parts-of-speech tags and a feature based method have also been proposed to handle context sensitive spelling correction (Golding & Schabes, 1996). There is a lot of work related to automatic correction of spelling errors (Agirre et. al., 1998), (Zamora et. al., 1983), (Golding,  1995). A complete bibliography of all the work related to spelling error detection and correction can be found in (Beebe,  2005). On a related note, automatic spelling error correction techniques have been applied for other ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07592529-d14e-457f-bf09-e11242699953', embedding=None, metadata={'page_label': '141', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180\\x18  Analytics for Noisy Unstructured Text Data II\\napplications such as semantic role labelling (Sang et. \\nal., 2005).\\nThere is also recent work on correcting the output of \\nSMS text (Aw et. al., 2006) (Choudhury et. al., 2007), OCR errors (Nartker et. al., 2003) and ASR errors (Sarma & Palmer, 2004).  \\nINFORMATION EXTRACTION FROM NOISy TEXT\\nThe goal of Information Extraction (IE) is to automati-cally extract structured information from the unstruc-tured documents. The extracted structured information has to be contextually and semantically well-defined data from a given domain. A typical application of IE is to scan a set of documents written in natural language and populate a database with the information extracted. The MUC (Message Understanding Conference) con-ference was one effort at codifying the IE task and expanding it (Chinchor, 1998).\\nThere are two basic approaches to the design of IE \\nsystems. One comprises the knowledge engineering \\napproach where a domain expert writes a set of rules \\nto extract the sought after information. Typically the process of building the system is iterative whereby a set of rules is written, the system is run and the output examined to see how the system is performing. The domain expert then modifies the rules to overcome any under- or over-generation in the output. The second is the automatic training approach. This approach is \\nsimilar to classification where the texts are appropriately annotated with the information being extracted. For example, if we would like to build a city name extractor, then the training set would include documents with all the city names marked. An IE system would be trained on this annotated corpus to learn the patterns that would help in extracting the necessary entities.  \\nAn information extraction system typically consists \\nof natural language processing steps such as morpho-logical processing, lexical processing and syntactic analysis. These include stemming to reduce inflected forms of words to their stem, parts of speech tagging to assign labels such as noun, verb, etc. to each word and parsing to determine the grammatical structure of sentences.Named Entity Annotation of Web Posts\\nExtraction of named entities is a key IE task. It seeks to locate and classify atomic elements in the text into predefined categories such as the names of persons, or -\\nganizations, locations, expressions of times, quantities, monetary values, percentages, etc. Entity recognition systems either use rule based techniques or statistical models. Typically a parser or a parts of speech tagger identifies elements such as nouns, noun phrases, or pronouns. These elements along with surface forms of the text are used to define templates for extract -\\ning the named entities. For example, to tag company names it would be desirable to look at noun phrases that contain the words company or incorporated in \\nthem. These rules can be automatically learnt using a tagged corpus or could be defined manually. Most known approaches do this on clean well formed text. However, named entity annotation of web posts such as online classifieds, product listings etc. is harder be-cause these texts are not grammatical or well written. In such cases reference sets have been used to annotate parts of the posts (Michelson & Knoblock, 2005). The reference set is thought of as a relational set of data with a defined schema and consistent attribute values. Posts are now matched to their nearest records in the reference set. In the biological domain gene name an-notation, even though it is performed on well written scientific articles, can be thought of in the context of noise, because many gene names overlap with common English words or biomedical terms. There have been studies on the performance of  the gene name annotator when trained on noisy data (Vlachos, 2006).\\nInformation Extraction from OCRed Documents\\nDocuments obtained from OCR may have not only unknown words and compound words, but also incor-rect words due to OCR errors. In their work Miller et. al. (Miller et. al., 2000) have measured the effect of OCR noise on IE performance. Many IE methods work directly on the document image to avoid errors resulting from converting to text. They adopt keyword matching by searching for string patterns and then use global document models consisting of keyword models and their logical relationships to achieve robustness in matching (Lu & Tan, 2004). The presence of OCR errors has a detrimental effect on information access ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='55549e53-3385-4401-b899-8ce984b6ae97', embedding=None, metadata={'page_label': '142', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x180\\x18Analytics for Noisy Unstructured Text Data II\\nAfrom these documents (Taghva et. al., 2004). How-\\never, post processing of these documents to correct these errors exist and have been shown to give large improvements.\\nInformation Extraction from ASRed Documents\\nThe output of an ASR system does not contain case information and punctuations. It has been shown that in the absence of punctuations extraction of different syntactic entities like parts of speech and noun phrases is not accurate (Nasukawa et. al., 2007). So IE from ASRed documents becomes harder. Miller et. al. (Miller et. al., 2000) have shown how IE performance varies with ASR noise. It has been shown that it is possible to build aggregate models from ASR data (Roy & Subramaniam, 2006). In this work topical models are constructed by utilizing inter document redundancy to overcome the noise. In this work only a few natural language processing steps have been used. Phrases have been aggregated over the noisy collection to get to the clean underlying text.\\nFUTURE TRENDS \\nMore and more data from sources like chat, conver-sations, blogs, discussion groups need to be mined to capture opinions, trends, issues and opportunities. These forms of communication encourage informal language which can be considered noisy due to spell-ing errors, grammatical errors and informal writing styles. Companies are interested in mining such data to observe customer preferences and improve customer satisfaction. Online agents need to be able to understand web posts to take actions and communicate with other agents. Customers are interested in collated product reviews from web posts of other users. The nature of the noisy text warrants moving beyond traditional text analytics techniques. There is need for developing natural language processing techniques that are robust to noise. Also techniques that implicitly and explicitly tackle textual noise need to be developed.CONCLUSION \\nIn this chapter we have looked at information extraction from noisy text. This topic is gaining in importance as more and more noisy data gets generated and useful information needs to be obtained from this. We have presented a survey of existing techniques information extraction techniques. We have also presented some of the future trends in noisy text analytics.\\nREFERENCES\\nE. Agirre, K. Gojenola, K. Sarasola & A. V outilainen (1998). Towards a Single Proposal in Spelling Correc-tion. Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computatio-nal Linguistics (22-28).\\nAw, M. Zhang, J. Xiao & J. Su (2006). A Phrase-Based \\nStatistical Model for SMS Text Normalization. In Pro-ceedings of the Joint conference of the Association for Computational Linguistics and the International Com-mittee on Computational Linguistics (ACL-COLING 2006), Sydney, Australia.\\nN. H. F. Beebe (2005). A Bibliography of Publications \\non Computer Based Spelling Error Detection and Cor-rection. http://www.math.utah.edu/pub/tex/bib/spell.ps.gz.\\nM. Choudhury, R. Saraf, V . Jain, S. Sarkar & A. Basu \\n(2007). Investigation and Modeling of the Structure of Texting Language. In Proceedings of the IJCAI 2007 Workshop on Analytics for Noisy Unstructured Text Data (AND 2007), Hyderabad, India.\\nN. Chinchor (1998). Overview of MUC-7. http://\\nwww-nlpir.nist.gov/related_projects/muc/proceedin-gs/muc_7_proceedings/overview.html\\nR. Golding (1995). A Bayesian Hybrid Method for Con-\\ntext-Sensitive Spelling Correction. Proceedings of the Third Workshop on Very Large Corpora (39—53).\\n R. Golding & D. Roth (1999). A Winnow-Based Appro-\\nach to Context-Sensitive Spelling Correction.  Journal of Machine Learning. V olume 34 (1-3) (107-130)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5e2f1046-f910-4695-a271-bbe5923f98f8', embedding=None, metadata={'page_label': '143', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x180\\x18  Analytics for Noisy Unstructured Text Data II\\nR. Golding & Y . Schabes (1996). Combining Tri-\\ngram-Based and Feature-Based Methods for Context-Sensitive Spelling Correction. Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics (71—78).\\nK. Kukich (1992). Technique for Automatically Correc-\\nting Words in Text. ACM Computing Survey. V olume 24 (4) (377—439).\\nY . Lu & C. L. Tan (2004). Information Retrieval in \\nDocument Image Databases. IEEE Transactions on Knowledge and Data Engineering. V ol 16, No. 11. (1398-1410)\\nL. Mangu & E. Brill (1997). Automatic Rule Acquisi-\\ntion for Spelling Correction. Proc. 14th International Conference on Machine Learning. (187—194). \\nM. Michelson & C. A. Knoblock (2005). Semantic \\nAnnotation of Unstructured and Ungrammatical Text. \\nIn Proceedings of the International Joint Conference on Artificial Intelligence .\\nD. Miller, S. Boisen, R. Schwartz, R. Stone & R. Wei-schedel (2000). Named Entity Extraction from Noisy Input: Speech and OCR. Proceedings of the Sixth Con-ference on Applied Natural Language Processing.\\nT. Nartker, K. Taghva, R. Young, J. Borsack, and A. \\nCondit (2003). OCR Correction Based On Document Level Knowledge. In Proc. IS&T/SPIE 2003 Intl. Symp. on Electronic Imaging Science and Technology, volume \\n5010, Santa Clara, CA.\\nT. Nasukawa, D. Punjani, S. Roy, L. V . Subramaniam \\n& H. Takeuchi (2007). Adding Sentence Boundaries to Conversational Speech Transcriptions Using Noisily Labeled Examples. In Proceedings of the IJCAI 2007 Workshop on Analytics for Noisy Unstructured Text Data (AND 2007), Hyderabad, India.\\nS. Roy & L. V . Subramaniam (2006). Automatic Gen-\\neration of Domain Models for Call-Centers from Noisy Transcriptions. In Proceedings of the Joint conference of the Association for Computational Linguistics and the International Committee on Computational Linguistics (ACL-COLING 2006), Sydney, Australia.\\nE. T. K. Sang, S. Canisius, A. van den Bosch & T. \\nBogers (2005). Applying Spelling Error Correction Techniques for Improving Semantic Role Labelling. In Proceedings of CoNLL.Sarma & D. Palmer (2004). Context-based Speech Rec-ognition Error Detection and Correction. In Proceed-ings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004.\\n K. Taghva, T. Narkter & J. Borsack (2004). Information \\nAccess in the Presence of OCR Errors. ACM Hardcopy Document Processing Workshop, Washington, DC, USA. (1-8)\\nK. Taghva, T. Narkter, J. Borsack, Lumos. S., A. Condit, \\n&  Young (2001). Evaluating Text Categorization in the Presence of OCR Errors. In Proceedings of IS&T SPIE 2001 International Symposium on Electronic Imaging Science and Technology, (68-74).\\nE. M. Zamora, J. J. Pollock, & A. Zamora (1983). \\nThe Use of Trigram Analysis for Spelling Error De-tection. Information Processing and Management 17. 305-316.\\nKEy TERMS\\nAutomatic Speech Recognition:  Machine recogni-\\ntion and conversion of spoken words into text.\\nData Mining:  The application of analytical methods \\nand tools to data for the purpose of identifying patterns, relationships or obtaining systems that perform useful tasks such as classification, prediction, estimation, or affinity grouping.\\nInformation Extraction: Automatic extraction of \\nstructured knowledge from unstructured documents.\\nKnowledge Extraction:  Explicitation of the internal \\nknowledge of a system or set of data in a way that is easily interpretable by the user.\\nNoisy Text: Text with any kind of difference in the \\nsurface form, from the intended, correct or original text.\\nOptical Character Recognition: Translation of \\nimages of handwritten or typewritten text (usually captured by a scanner) into machine-editable text.\\nRule Induction:  Process of learning, from cases \\nor instances, if-then rule relationships that consist of an antecedent (if-part, defining the preconditions or ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='20299b73-2b00-43aa-99a5-b7be93ca0e8c', embedding=None, metadata={'page_label': '144', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x180\\x18Analytics for Noisy Unstructured Text Data II\\nAcoverage of the rule) and a consequent (then-part, \\nstating a classification, prediction, or other expres-sion of a property that holds for cases defined in the antecedent).\\nText Analytics:  The process of extracting useful and \\nstructured knowledge from unstructured documents to find useful associations and insights.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19c38052-21b9-4712-aa76-fafdd6d1b1a6', embedding=None, metadata={'page_label': '145', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x180\\nAngiographic Images Segmentation \\nTechniques\\nFrancisco J. Nóvoa\\nUniversity of A Coruña, Spain\\nAlberto Curra\\nUniversity of A Coruña, Spain\\nM. Gloria López\\nUniversity of A Coruña, Spain\\nVirginia Mato\\nUniversity of A Coruña, Spain\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nHeart-related pathologies are among the most frequent health problems in western society. Symptoms that point towards cardiovascular diseases are usually diagnosed with angiographies, which allow the medical expert \\nto observe the bloodflow in the coronary arteries and detect severe narrowing (stenosis). According to the severity, extension, and location of these narrowings, the expert pronounces a diagnosis, defines a treatment, and establishes a prognosis.\\nThe current modus operandi is for clinical experts to \\nobserve the image sequences and take decisions on the basis of their empirical knowledge. Various techniques and segmentation strategies now aim at objectivizing \\nthis process by extracting quantitative and qualitative information from the angiographies. \\nBACKGROUND\\nSegmentation is the process that divides an image in its constituting parts or objects. In the present context, it consists in separating the pixels that compose the coro-nary tree from the remaining “background” pixels. \\nNone of the currently applied segmentation  methods \\nis able to completely and perfectly extract the vascula-ture of the heart, because the images present complex morphologies and their background is inhomogeneous due to the presence of other anatomic elements and artifacts such as catheters.\\nThe literature presents a wide array of coronary tree \\nextraction methods: some apply pattern recognition techniques based on pure intensity, such as threshold-\\ning followed by an analysis of connected components, whereas others apply explicit vessel models to extract the vessel contours. \\nDepending on the quality and noise of the image, \\nsome segmentation methods may require image pre-processing prior to the segmentation algorithm; others \\nmay need postprocessing operations to eliminate the effects of a possible oversegmentation. \\nThe techniques and algorithms for vascular seg-\\nmentation could be categorized as follows (Kirbas, Quek, 2004):\\n1. Techniques for “pattern-matching” or pattern \\nrecognition\\n2. Techniques based on models \\n3. Techniques based on tracking4. Techniques based on artificial intelligence5. Main Focus\\nThis section describes the main features of the \\nmost commonly accepted coronary tree segmentation \\ntechniques. These techniques automatically detect objects and their characteristics, which is an easy and immediate task for humans, but an extremely complex process for artificial computational systems. \\nTechniques Based on Pattern Recognition\\nThe pattern recognition approaches can be classified into four major categories:', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='458ad075-d672-4211-8a1c-d4106ea8291f', embedding=None, metadata={'page_label': '146', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Angiographic Images Segmentation Techniques\\nA\\nMultiscale Methods\\nThe multiscale method extracts the vessel method by \\nmeans of images of varying resolutions. The main advantage of this technique resides in its high speed. Larger structures such as main arteries are extracted by segmenting low resolution images, whereas smaller structures are obtained through high resolution im-ages. \\nMethods Based on Skeletons\\nThe purpose of these methods is to obtain a skeleton  \\nof the coronary tree: a structure of smaller dimen-sions than the original that preserves the topological properties and the general shape of the detected object. Skeletons based on curves are generally used to recon-struct vascular structures (Nyström, Sanniti di Baja & Svensson, 2001). Skeletonizing algorithms are also called “thinning algorithms”.\\nThe first step of the process is to detect the central \\naxis of the vessels or “centerline”. This axis is an imaginary line that follows each vessel in its central axis, i.e. two normal segments that cross the axis in opposite sense should present the same distance from the vessel’s edges. The total of these lines constitutes the skeleton of the coronary tree. The methods that are used to detect the central axes can be classified into three categories:\\nMethods Based on Crests\\nOne of the first methods to segment angiographic im-ages on the basis of crests was proposed by Guo and Richardson (Guo & Ritchardson, 1998). This method treats angiographies as topographic maps in which \\nthe detected crests constitute the central axes of the vessels.\\nThe image is preprocessed by means of a median \\nfilter and smoothened with non-linear diffusion. The region of interest is then selected through thresholding , \\na process that eliminates the crests that do not correspond with the central axes. Finally, the candidate central axes are joined with curve relaxation techniques. \\nMethods Based on Regions Growth\\nTaking a known point as seed point, these techniques segment images through the incremental inclusion of pixels in a region on the basis of an a priori established \\ncriterion. There are two especially important criteria: similitude in the value, and spatial proximity (Jain, Kasturi & Schunck, 1995). It is established that pixels that are sufficiently near others with similar grey levels belong to the same object. The main disadvantage of this method is that it requires the intervention of the user to determine the seed points. \\nO’Brien and Ezquerra (O’Brien & Ezquerra, 1994) \\npropose the automatic extraction of the coronary ves-sels in angiograms on the basis of temporary, spatial, and structural restrictions. The algorithm starts with a low-pass filter and the user’s definition of a seed point. The system then starts to extract the central axes by means of the “globe test” mechanism, after which the detected regions are entangled through the graph theory. The applied test also allows us to discard the regions that are detected incorrectly and do not belong to the vascular tree.Figure 1. Regions growth applied to an angiography\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a613ca0-0400-4517-8213-326d807eb1c0', embedding=None, metadata={'page_label': '147', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Angiographic Images Segmentation Techniques\\nMethods Based on Differential Geometry\\nThe methods that are based on differential geometry treat images as hypersurfaces and extract their fea-tures using curvature and surface crests. The points of hypersurface’s crest correspond to the central axis of the structure of a vessel. This method can be applied to bidimensional as well as tridimensional images; angiograms are bidimensional images and are therefore modelled as tridimensional hypersurfaces.\\nExamples of reconstructions can be found in Prinet \\net al (Prinet, Mona & Rocchisani, 1995), who treat the images as parametric surfaces and extract their features by means of surfaces and crests. \\nCorrespondence Filters Methods\\nThe correspondence filter approach convolutes the image with multiple correspondence filters so as to extract the regions of interest. The filters are designed to detect different sizes and orientations. \\nPoli and Valli (Poli, R & Valli, 1997) apply this \\ntechnique with an algorithm that details a series of multiorientation linear filters that are obtained as linear combinations of Gaussian “kernels”. These filters are sensitive to different vessel widths and orientations. \\nMao et al (Mao, Ruan, Bruno, Toumoulin, Col-\\nlorec & Haigron, 1992) also use this type of filters in an algorithm based on visual perception models that affirm that the relevant parts of the objects in images with noise appear normally grouped.\\nMorphological Mathematical Methods\\nMathematical morphology defines a series of operators that apply structural elements to the images so that their morphological features can be preserved and ir-relevant elements eliminated. The main morphological operations are the following:\\n• Dilatation: Expands objects, fills up empty spaces, \\nand connects disjunct regions.\\n• Erosion: Contracts objects, separates regions.\\n• Closure: Dilatation + Erosion.\\n• Opening: Erosion + Dilatation.\\n• \"Top hat\" transformation: Extracts the struc-\\ntures with a linear shape\\n• \"Watershed” transformation: \"Inundates” the \\nimage that is taken as a topographic map , and \\nextracts the parts that are not \"flooded\".\\nEiho and Qian (Eiho & Qian, 1997) use a purely \\nmorphological approach to define an algorithm that consists of the following steps:\\n1. Application of the “top hat” operator to emphasize \\nthe vessels\\n2. Erosion to eliminate the areas that do not cor-\\nrespond to vessels\\n3. Extraction of the tree from a point provided by \\nthe user and on the basis of grey levels.\\n4. Slimming down of the tree\\n5. Extraction of edges through “watershed” trans-\\nformation\\nMODEL-BASED TECHNIQUES\\nThese approaches use explicit vessel models to extract the vascular tree. They can be divided into four catego-\\nFigure 2. Morphological operators applied to an angiography\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d269144f-7dfe-46da-afaa-25c23fe9d806', embedding=None, metadata={'page_label': '148', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Angiographic Images Segmentation Techniques\\nAries: deformable models, parametric models, template \\ncorrespondence models, and generalized cylinders. \\nDeformable Models\\nStrategies based on deformable models can be classified in terms of the work by McInerney and Terzopoulos (McInerney & Terzopoulos, 1997).\\nAlgorithms that use deformable models (Merle, \\nFinet, Lienard, & Magnin, 1997) are based on the progressive refining of an initial skeleton built with curves from a series of reference points:\\n• Root points: Starting points for the coronary \\ntree.\\n• Bifurcation points: Points where a main branch \\ndivides into a secundary branch.\\n• End points: Points where a tree branch ends.\\nThese points have to be marked manually.\\nDeformable Parametric Models: \\nActive Contours\\nThese models use a set of parametric curves that \\nadjust to the object’s edges and are modified by both external forces, that foment deformation, and internal forces that resist change. The active contour models or “snakes” in particular are a special case of a more general technique that pretends to adjust deformable models by minimizing energy.\\nKlein et al. (Klein, Lee & Amini, 1997) propose \\nan algorithm that uses “snakes” for 4D reconstruction: they trace the position of each point of the central axis of a skeleton in a sequence of angiograms.\\nDeformable Geometric Models\\nThese models are based on topographic models that are adapted for shape recognition. Malladi et al. (Malladi, Sethian & Vemuri, 1995) for instance adapt the “Level Set Method” (LSM) by representing an edge as a level zero set of a hypersurface of a superior order; the model evolves to reduce a metric defined by the restrictions of edges and curvature, but less rigidly than in the case of the “snakes”. This edge, which constitutes the zero level of the hypersurface, evolves by adjusting to the edges of the vessels, which is what we want to detect.Propagation Methods\\nQuek and Kirbas (Quek & Kirbas, 2001) developed a system of wave propagation combined with a back-tracking mechanism to extract the vessels from an-giographic images. This method basically labels each pixel according to its likeliness to belong to a vessel and then propagates a wave through the pixels that are labeled as belonging to the vessel; it is this wave that definitively extracts the vessels according to the local features it encounters.\\nApproaches based on the correspondence of de-\\nformable templates:\\nThis approach tries to recognize structural models \\n(templates) in an image by using a template as context, i.e. as a priori model. This template is generally repre-\\nsented as a set of nodes connected by a segment. The initial structure is deformed until it adjusts optimally to the structures that were observed in the image.\\nPetrocelli et al. (Petrocelli, Manbeck, & Elion, 1993) \\ndescribe a method based on deformable templates that also incorporates additional previous knowledge into the deformation process. \\nParametric Models\\nThese models are based on the a priori knowledge of the artery’s shape and are used to build models \\nwhose parameters depend on the profiles of the entire vessel; as such, they consider the global information of the artery instead of merely the local information. \\nThe value of these parameters is established after a learning process.\\nThe literature shows the use of models with circu-\\nlar sections (Shmueli, Brody, & Macovski, 1983) and spiral sections (Pappas, & Lim, 1984), because various studies by  Brown, B. G., (Bolson, Frimer, & Dodge, 1977) (Brown, Bolson, Frimer & Dodge, 1982) show that sections of healthy arteries tend to be circular and sections with stenosis are usually elliptical. However, \\nboth circular and elliptical shapes fail to approach ir-regular shapes caused by pathologies or bifurcations.\\nThis model has been applied to the reconstruction \\nof vascular structures with two angiograms (Pellot, Herment, Sigelle, Horain, Maitre & Peronneau, 1994), which is why both healthy and stenotic sections are mod-eled by means of ellipses. This model is subsequently deformed until it corresponds to the shape associated to the birth of a new branch or pathology. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='480e1ae5-6f66-4ed6-b458-e904a128734a', embedding=None, metadata={'page_label': '149', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Angiographic Images Segmentation Techniques\\nGeneralized Cylinder Models\\nA generalized cylinder (GC) is a solid whose central \\naxis is a 3D curve. Each point of that axis has a limited and closed section that is perpendicular to it. A CG is therefore defined in space by a spatial curve or axis and a function that defines the section in that axis. The section is usually an ellipse. Tecnically, GCs should be included in the parametric methods section, but the work that has been done in this field is so extense that it deserves its own category. \\nThe construction of the coronary tree model requires \\none single view to build the 2D tree and estimate the sections. However, there is no information on the depth or the area of the sections, so a second projection will be required.ARTERIAL TRACKING\\nContrary to the approaches based on pattern recognition, where local operators are applied to the entire image, techniques based on arterial follow-up are based on the application of local operators in an area that presumibly belongs to a vessel and that cover its length. From a given point of departure the operators detect the central axis and, by analyzing the pixels that are orthogonal to the tracking direction, the vessel’s edges. There are various methods to determine the central axis and the edges: some methods carry out a sequential track-ing and incorporate connectivity information after a simple edge detection operation, other methods use this information to sequentially track the contours. There are also approaches based on the intensity of the crests, on fuzzy sets, or on the representation of Figure 3. “Snakes” applied to a blood vessel. http://vislab.cs.vt.edu/review/extraction.html\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='21eae8de-5d53-4376-8243-9fb7da04390b', embedding=None, metadata={'page_label': '150', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Angiographic Images Segmentation Techniques\\nA\\ngraphs, where the purpose lies in finding the optimal \\nroad in the graph that represents the image.\\nLu and Eiho (Lu, Eiho, 1993) have described a \\nfollow-up algorithm for the vascular edges in angiog-\\nraphies that considers the inclusion of branches and consists of three steps:\\n1. Edge detection\\n2. Branch search3. Tracking of sequential contours\\nThe user must provide the point of departure, the \\ndirection, and the search range. The edge points are evaluated with a differential smoothening operator in a line that is perpendicular to the direction of the vessel. This operator also serves to detect the branches.\\nTECHNIQUES BASED ON ARTIFICIAL INTELLIGENCE\\nApproaches based on Artificial Intelligence use high-level knowledge to guide the segmentation  and delinea-\\ntion of vascular structures and sometimes use different types of knowledge from various sources.\\nOne possibility (Smets, Verbeeck, Suetens, & \\nOosterlinck, 1988) is to use rules that codify knowl-edge on the morphology of blood vessels; these rules are then used to formulate a hierarchy with which to create the model. This type of system does not offer any good results in arterial bifurcations or in arteries with occlusions.\\nAnother approach (Stansfield, 1986) consists in \\nformulating a rules-based Expert System to identify \\nthe arteries. During the first phase, the image is pro -\\ncessed without making use of domain knowledge to extract segments of the vessels. It is only in the second phase that domain knowledge on cardiac anatomy and physiology is applied.\\nThe latter approach is more robust than the former; \\nbut it presents the inconvencience of not combining all the segments into one vascular structure.\\nFUTURE TRENDS\\nIt cannot be said that one technique has a more promising future than another, but the current tendency is to move away from the abovementioned classical segmentation  \\nalgorithms towards 3D and even 4D reconstructions of the coronary tree.\\nOther lines of research focus on obtaining angio-\\ngraph images by means of new acquisition technologies such as Magnetic Resonance, Computarized High \\nSpeed Tomography, or two-armed angiograph de-\\nvices that achieve two simultaneous projections in Figure 4. Tracking applied to an angiography\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c615ff5-c512-4767-b1fd-7b3175843145', embedding=None, metadata={'page_label': '151', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Angiographic Images Segmentation Techniques\\ncombination with the use of ultrasound intravascular \\ndevices. This type of acquisition simplifies the creation of tridimensional structures, either directly from the acquisition or after a simple processing of the bidi-mensional images.\\nREFERENCES\\nBrown, B. G., Bolson E., Frimer, M., & Dodge, H. T. (1977). Quantitative coronary arteriography. Circula-tion, 55:329-337.\\nBrown, B. G., Bolson E., Frimer, M., & Dodge, H. T. \\n(1982). Arteriographic assessment of coronary athero-sclerosis. Arteriosclerosis, 2:2-15.\\nEiho, S., & Qian, Y . (1997). Detection of coronary \\nartery tree using morphological operator. In Computers in Cardiology 1997, pages 525-528.\\nGonzalez, R. C., & Woods, R. E. (1996). Digital Image \\nProccessing. Addison-Wesley Publishing Company, Inc. Reading, Massachusets, USA.\\nGreenes, R. A., & Brinkley, K. F. (2001). Imaging Sys-\\ntems. De Medical informatics: computer applications in health care and biomedicine. Pp. 485 – 538. Second Edition. 2001. Ed. Springer-Verlag. New York. USA.\\nGuo, D., & Richardson, P. (1998) . Automatic vessel \\nextraction from angiogram images. In Computers in Cardiology 1998, 441 - 444.\\nJain, R.C., Kasturi, R., & Schunck,B. G. (1995). Ma-\\nchine Vision.McGraw-Hill.\\nKirbas, C. & Quek, F. (2004). A review of vessel \\nextraction techniques and algorithms. ACM Comput. Surv., 36(2),81-121.\\nKlein, A. K., Lee, F., & Amini, A. A. (1997). Quan-\\ntitative coronary angiography with deformable spline models. IEEE Transactions on Medical Imaging, 16(5):468-482\\nLu, S., & Eiho, S. (1993). Automatic detection of the \\ncoronary arterial contours with sub-branches from an x-ray angiogram.In Computers in Cardiology 1993. Proceedings., 575-578.\\nNyström, I., Sanniti di Baja, G., & Svensson, S. (2001). \\nRepresenting volumetric vascular structures using curve skeletons. In Edoardo Ardizzone and Vito Di Gesµu, editors, Proceedings of 11th International Conference on Image Analysis and Processing (ICIAP 2001), 495-500, Palermo, Italy, IEEE Computer Society.\\nMalladi, R., Sethian, J. A., & Vemuri, B. C. (1995). \\nShape modeling with front propagation: a level set approach. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 17:158-175.\\nMao, F., Ruan, S.,Bruno, A., Toumoulin, C., Collorec, \\nR., & Haigron, P. (1992). Extraction of structural features in digital subtraction angiography. Biomedi-cal Engineering Days, 1992.,Proceedings of the 1992 International, 166-169.\\nMcInerney, T., & Terzopoulos, D.(1997). Medical \\nimage segmentation using topologically adaptable surfaces. In CVRMedMRCAS ‘97: Proceedings of the First Joint Conference on Computer Vision, Virtual Reality and Robotics in Medicine and Medial Robotics and Computer-Assisted Surgery, 23-32, London, UK, Springer-Verlag.\\nO’Brien, J. F., & Ezquerra, N. F. (1994). Automated \\nsegmentation of coronary vessels in angiographic im-age sequences utilizing temporal, spatial and structural constraints. (Technical report), Georgia Institute of Technology.\\nPappas, T. N, & Lim, J.S. (1984). Estimation of coronary \\nartery boundaries in angiograms. Appl. Digital Image Processing VII, 504:312-321.\\nPellot, C., Herment, A., Sigelle, M., Horain, P., Maitre, \\nH., & Peronneau, P. (1994). A 3d reconstruction of vascular structures from two x-ray angiograms using an adapted simulated annealing algorithm. Medical Imaging, IEEE Transactions on, 13:48-60.\\nPetrocelli, R. R., Manbeck, K. M., & Elion, J. L. (1993). \\nThree dimensional structure recognition in digital an-giograms using gauss-markov methods. In Computers in Cardiology 1993. Proceedings., 101-104.\\nPoli, R., & Valli, G. (1997). An algorithm for real-time \\nvessel enhancement and detection. Computer Methods and Programs in Biomedicine, 52:1-22.\\nPrinet, V ., Mona, O., & Rocchisani, J. M. (1995). \\nMulti-dimensional vessels extraction using crest lines. In Engineering in Medicine and Biology Society, 1995. IEEE 17th Annual Conference, 1:393-394.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a2c3756c-025a-46b4-9636-33a12f202ff2', embedding=None, metadata={'page_label': '152', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Angiographic Images Segmentation Techniques\\nAQuek, F. H. K., & Kirbas, C. (2001). Simulated wave \\npropagation and traceback in vascular extraction. In Medical Imaging and Augmented Reality, 2001. Pro-ceedings. International Worksho, 229-234.\\nShmueli, K., Brody, W. R., & Macovski, A. (1983). \\nEstimation of blood vessel boundaries in x-ray images. Opt. Eng., 22:110-116.\\nSmets, C., Verbeeck, G., Suetens, P., & Oosterlinck, A. \\n(1988). A knowledge-based system for the delineation of blood vessels on subtraction angiograms. Pattern Recogn. Lett., 8(2):113-121.\\nStansfield, S. A. (1986). Angy: A rule-based expert \\nsystem for automatic segmentation of coronary vessels from digital subtracted angiograms. PAMI, 8(3):188-199.\\nKEy TERMS\\nAngiography: Image of blood vessels obtained by \\nany possible procedure.\\nArtery: Each of the vessels that take the blood from \\nthe heart to the other bodyparts.\\nComputerized Tomography: Exploration of X-\\nrays that produces detailed images of axial cuts of the body. A CT obtains many images by rotating around the body. A computer combines all these images into a final image that represents the bodycut like a slice.  \\nExpert System: Computer or computer program \\nthat can give responses that are similar to those of an expert.\\nSegmentation: In computer vision, segmentation \\nrefers to the process of partitioning a digital image into multiple regions. The goal of segmentation is to sim-plify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (structures) in images, in this case, the coronary tree in digital angiography frames.\\nStenosis: A stenosis is an abnormal narrowing \\nin a blood vessel or other tubular organ or structure. A coronary artery that’s constricted or narrowed is called stenosed. Buildup of fat, cholesterol and other substances over time may clog the artery. Many heart attacks are caused by a complete blockage of a vessel in the heart, called a coronary artery.\\nThresholding: A technique for the processing of \\ndigital images that consists in applying a certain prop-erty or operation to those pixels whose intensity value exceeds a defined threshold.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac4b4bdc-da90-47eb-8f97-2c5efb83c5d7', embedding=None, metadata={'page_label': '153', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nANN Application in the Field of Structural \\nConcrete\\nJuan L. Pérez\\nUniversity of A Coruña, Spain\\nMª Isabel Martínez\\nUniversity of A Coruña, Spain\\nManuel F. Herrador\\nUniversity of A Coruña, Spain\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nArtificial Intelligence (AI) mechanisms are more and more frequently applied to all sorts of civil engineering problems. New methods and algorithms which allow civil engineers to use these techniques in a different way on diverse problems are available or being made available. One AI techniques stands out over the rest: Artificial Neural Networks (ANN). Their most remarkable traits are their ability to learn, the possibility of generalization and their tolerance towards mistakes. These characteristics make their use  viable and cost-efficient in any field in general, and in Structural Engineering in particular. The most extended construction material nowadays is concrete, mainly because of its high resistance and its adaptability to formwork during its fabrication process. Along this chapter we will find different applications of ANNs to structural concrete.\\nArtificial Neural Networks\\nWarren McCulloch and Walter Pitts are credited for the origin of Artificial Networks in the 1940s, since they were the first to design an artificial neuron (McCulloch & Pitts, 1943). They proposed the binary mode (active or inactive) neuron model with a fixed threshold which must be surpassed for it to change state. Some of the concepts they introduced still hold useful today.\\nArtificial Neural Networks intend to simulate \\nthe properties found in biological neural systems through mathematical models by the way of artificial mechanisms. A neuron is considered a formal element, or module, or basic network unit which receives information from other modules or the environment; it then integrates and computes this information to emit a single output which will be identically transmitted to subsequent multiple neurons (Wasserman, 1989). \\nThe output of an artificial neuron is determined by \\nits propagation or excitation, activation and transfer functions.\\nThe propagation function is generally the \\nsummation of each input multiplied by the weight of its interconnection (net value):\\n [ ]∑−\\n=⋅ =1\\n0N\\njj ij i p W n    (1)\\nThe activation function modifies the latter, relating \\nthe neural input to the next activation state.\\n [ ] ) 1 ( ), 1 ( ) (− − =t n t a FA t ai i i    (2)\\nThe transfer function is applied to the result of the \\nactivation function. It is used to bound the neuron’s output and is generally given by the interpretation intended for the output. Some of the most commonly used transfer functions are the sigmoid (to obtain values in the [0,1] interval) and the hyperbolic tangent (to obtain values in the [-1,1] interval).\\n ( )) (t a FT outi i=     (3)\\nOnce each element in the process is defined, the type \\nof network (network topology) to use must be designed. These can be divided in forward-feed networks, where ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42b7699e-7ce7-47b4-ab4f-dd44f6033f1e', embedding=None, metadata={'page_label': '154', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN Application in the Field of Structural Concrete\\nAinformation moves in one direction only (from input \\nto output), and networks with partial or total feedback, where information can flow in any direction.\\nFinally, learning rules and training type must be \\ndefined. Learning rules are divided in supervised and non-supervised (Brown & Harris, 1994) (Lin & Lee, 1996) and within the latter, self-organizing learning and reinforcement learning (Hoskins & Himmelblau, 1992). The type of training will be determined by the type of learning chosen.\\nAn Introduction to Concrete (Material and Structure)\\nStructural concrete is a construction material created from the mixture of cement, water, aggregates and additions or admixtures with diverse functions. The goal is to create a material with rock-like appearance, with sufficient compressive strength and the ability to adopt adequate structural shapes. Concrete is moldable during its preparation phase, once the components have mixed together go produce a fluid mass which conveniently occupies the cavities in a mould named formwork. After a few hours, concrete hardens thanks to the chemical hydration reaction experimented by cement, generating a paste which envelops the aggregates and gives the ensemble the appearance of an artificial rock somewhat similar to a conglomerate.\\nHardened concrete offers good compressive \\nstrength, but very low tensile strength. This is why structures created with this material must be reinforced by use of steel rebars, configured by rods which are placed (before pouring the concrete) along the lines where calculation predicts the highest tensile stresses. Cracking, which reduces the durability of the structure, is thus hindered, and sufficient resistance is guaranteed with a very low probability of failure. The entirety formed by concrete and rebar is referred to as Structural Concrete (Shah, 1993).\\nTwo phases thus characterize the evolution of \\nconcrete in time. In the first phase, concrete must be fluid enough to ensure ease of placement, and a time to initial set long enough to allow transportation from plant to worksite. Flowability depends basically on the type and quantity of the ingredients in the mixture. Special chemical admixtures (such as plasticizers and superplasticizers) guarantee flowability without grossly increasing the amount of water, whose ratio relative to the amount of cement (or water/cement ratio, w/c) is on reverse proportion to strength attained. The science of rheology deals with the study of the behavior of fresh concrete. A variety of tests can be used to determine flowability of fresh concrete, the most popular amongst them being the Abrams cone (Abrams, 1922) or slump cone test (Domone, 1998).\\nThe second phase (and longest over time) is the \\nhardened phase of concrete, which determines the behavior of the structure it gives shape to, from the point of view of serviceability (by imposing limitations on cracking and compliance) and resistance to failure (by imposing limitations on the minimal loads that can be resisted, as compared to the internal forces produced by external loading), always within the frame of sufficient durability for the service life foreseen.\\nThe study of structural concrete from every \\npoint of view has been undertaken following many different optics. The experimental path has been very productive, generating along the past 50 years a database (with a tendency to scatter) which has been used to sanction studies carried along the second and third path that follow. The analytical path also constitutes a fundamental tool to approach concrete behavior, both from the material and structural point of view. Development of theoretical behavior models goes back to the early 20th century, and theoretical equations developed since have been corrected through testing (as mentioned above) before becoming a part of codes and specifications. This method of analysis has been reinforced with the development of numerical methods and computational systems, capable of solving a great number of simultaneous equations. In particular, the Finite Element Method (and other methods in the same family) and optimization techniques have brought a remarkable capacity to approximate behavior of structural concrete, having their results benchmarked in may applications by the aforementioned experimental testing.\\nThree basic lines of study are thus available. Being \\ncomplementary between them, they have played a decisive role in the production of national and international codes and rules which guide or legislate the project, execution and maintenance of structural concrete works. Concrete is a complex material, which presents a number of problems for analytical study, and so is an adequate field for the development of analysis techniques based on neural networks (Gonzalez, Martínez and Carro, 2006)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d67d088c-b43d-4b0c-a597-38526b8a46d2', embedding=None, metadata={'page_label': '155', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x180  ANN Application in the Field of Structural Concrete\\nApplication of Artificial Neural Networks to \\nproblems in the field of structural concrete has unfolded \\nin the past few years in two ways. On one hand, analytical and structural optimization systems faster than traditional (usually iterative) methods have been generated starting with expressions and calculation rules. On the other, the numerous databases created form the large amount of tests published in the scientific community have allowed for the development of very powerful ANN which have thrown light on various complex phenomena. In a few cases, specific designed codes have been improved through the use of these techniques; some examples follow.\\nApplication of Artificial Neural Networks to Optimization Problems\\nDesign of concrete structures is based on the determination of two basic parameters: member thickness (effective depth d, depth of a beam or slab \\nsection measured from the compression face to the centroid of reinforcement) and amount of reinforcement (established as the total area A\\ns of steel in a section, \\nmaterialized as rebars, or the reinforcement ratio, the ratio between steel area and concrete area in the section). Calculation methods are iterative, since a large number of conditions must be verified in the structure, and the aforementioned parameters are fixed as a function of three basic conditions which are sequentially followed: structural safety, maximum ductility at failure and minimal cost. Design rules, expressed through equations, allow for a first solution which is corrected to meet all calculation scenarios, finally converging when the difference between input and output parameters are negligible.\\nIn some cases it is possible to develop optimization \\nalgorithms, whose analytical formulation opens the way to the generation of a database. Hadi (Hadi, 2003) has performed this work for simply supported reinforced concrete beams, and the expressions obtained after the optimization process determine the parameters specified above, while simultaneously assigning the cost associated to the optimal solution (related to the cost of materials and formwork). With these expressions, Hadi develops a database with the following variables: applied flexural moment (M), compressive strength of concrete (f\\nc), steel strength (fy), section width (b), \\nsection depth (h), and unit costs of concrete ( Cc), steel \\n(Cs) and formwork (Cf).Network parameters used are as follows. The number \\nof training samples is 550; number of input layer neurons is 8; number of hidden layer neurons is 10; number of output layer neurons is 4; type of backpropagation is Levenberg–Marquardt backpropagation; activation function is sigmoidal function; learning rate; 0.01; number of epochs is 3000; sum-square error achieved is 0.08. The network had been tested with 50 samples and yielded the average error of 6.1%. \\nHadi studies various factors when choosing network \\narchitecture and backpropagation algorithm type. When two layers of hidden neurons are used, precision is not improved while computation time is increased. The number of samples depends on the complexity of the problem and the number of input and output parameters. If a value is fixed for the input costs, there are no noticeable precision improvements between training the network with 200 or 1000 samples. When costs are introduced as input parameters, 100 samples are not enough to achieve convergence in training. Finally, the training algorithm is also checked, studying the range between pure backpropagation (too slow for training), backpropagation with momentum and with adaptive learning, backpropagation with Levenberg–Marquardt updating rule and fast learning backpropagation. The latter is finally retained since it requires less time to get the network to converge while providing very good results (Demuth, H. & Beale, M.,1995)\\nApplication of Artificial Neural Networks to Prediction of Concrete Physical  Parameters Measurable Through  Testing: Concrete Strength and  Consistency\\nOther neural network applications are supported by large experimental databases, created through years of research, which allow for the prediction of phenomena with complex analytical formulation.\\nOne of these cases is the determination of two basic \\nconcrete parameters: its workability when mixed, necessary for ease of placement in concrete, and its compressive strength once hardened, which is basic to the evaluation of the capacity of the structure. The variables that necessarily determine these two parameters are the components of concrete: amounts of cement, water, fine aggregate (sand), coarse aggregate (small gravel and large gravel), and other components such as pozzolanic additions (which bring soundness ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a29879b-a5b2-474c-aaf9-950b22e48dcc', embedding=None, metadata={'page_label': '156', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN Application in the Field of Structural Concrete\\nAand delayed strength increase, especially in the case of \\nfly ash and silica fume) and admixtures (which fluidify the fresh mixture allowing the use of reduced amounts of water). There are still no analytical or numerical models that faithfully predict fresh concrete consistency (related to flowability, and usually evaluated by the slump of a molded concrete cone) or compressive strength (determined by crushing of prismatic specimens in a press).\\nÖztaş et al. (Öztaş, Pala, Özbay, Kanca, Çağlar & \\nBatí, 2006) have developed a neural network from 187 concrete mixes, for which all parameters are know, using 169 of them for training and 18, randomly selected, for verification. Database variables are sometimes taken as a ratio between them, since there is available knowledge about the dependency of slump and strength on such parameters.  The established range for the 7 parameter set is shown in Table 1.\\nNetwork architecture, as determined by 7 input \\nneurons and two hidden layers of 5 and 3 neurons respectively.\\nThe back-propagation learning algorithm has been \\nused in feed-forward two hidden-layers. The learning algorithm used in the study is scaled conjugate gradients algorithm (SCGA), activation function is sigmoidal function, and number of epochs is 10,000. The prediction capacity of the network is better in the “Compressive Strength” output (maximum error of 6%) than in the “Slump” output (errors up to 25%). This is due to the fact that the relation between the chosen variables and strength is much stronger than in the case of slump, which is influenced by other non-contemplated variables (e. g. type and power of concrete mixer, mixing order of components, aggregate moisture) and the method for measurement of consistency, whose adequacy for the particular type of concrete used in the database is questioned by some authors.\\nApplication of Artificial Neural Networks to the Development of Design Formulae and Codes\\nThe last application presented in this paper is the response analysis to shear forces in concrete beams. These forces generate transverse tensile stresses in concrete beams which require placement of rebars perpendicular to the beam axis, known as hoops or ties. Analytical determination of failure load from the variables that intervene in this problem is very complex, and in general most of the formulae used today are based on experimental interpolations with no dimensional consistency. Cladera and Marí (Cladera & Marí, 2004) have studied the problem through laboratory testing, developing a neural network for the strength analysis of beams with no shear reinforcement. They rely on a database compiled by Bentz (Bentz, 2000) and Kuchma (Kuchma, 2002), where the variables are effective depth (d), beam width ( b, though introduced as d/b), shear \\nspan (a/d, see Figure 1), longitudinal reinforcement ratio (ρ\\nl = As/bd) and compressive strength of concrete \\n(fc). Of course, failure load is provided for each of \\nthe 177 tests found in the database. They use 147 tests to train the network and 30 for verification, on a one layer architecture with 10 hidden neurons and a retropropagation learning mechanism. The ranges Input parameters Minimum Maximum\\nW/B (ratio, %)a\\x18\\x18 \\x18\\x18\\nW (kg/m\\x18)b\\x18\\x180 \\x18\\x18\\x18\\ns/a (ratio, %)c\\x18\\x18 \\x18\\x18\\nFA (ratio, %)d0 \\x180\\nAE (kg/m\\x18)e0.0\\x18\\x18 0.0\\x18\\x18\\nSF (ratio, %)f\\x18 \\x18\\x18\\nSP (kg/m\\x18)g\\x18.\\x18\\x18 \\x18\\x18.\\x18\\n(a) [Water]/[binder] ratio, considering binder as the lump sum of \\ncement, fly ash and silica fume(b) Amount of water(c) [Amount of sand]/[Total aggregate (sand+small gravel+large gravel)](d) Percentage of cement substituted by fly ash(e) Amount of air-entraining agent(f) Percentage of cement substituted by silica fume(g) Amount of superplasticizerTable 1. Input parameter range\\nParameter Minimum Maximum\\nd(mm) \\x180\\x18.\\x18 \\x180\\x180\\nd/b 0.\\x18\\x18 \\x18.\\x18\\x18\\nρℓ (%) 0.\\x180 \\x18.\\x18\\x18\\nfc(MPa) \\x18\\x18.\\x18 \\x180\\x18.\\x18\\na/d \\x18.\\x18\\x18 \\x18.\\x18\\x18\\nVfail(kN) \\x18\\x18.\\x18\\x18 \\x18\\x18\\x18.\\x18\\x18Table 2 Input parameter ranges', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd19118f-5297-49f0-b5db-fa4120fb75fe', embedding=None, metadata={'page_label': '157', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  ANN Application in the Field of Structural Concrete\\nfor the variables are shown on Table 2. Almost 8000 \\niterations were required to attain best results. \\nThe adjustment provided by training presents an \\naverage ratio Vtest/Vpred of 0.99, and 1.02 in validation. \\nThe authors have effectively created a laboratory with a neural network, in which they “test” (within parameter range) new beams by changing exclusively one parameter each time. Finally, they come up with two alternative design formulae that improve noticeably any given formula developed up to that moment. Table 3 presents a comparison between those two expressions (named Eq. 7 and Eq. 8) and others found in a series of international codes.\\nCONCLUSION\\nThe field of structural concrete shows great potential for the application of neural networks. Successful approaches to optimization, prediction of complex physical parameters and design formulae development have been presented.The network topology used in most cases for structural concrete is forward-feed, multilayer with backpropagation, typically with one or two hidden •\\n•layers. The most commonly used training algorithms \\nare descent gradient with momentum and adaptive learning, and Levenberg-Marquardt.The biggest potential of ANNs is their capacity to generate virtual testing laboratories which substitute with precision expensive real laboratory tests within the proper range of values. A methodical “testing” program throws light on the influence of the different variables in complex phenomena at reduced cost.The field of structural concrete counts upon extensive databases, generated through the years, that can be analyzed with this technique. An effort should be made to compile and homogenize these databases to extract the maximum possible knowledge, which has great influence on structural safety.\\nACKNOWLEDGMENT\\nThis work was partially supported by the Spanish Ministry of Education and Science (Ministerio de Educación y Ciencia) (Ref BIA2005-09412-C03-01), grants (Ref. 111/2006/2-3.2) funded by the Spanish •\\n•Figure 1. Span loading a of a beam. (González, 2002)\\nProcedure ACI \\x18\\x18-\\x18 ACI \\x18\\x18-\\x18 MC-\\x180 EC-\\x18 AASHTO Eq.(\\x18) Eq.(\\x18)\\nAverage \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.0\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18\\nMedian \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 0.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18\\nS t a n d a r d \\ndeviation0.\\x18\\x18 0.\\x180 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18\\nCoV (%) \\x18\\x18.\\x18\\x18 \\x18\\x18.\\x18\\x18 \\x18\\x18.\\x18\\x18 \\x18\\x18.0\\x18 \\x18\\x18.\\x180 \\x18\\x18.\\x18\\x18 \\x18\\x18.\\x18\\x18Minimum 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18 0.\\x18\\x18\\nMaximum \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18 \\x18.\\x18\\x18Table 3. Comparison between available codes and proposed equations for shear strength.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3404ed0e-aace-4f1d-889f-6ab8a0405c21', embedding=None, metadata={'page_label': '158', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN Application in the Field of Structural Concrete\\nAMinistry of Enviroment ( Ministerio de Medio ambiente) \\nand grants from the General Directorate of Research, Development and  Innovation (Dirección Xeral de Investigación, Desenvolvemento e Innovación) of the Xunta de Galicia (Ref. PGIDT06PXIC118137PN).   The work of Juan L. Pérez is supported by an FPI grant (Ref. BES-2006-13535) from the Spanish Ministry of Education and Science (Ministerio de Educación y Ciencia). \\nREFERENCES\\nAbrams, D.A. (1922). Proportion Concrete Mixtures. Proceedings of the American Concrete Institute,  174-181.\\nBentz, EC. (2000). Sectional analysis of reinforced \\nconcrete members. PhD thesis, Department of Civil Engineering, University of Toronto.\\nBrown, M. & Harris, C. (1994). Neurofuzzy adaptive \\nmodelling and control. Prentice-Hall.\\nCladera, A. & Marí, A.R. (2004). Shear design procedure \\nfor reinforced normal and high-strength concrete beams using artificial neural networks. Part I: beams without stirrups. Engineering Structures (26) 917–926\\nDemuth, H. & Beale, M. (1995). Neural network toolbox \\nfor use with MATLAB. MA: The Mathworks, Inc.\\nDomone, P.(1998). The Slump Flow Test for High-\\nWorkability Concrete. Cement and Concrete Research  (28-2), 177-182. \\nGonzález B. (2002). Hormigones con áridos reciclados \\nprocedentes de demoliciones: dosificaciones, propiedades mecánicas y comportamiento estructural a cortante. PhD thesis, Department of Construction Technology, University of A Coruña.\\nGonzález, B. Martínez, I. and Carro, D. (2006). \\nPrediction of the consistency of concrete by means of the use of ANN. Artificial Neural Networks in Real-Life Applications.Ed. Idea Group Inc. 188-200\\nHadi, M (2003). Neural networks applications in \\nconcrete structures. Computers and Structures (81) 373–381\\nHoskins, J.C. & Himmelblau, D.M.(1992). Process \\ncontrol via artificial neural networks and reinforcement learning. Computers and Chemical Engineering, vol. 16(4). 241-251.\\nKuchma D. (1999-2002) Shear data bank. University \\nof Illinois, Urbana-Champaign.\\nLin, C.T. & Lee, C.S.(1996). Neural Fuzzy Systems: \\nA neuro-fuzzy synergism to intelligent systems. Prentice-Hall.\\nMcCulloch, W. S. & Pitts, W. (1943). A Logical Calculus \\nof Ideas Immanent in Nervous Activity. Bulletin of Mathematical Biophysics. (5). 115-133.\\nÖztaş, A. Pala, M. Özbay E. Kanca E. Çağlar N. & Bhatti \\nM.A. (2006) Predicting the compressive strength and slump of high strength concrete using neural network. Construction and Building Materials. (20). 769–775.\\nShah, SP. (1993). Recent trends in the science and \\ntechnology of concrete, concrete technology, new trends, industrial applications. Proceedings of the international RILEM workshop, London, E & FN Spon. 1–18.\\nWasserman, P. (1989) Neural Computing, Ed. Van \\nNostrand Reinhold, New York.\\nKEy TERMS\\nCompression:  Stress generated by pressing or \\nsqueezing.\\nConsistency: The relative mobility or ability of \\nfreshly mixed concrete or mortar to flow; the usual measurement for concrete is slump , equal to the \\nsubsidence measured to the nearest 1/4 in. (6 mm) of a molded specimen immediately after removal of the slump cone.\\nDuctility: That property of a material by virtue of \\nwhich it may undergo large permanent deformation without rupture.\\nFormwork: Total system of support for freshly \\nplaced concrete including the mold or sheathing that contacts the concrete as well as supporting members, hardware, and necessary bracing; sometimes called shuttering in the UK.\\nShear Span: Distance between a reaction and the \\nnearest load point.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7674a273-8a54-4787-b35b-913c8af23ec3', embedding=None, metadata={'page_label': '159', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  ANN Application in the Field of Structural Concrete\\nStructural Safety:  Structural response stronger than \\nthe internal forces produced by external loading.\\nTension: Stress generated by stretching.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5eafa45a-78e9-4525-99a8-4ea419d220ef', embedding=None, metadata={'page_label': '160', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nAANN Development with EC Tools: \\nAn Overview\\nDaniel Rivero\\nUniversity of A Coruña, Spain\\nJuan Rabuñal\\nUniversity of A Coruña, Spain\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nAmong all of the Artificial Intelligence techniques, \\nArtificial Neural Networks (ANNs) have shown to be a very powerful tool (McCulloch & Pitts, 1943) (Haykin, 1999). This technique is very versatile and therefore has been succesfully applied to many different disciplines (classification, clustering, regression, modellization, etc.) (Rabuñal & Dorado, 2005).\\nHowever, one of the greatest problems when using \\nANNs is the great manual effort that has to be done in their development. A big myth of ANNs is that they are easy to work with and their development is almost automatically done. This development process can be divided into two parts: architecture development and training and validation. As the network architecture is problem-dependant, the design process of this architec-ture used to be manually performed, meaning that the expert had to test different architectures and train them until finding the one that achieved best results after the training process. The manual nature of the described process determines its slow performance although the training part is completely automated due to the exis-tence of several algorithms that perform this part.\\nWith the creation of Evolutionary Computation \\n(EC) tools, researchers have worked on the application of these techniques to the development of algorithms for automatically creating and training ANNs so the whole process (or, at least, a great part of it) can be automatically performed by computers and therefore few human efforts has to be done in this process.\\nBACKGROUND\\nEC is a set of tools based on the imitation of the natural behaviour of the living beings for solving optimization problems. One of the most typical subset of tools inside EC is called Evolutionary Algorithms (EAs), which are based on natural evolution and its implementation on computers. All of these tools work with the same basis: a population of solutions to that particular problem is randomly created and an evolutionary process is applied to it. From this initial random population, the evolution is done by means of selection and combination of the best individuals (although the worst ones also have a small probability of being chosen) to create new solutions. This process is carried out by selection, crossover, and mutation operators. These operators are typically used in biology in its evolution for adaptation and survival. After several generations, it is hoped that the population contains a good solution to the problem.\\nThe first EA to appear was Genetic Algorithms \\n(GAs), in 1975 (Holland, 1975). With the working explained above, GAs use a binary codification (i.e., each solution is codified into a string of bits). Later, in the early 90s a new technique appeared, called Genetic Programming (GP). This one is based ob the evolution of trees, i.e., each individual is codified as a tree instead of a binary string. This allows its application to a wider set of environments.\\nAlthough GAs and GP are the two most used tech-\\nniques in EAs, more tools can be classified as part of this world, such as Evolutionary Programming or Evolution Strategies, all of them with the same basis: the evolution of a population following the natural evolution rules.\\nDEVELOPMENT OF ANNS WITH EC TOOLS\\nThe development of ANNs is a topic that has been extensively dealt with very diverse techniques. The world of evolutionary algorithms is not an exception, and proof of that is the great amount of works that have ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8de0c330-5a54-46ee-827b-1655640e39be', embedding=None, metadata={'page_label': '161', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  ANN Development with EC Tools\\nbeen published about different techniques in this area \\n(Cantú-Paz & Kamath, 2005). These techniques follow the general strategy of an evolutionary algorithm: an initial population consisting of different genotypes, each one of them codifying different parameters (typically, the weight of the connections and / or the architecture of the network and / or the learning rules), and is ran-domly created. This population is evaluated in order to determine the fitness of each individual. Afterwards, this population is repeatedly made to evolve by means of different genetic operators (replication, crossover, mutation, etc.) until a determined termination criteria is fulfilled (for example, a sufficiently good individual is obtained, or a predetermined maximum number of generations is achieved).\\nEssentially, the ANN generation process by means \\nof evolutionary algorithms is divided into three main groups: evolution of the weights, architectures, and learning rules.\\nEvolution of Weights\\nThe evolution of the weights begins with a network with a predetermined topology.  In this case, the problem is to establish, by means of training, the values of the network connection weights. This is generally conceived as a problem of minimization of the network error, taken, for example, as the result of the Mean Square Error of the network between the desired outputs and the ones achieved by the network. Most the training algorithms, such as the backpropagation algorithm (BP) (Rumel-hart, Hinton & Williams, 1986), are based on gradient minimization. This has several drawbacks (Whitley, Starkweather & Bogart, 1990), the most important is that quite frequently the algorithm becomes stuck in a local minimum of the error function and is unable of finding the global minimum, especially if the error function is multimodal and / or non-differentiable. One way of overcoming these problems is to carry out the training by means of an Evolutionary Algorithm (Whitley, Starkweather & Bogart, 1990); i.e., formulate the training process as the evolution of the weights in an environment defined by the network architecture and the task to be done (the problem to be solved). In these cases, the weights can be represented in the individuals’ genetic material as a string of binary values (Whitley, Starkweather & Bogart, 1990) or a string of real numbers (Greenwood, 1997). Traditional genetic algorithms (Holland, 1975) use a genotypic codification method with the shape of binary strings. In this way, much work has emerged that codifies the values of the weights by means of a concatenation of the binary values which represent them (Whitley, Starkweather & Bogart, 1990). The big advantage of these approximations is their generality and that they are very simple to apply, i.e., it is very easy and quick to apply the operators of uniform crossover and mutation on a binary string. The disadvantage of using this type of codification is the problem of permutation. This problem was raised upon considering that the order in which the weights are taken in the string causes equivalent networks to possibly correspond with totally different individuals. This leads the crossing operator to become very inef-ficient. Logically, the weight value codification has also emerged in the form of real number concatenation, each one of them associated with a determined weight (Greenwood 1997). By means of genetic operators designed to work with this type of codification, and given that the existing ones for bit string cannot be used here, several studies (Montana & Davis, 1989) showed that this type of codification produces better results and with more efficiency and scalability than the BP algorithm.\\nEvolution of the Architectures\\nThe evolution of the architectures includes the genera-tion of the topological structure; i.e., the topology and connectivity of the neurons, and the transfer function of each neuron of the network. The architecture of a network has a great importance in order to success-fully apply the ANNs, as the architecture has a very significant impact on the process capacity of the net-work. In this way, on one hand, a network with few connections and a lineal transfer function may not be able to resolve a problem that another network hav-ing other characteristics (distinct number of neurons, connections or types of functions) would be able to resolve. On the other hand, a network having a high number of non-lineal connections and nodes could be overfitted and learn the noise which is present in the training as an inherent part of it, without being able to discriminate between them, and in the end, not have a good generalization capacity. Therefore, the design of a network is crucial, and this task is classically carried out by human experts using their own experience, based on “trial and error”, experimenting with a different set of architectures. The evolution of architectures has ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='26b0ef08-a9fd-4fe1-9a30-d8adb9dd11da', embedding=None, metadata={'page_label': '162', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN Development with EC Tools\\nAbeen possible thanks to the appearance of constructive \\nand destructive algorithms (Sietsma & Dow, 1991). In general terms, a constructive algorithm begins with a minimum network (with a small number of layers, neurons and connections) and successively adds new layers, nodes and connections, if they are necessary, during the training. A destructive algorithm carries out the opposite operation, i.e., it begins with a maximum network and eliminates unnecessary nodes and con-nections during the training. However, the methods based on Hill Climbing algorithms are quite susceptible into falling to a local minimum (Angeline, Suders & Pollack, 1994).\\nIn order to develop ANN architectures by means \\nof an evolutionary algorithm, it is necessary to decide how to codify a network inside the genotype so it can be used by the genetic operators. For this, different types of network codifications have emerged.\\nIn the first codification method, direct codification, \\nthere is a one-to-one correspondence between the genes and the phenotypic representation (Miller, Todd & Hedge, 1989). The most typical codification method consists of a matrix C=(c\\nij) of NxN size which repre-\\nsents an architecture of N nodes, where cij indicates the \\npresence or absence of a connection between the i and j nodes.  It is possible to use c\\nij=1 to indicate a connec-\\ntion and cij=0 to indicate an absence of connection. In \\nfact, cij could take real values instead of Booleans to \\nrepresent the value of the connection weight between neuron “i” and “j”, and in this way, architecture and connections can be developed simultaneously (Alba, Aldana & Troya, 1993). The restrictions which are required in the architectures can easily be incorporated into this representational scheme. For example, a feed-forward network would have non-zero coefficients only in the upper right hand triangle of the matrix. These types of codification are generally very simple and easy to implement. However, they have a lot of disadvantages, such as scalability, the impossibility of codifying repeated structures, or permutation (i.e., different networks which are functionally equivalent can correspond with different genotypes) (Yao & Liu, 1998).\\nAs a counterproposal to this type of direct codifi-\\ncation method, there are also the indirect codification types in existence. With the objective of reducing the length of the genotypes, only some of the characteristics of the architecture are codified into the chromosome. Within this type of codification, there are various types of representation.First, the parametric representations have to be \\nmentioned. The network can be represented by a set of parameters such as the number of hidden layers, the number of connections between two layers, etc. There are several ways of codifying these parameters inside the chromosome (Harp, Samad & Guha, 1989). Although the parametric representations can reduce the length of the chromosome, the evolutionary algorithm makes a search in a limited space within the possible searchable space that represents all the possible ar-chitectures. Another type of non-direct codification is based on a representational system with the shape of grammatical rules (Yao & Shi, 1995). In this system, the network is represented by a set of rules, with shape of production rules, which will build a matrix that represents the network.\\n Other types of codification, more inspired in the \\nworld of biology, are the ones known as “growing methods”. With them, the genotype does not codify the network any longer, but instead it contains a set of instructions. The decodification of the genotype con-sists of the execution of these instructions, which will provoke the construction of the phenotype (Husbands, Harvey, Cliff & Miller, 1994). These instructions usu-ally include neural migrations, neuronal duplication or transformation, and neuronal differentiation. \\nFinally, and within the indirect codification meth -\\nods, there are other methods which are very different from the ones already described. Andersen describes a technique in which each individual of a population represents a hidden node instead of the architecture (Andersen & Tsoi, 1993). Each hidden layer is con-structed automatically by means of an evolutionary process which uses a genetic algorithm. This method has the limitation that only feed-forward networks can be constructed and there is also a tendency for various nodes with a similar functionality to emerge, which inserts some redundancy inside the network that must be eliminated.\\nOne important characteristic is that, in general, \\nthese methods only develop architectures, which is the most common, or else architectures and weights together. The transfer function of each architecture node is assumed to have been previously determined by a human expert, and that it is the same for all of the network nodes (at least, for all of the nodes of the same layer), although the transfer function has been shown to have a great importance on the behaviour of the network (Lovell & Tsoi, 1992). Few methods have ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99490e53-0f53-49ed-83bd-aeb9dd10a904', embedding=None, metadata={'page_label': '163', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  ANN Development with EC Tools\\nbeen developed which cause the transfer function to \\nevolve, and, therefore, had little repercussion in the world of ANNs with EC.\\nEvolution of the Learning Rule\\nAnother interesting approximation to the development of ANNs by means of EC is the evolution of the learning rule. This idea emerges because a training algorithm works differently when it is applied to networks with different architectures. In fact, and given that a priori, the expert usually has very few knowledge about a network, it is preferable to develop an automatic system to adapt the learning rule to the architecture and the problem to be resolved.\\nThere are several approximations to the evolution \\nof the learning rule (Crosher, 1993) (Turney, Whitley & Anderson, 1996), although most of them are based only on how the learning can modify or guide the evo-lution, and in the relation between the architecture and the connection weights. Actually, there are few works that focus on the evolution of the learning rule in itself (Bengio & Bengio, Cloutier & Gecsei, 1992) (Ribert, Stocker, Lecourtier & Ennaji, 1994).\\nOne of the most common approaches is based on \\nsetting the parameters of the BP algorithm: learning rate and momentum. Some authors propose methods in which an evolutionary process is used to find these parameters while leaving the architecture constant (Kim, Jung, Kim & Park, 1996). Other authors, on the other hand, propose codifying these BP algorithm parameters together with the network architecture inside of the individuals of the population (Harp, Samad & Guha, 1989).\\nFUTURE TRENDS\\nThe evolution of ANNs has been a research topic since some decades ago. The creation of new EC and, in general, new AI techniques and the evolution and improvement of the existing ones allow the develop-ment of new methods of automatically developing of ANNs. Although there are methods that (more or less) automatically develop ANNs, they are usually not very efficient, since evolution of architectures, weights and learning rules at once leads to having a very big search space, so this feature definitely has to be improved.CONCLUSION \\nThe world of EC has provided a set of tools that can be applied to optimization problems. In this case, the problem is to find an optimal architecture and/or weight value set and/or learning rule. Therefore, the develop-ment of ANNs was converted into an optimization problem. As the described techniques show, the use of EC techniques has made possible the development of ANNs without human intervention, or, at least, mini-mising the participation of the expert in this task.\\nAs has been explained, these techniques have \\nsome problems. One of them is the already explained permutation problem. Another problem is the loss of efficiency: the more complicated the structure to evolve is (weigths, learning rule, architecture), less efficient the system will be, because the search space becomes much bigger. If the system has to evolve several things at a time (for example, architecture and weights so the ANN development is completely automated), this loss of efficiency increases. However, these systems still work faster than the whole manual process of designing and training several times an ANN.\\nREFERENCES\\nAlba E., Aldana J.F. & Troya J.M. (1993) Fully au-tomatic ANN design: A genetic approach. Proc. Int. \\nWorkshop Artificial Neural Networks (IWANN’93), Lecture Notes in Computer Science . (686) 399-404.\\nAndersen H.C. & Tsoi A.C. (1993) A constructive algorithm for the training of a multilayer perceptron based on the genetic algorithm. Complex systems 7 \\n(4) 249-268.\\nAngeline P.J., Suders G.M. & Pollack J.B. (1994) An \\nevolutionary algorithm that constructs recurrent neural networks. IEEE Trans. Neural Networks. (5) 54-65.\\nBengio S., Bengio Y ., Cloutier J. & Gecsei J. (1992) \\nOn the optimization of a synaptic learning rule. Pre-\\nprints of the Conference on Optimality in Artificial and Biological Neural Networks .\\nCantú-Paz E. & Kamath C. (2005) An Empirical Com-parison of Combinatios of Evolutionary Algorithms and Neural Networks for Classification Problems. IEEE \\nTransactions on systems, Man and Cybernetics – Part B: Cybernetics . 915-927.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='08d578c9-8982-4ce4-9a3b-fcaeedd3cbc5', embedding=None, metadata={'page_label': '164', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN Development with EC Tools\\nACrosher D. (1993) The artificial evolution of a gener -\\nalized class of adaptive processes. Preprints of AI’93 \\nWorkshop on Evolutionary Computation . 18-36.\\nGreenwood G.W. (1997) Training partially recurrent \\nneural networks using evolutionary strategies. IEEE \\nTrans. Speech Audio Processing. (5) 192-194.\\nHarp S.A., Samad T. & Guha A. (1989) Toward the \\ngenetic synthesis of neural networks. Proc. 3rd Int. \\nConf. Genetic Algorithms and Their Applications . \\n360-369.\\nHaykin, S. (1999). Neural Networks (2nd ed.) . Engle-\\nwood Cliffs, NJ: Prentice Hall.Holland, J.J. (1975) Adaptation in natural and artifi-\\ncial systems. Ann Arbor, MI: University of Michigan \\nPress.\\nHusbands P., Harvey I., Cliff D. & Miller G. (1994) \\nThe use of genetic algorithms for the development of sensorimotor control systems. From Perception to Ac-\\ntion. (P. Gaussier and JD Nicoud, eds.). Los alamitos CA: IEEE Press.\\nKim H., Jung S., Kim T. & Park K. (1996) Fast learning \\nmethod for backpropagation neural network by evolu-tionary adaptation of learning rates. Neurocomputing, \\n11(1) 101-106.\\nLovell D.R. & Tsoi A.C. (2002) The Performance of the \\nNeocognitron with various S-Cell and C-Cell Transfer \\nFunctions, Intell. Machines Lab., Dep. Elect. Eng., Univ. Queensland, Tech. Rep.\\nMcCulloch W.S., & Pitts, W. (1943) A Logical Calculus \\nof Ideas Immanent in Nervous Activity. Bulletin of \\nMathematical Biophysics . (5) 115-133.\\nMiller G.F., Todd P.M. & Hedge S.U. (1989) Designing neural networks using genetic algorithms. Proceed-\\nings of the Third International Conference on Genetic algorithms. San Mateo, CA: Morgan Kaufmann. 379-384.\\nMontana D. & David L. (1989) Training feed-forward \\nneural networks using genetic algorithms. Proc. 11th \\nInt. Joint Conf. Artificial Intelligence . San Mateo, CA: \\nMorgan Kaufmann. 762-767.\\nRabuñal, J.R. & Dorado J. (2005) Artificial Neural \\nNetworks in Real-Life Applications . Idea Group Inc.Ribert A., Stocker E., Lecourtier Y . & Ennaji A. (1994) \\nOptimizing a Neural Network Architecture with an Adaptive Parameter Genetic Algorithm. Lecture \\nNotes in Computer Science. Springer-Verlag. (1240) 527-535.\\nRumelhart D.E., Hinton G.E. & Williams R.J. (1986) \\nLearning internal representations by error propaga-tion. Parallel Distributed Processing: Explorations \\nin the Microstructures of Cognition . D. E. Rumelhart \\n& J.L. McClelland, Eds. Cambridge, MA: MIT Press. (1) 318-362.\\nSietsma J. & Dow R. J. F. (1991) Creating Artificial \\nNeural Networks that generalize. Neural Networks. \\n(4) 1: 67-79.\\nTurney P., Whitley D. & Anderson R. (1996) Special \\nissue on the baldwinian effect. Evolutionary Computa-\\ntion. 4(3) 213-329.\\nWhitley D., Starkweather T. & Bogart C. (1990) \\nGenetic algorithms and neural networks: Optimizing connections and connectivity. Parallel Comput., V ol. \\n14, No 3. 347-361.\\nYao X. & Shi Y . (1995) A preliminary study on design-\\ning artificial neural networks using co-evolution. Proc. \\nIEEE Singapore Int. Conf. Intelligence Control and Instrumentation. 149-154.\\nYao X. & Liu Y . (1998) Toward designing artificial \\nneural networks by evolution. Appl. Math. Computa-\\ntion. vol. 91, no. 1, 83-90.\\nKEy TERMS\\nArtificial Neural Networks: Interconnected set \\nof many simple processing units, commonly called neurons, that use a mathematical model, that represents an input/output relation,\\nBack-Propagation Algorithm:  Supervised learn-\\ning technique used by ANNs, that iteratively modifies the weights of the connections of the network so the error given by the network after the comparison of the outputs with the desired one decreases.\\nEvolutionary Computation: Set of Artificial In-\\ntelligence techniques used in optimization problems, which are inspired in biologic mechanisms such as natural evolution.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='800a168d-2092-47fa-80aa-a476e035db9a', embedding=None, metadata={'page_label': '165', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x180  ANN Development with EC Tools\\nGenetic Programming: Machine learning tech-\\nnique that uses an evolutionary algorithm in order to \\noptimise the population of computer programs accord-ing to a fitness function which determines the capability of a program for performing a given task.\\nGenotype: The representation of an individual on \\nan entire collection of genes which the crossover and mutation operators are applied to.\\nPhenotype: Expression of the properties coded by \\nthe individual’s genotype.\\nPopulation: Pool of individuals exhibiting equal or \\nsimilar genome structures, which allows the application of genetic operators.\\nSearch Space: Set of all possible situations of the \\nproblem that we want to solve could ever be in.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c28a1c77-aa6d-4ca2-ad53-8eb0a60c9579', embedding=None, metadata={'page_label': '166', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nAANN-Based Defects’ Diagnosis of Industrial\\nOptical Devices\\nMatthieu Voiry\\nUniversity of Paris, FranceSAGEM REOSC, France\\nVéronique Amarger\\nUniversity of Paris, France\\nJoel Bernier\\nSAGEM REOSC, France\\nKurosh Madani\\nUniversity of Paris, France\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nA major step for high-quality optical devices faults diagnosis concerns scratches and digs defects detec-tion and characterization in products. These kinds of aesthetic flaws, shaped during different manufacturing steps, could provoke harmful effects on optical devices’ functional specificities, as well as on their optical per -\\nformances by generating undesirable scatter light, which could seriously damage the expected optical features. A reliable diagnosis of these defects becomes therefore a crucial task to ensure products’ nominal specifica-tion. Moreover, such diagnosis is strongly motivated by manufacturing process correction requirements in order to guarantee mass production quality with the aim of maintaining acceptable production yield. \\nUnfortunately, detecting and measuring such defects \\nis still a challenging problem in production conditions and the few available automatic control solutions remain ineffective. That’s why, in most of cases, the diagnosis is performed on the basis of a human expert based visual inspection of the whole production. However, this conventionally used solution suffers from several acute restrictions related to human operator’s intrinsic limitations (reduced sensitivity for very small defects, detection exhaustiveness alteration due to attentiveness shrinkage, operator’s tiredness and weariness due to repetitive nature of fault detection and fault diagnosis tasks).\\nTo construct an effective automatic diagnosis \\nsystem, we propose an approach based on four main operations: defect detection, data extraction, dimen-sionality reduction and neural classification. The first operation is based on Nomarski microscopy issued imaging. These issued images contain several items which have to be detected and then classified in order to discriminate between “false” defects (correctable defects) and “abiding” (permanent) ones. Indeed, because of industrial environment, a number of cor-rectable defects (like dusts or cleaning marks) are usually present beside the potential “abiding” defects. Relevant features extraction is a key issue to ensure accuracy of neural classification system; first because raw data (images) cannot be exploited and, moreover, because dealing with high dimensional data could affect learning performances of neural network. This article presents the automatic diagnosis system, describing the operations of the different phases. An implementation on real industrial optical devices is carried out and an experiment investigates a MLP artificial neural network based items classification.  \\nBACKGROUND\\n \\nToday, the only solution which exists to detect and classify optical surfaces’ defects is a visual one, carried out by a human expert. The first originality of this work is in the sensor used: Normarski microscopy. Three main advantages distinguishing Nomarski microscopy (known also as “Differential Interference Contrast microscopy” (Bouchareine, 1999) (Chatterjee, 2003)) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='953fea32-2d7e-4461-a9f8-dbfa4a633a44', embedding=None, metadata={'page_label': '167', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  ANN-Based Defects’ Diagnosis of Industrial Optical Devices\\nfrom other microscopy techniques, have motivated our \\npreference for this imaging technique. The first of them is related to the higher sensitivity of this technique comparing to the other classical microscopy techniques (Dark Field, Bright Field) (Flewitt & Wild, 1994). Furthermore, the DIC microscopy is robust regarding lighting non-homogeneity. Finally, this technology provides information relative to depth (3-th dimen-sion) which could be exploited to typify roughness or defect’s depth. This last advantage offers precious ad-ditional potentiality to characterize scratches and digs flaws in high-tech optical devices. Therefore, Nomarski microscopy seems to be a suitable technique to detect surface imperfections.\\n On the other hand, since they have shown many \\nattractive features in complex pattern recognition and classification tasks (Zhang, 2000) (Egmont-Petersen, de Ridder, & Handels, 2002), artificial neural network based techniques are used to solve difficult problems. In our particular case, the problem is related to the classification of small defects on a great observation’s surface. These promising techniques could however encounter difficulties when dealing with high dimen -\\nsional data. That’s why we are also interested in data dimensionality reducing methods. \\nDEFECTS’ DETECTION AND CLASSIFICATION\\n \\nThe suggested diagnosis process is described in broad outline in the diagram of Figure 1. Every step is pre-sented, first detection and data extraction phases and then classification phase coupled with dimensionality reduction. In a second part, some investigations on real industrial data are carried out and the obtained results are presented.\\nDetection and Data Extraction\\nThe aim of defect’s detection stage is to extract defects images from DIC detector issued digital image. The proposed method  (V oiry, Houbre, Amarger, & Madani, 2005) includes four phases:\\n• Pre-processing: DIC issued digital image trans-\\nformation in order to reduce lighting heterogene-ity influence and to enhance the aimed defects’ visibility, \\n• Adaptive matching: adaptive process to match defects,\\n• Filtering and segmentation: noise removal and defects’ outlines characterization.\\n• Defect image extraction: correct defect represen-tation construction.\\nFinally, the image associated to a given detected \\ngives an isolated (from other items) representation of the defect (e.g. depicts the defect in its immediate environment), like depicted in Figure 2.\\nBut, information contained in such generated \\nimages is highly redundant and these images don’t have necessarily the same dimension (typically this dimension can turn out to be hundred times as high). That is why this raw data (images) can not be directly processed and has first to be appropriately encoded, using some transformations. Such ones must naturally be invariant with regard to geometric transformations (translation, rotation and scaling) and robust regarding different perturbations (noise, luminance variation and background variation). Fourier-Mellin transformation is used as it provides invariant descriptors, which are considered to have good coding capacity in classifica -\\ntion tasks (Choksuriwong, Laurent, & Emile, 2005) (Derrode, 1999) (Ghorbel, 1994). Finally, the processed features have to be normalized, using the centring-re-ducing transformation. Providing a set of 13 features using such transform, is a first acceptable compromise between industrial environment real-time processing constraints and defect image representation quality (V oiry, Madani, Amarger, & Houbre, 2006).\\nFigure 1. Block diagram of the proposed defect diagnosis system', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0247b67-f884-4060-b3bc-a90b6184e97b', embedding=None, metadata={'page_label': '168', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN-Based Defects’ Diagnosis of Industrial Optical Devices\\nA\\nDimensionality Reduction \\nTo obtain a correct description of defects, we must \\nconsider more or less important number of Fourier-Mel-lin invariants. But dealing with high-dimensional data poses problems, known as “curse of dimensionality” (Verleysen, 2001). First, sample number required to reach a predefined level of precision in approximation tasks increases exponentially with dimension. Thus, intuitively, the sample number needed to properly learn problem becomes quickly much too large to be collected by real systems, when dimension of data increases. Moreover surprising phenomena appear when working in high dimension (Demartines, 1994): for example, variance of distances between vectors remains fixed while its average increases with the space dimension, and Gaussian kernel local properties are also lost. These last points explain that behaviour of a number of artificial neural network algorithms could be affected while dealing with high-dimensional data. Fortunately, most real-world problem data are located in a manifold of dimension p (the data intrinsic dimen-sion) much smaller than its raw dimension. Reducing data dimensionality to this smaller value can therefore decrease the problems related to high dimension.\\n In order to reduce the problem dimensionality, we \\nuse Curvilinear Distance Analysis (CDA). This tech-nique is related to Curvilinear Component Analysis (CCA), whose goal is to reproduce the topology of a n-dimension original space in a new p-dimension space (where p<n) without fixing any configuration of the topology (Demartines & Hérault, 1993). To do so, a criterion characterizing the differences between original and projected space topologies is processed:∑∑\\n≠− =\\ni i jp\\ni jp\\ni jn\\ni j CCA d F d d E ) ( ) (212\\n (1)\\nWhere n\\ni jd(respectivelyp\\ni jd) is the Euclidean distance \\nbetween vectors xi and xj of considered distribution in \\noriginal space (resp. in projected space), and F is a decreasing function which favours local topology with respect to the global topology. This energy function is minimized by stochastic gradient descent (Demartines & Hérault, 1995):\\n), )( ) ( ( ) ( ,p\\njp\\nip\\nij p\\nijp\\nijn\\nij p\\ni x x d t udd dt x j i − −−= ∆ ≠ ∀\\n      (2)\\nWhere  ] 1 ; 0 [ : → ℜ+  and + +ℜ → ℜ :  are two de-\\ncreasing functions representing respectively a learning parameter and a neighbourhood factor. CCA provides also a similar method to project, in continuous way, new points in the original space onto the projected space, using the knowledge of already projected vectors. \\nBut, since CCA encounters difficulties with unfold-\\ning of very non-linear manifolds, an evolution called CDA has been proposed (Lee, Lendasse, Donckers, & Verleysen, 2000). It involves curvilinear distances (in order to better approximate geodesic distances on the considered manifold) instead of Euclidean ones. Curvilinear distances are processed in two steps way. First is built a graph between vectors by consider-ing k-NN, e, or other neighbourhood, weighted by \\nEuclidean distance between adjacent nodes. Then the curvilinear distance between two vectors is computed as the minimal distance between these vectors in the graph using Dijkstra’s algorithm. Finally the original CCA algorithm is applied using processed curvilinear Figure 2. Images of characteristic items: (a) Scratch; (b) dig; (c) dust; (d) cleaning marks\\n(a) (b) (c) (d)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15fe5878-5d43-4a04-9d83-a27cf3cc5365', embedding=None, metadata={'page_label': '169', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  ANN-Based Defects’ Diagnosis of Industrial Optical Devices\\ndistances. This algorithm allows dealing with very \\nnon-linear manifolds and is much more robust against the choices of a and l functions.\\n It has been successfully used as a preliminary step \\nbefore maximum likelihood classification in (Lennon, Mercier, Mouchot, & Hubert-Moy, 2001) and we have also showed its positive impact on neural network technique based classification performance (V oiry, Madani, Amarger, & Bernier, 2007). In this last paper, we have first demonstrated that a synthetic problem (nevertheless defined from our real industrial data) whose intrinsic dimensionality is two, is better treated by MLP after 2D dimension reduction than in its raw expression. We have also showed that CDA performs better for this problem than CCA and Self Organizing Map pre-processing.  \\nImplementation on Industrial Optical Devices\\nIn order to validate the above-presented concepts and to provide an industrial prototype, an automatic control system has been realized. It involves an Olympus B52 microscope combined with a Corvus stage, which al-lows scanning an entire optical component (presented in Figure 3). 50x magnification is used, that leads to microscopic 1.77 mm x 1.33 mm fields and 1.28 μm x 1.28 μm sized pixels. The proposed image processing method is applied on-line. A post-processing software enables to collect pieces of a defect that are detected in different microscopic fields (for example pieces of a long scratch) to form only one defect, and to compute an overall cartography of checked device (Figure 3).\\nThese facilities were used to acquire a great number \\nof Nomarski images, from which were extracted de-fects images using aforementioned technique. Two experiments called A and B were carried out, using two different optical devices. Table 1 shows the different parameters corresponding to these experiments. It’s important to note that, in order to avoid false classes learning, items images depicting microscopic field boundaries or two (or more) different defects were discarded from used database. Furthermore, studied optical devices were not specially cleaned, what ac-counts for the presence of some dusts and cleaning marks. Items of these two databases were labelled by an expert with two different labels: “dust” (class1) and “other defects” (class -1). Table 1 shows also items repartition between the two defined classes.\\nUsing this experimental set-up, classification experi -\\nment was performed. It involved a multilayer perceptron with n input neurons, 35 neurons in one hidden layer, and 2 output neurons (n-35-2) MLP. First this artificial neural network was trained for discrimination task be-tween classes 1 and -1, using database B. This training phase used BFGS (Broyden, Fletcher, Goldfarb, and Shanno) with Bayesian regularization algorithm, and was achieved 5 times. Subsequently, the generaliza-tion ability of obtained neural network was processed using database A. Since database A and B issued from different optical devices, such generalization results are significant. Following this procedure, 14 different experiments were conducted with the aim of studying the global classification performance and the impact of CDA dimensionality reduction on this performance. First experiment used original Fourrier-Mellin issued features (13-dimensional), the others used the same features after CDA n-dimensional space reduction (with n varying between 2 and 13).  Figure 4 depicts \\nglobal classification performances (calculated by av-eraging percentage of well-classified items for the 5 trainings) for the 14 different experiments, as well as \\nFigure 3. Automatic control system and cartography of a 100mm x 65mm optical device\\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c19cf28-7eca-4d7f-8180-89ba187eb013', embedding=None, metadata={'page_label': '170', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN-Based Defects’ Diagnosis of Industrial Optical Devices\\nA\\nDatabaseOptical \\nDevice Number of \\nmicroscopic \\nfieldsCorresponding \\nareaTotal items \\nnumberClass 1 items \\nnumberClass -1 items \\nnumber\\nA 1 1178 28 cm² 3865 275 3590\\nB 2 605 14 cm² 1910 184 1726Table 1. Description of the two databases used for validation experiments\\nclass 1 classification and class -1 classification perfor -\\nmances. It shows first that equivalent performances can \\nbe obtained using only 5-dimensional data instead of unprocessed defects representations (13-dimensional). As a consequence neural architecture complexity and therefore processing time can be saved using CDA dimensionality reduction, while keeping performance level. Moreover, obtained scores are satisfactory: about 70% of “dust” defects are well-recognized (this can be enough for aimed application) as well as about 97% of other defects (the few 3% errors can however pose problems because every “permanent” defect has to be reported). Furthermore, we think that this significant performances difference between class 1 and class -1 recognition is due to the fact that class 1 is underrep-resented in learning database.Figure 4. Classification performances for different CDA issued data dimensionality. Classification performances \\nusing raw data (13-dimensional) are also depicted as dotted lines.\\n50556065707580859095100\\n2 3 4 5 6 7 8 9 10 11 12 13Data Dimensionality% of classifier correct answersClas s -1 c las s ific ation pe rform anc e\\nClas s 1 c las s ific ation per form anc e\\nG lobal c las s ific ation per form anc e\\nFUTURE TRENDS\\nNext phase of this work will deal with classification tasks involving more classes. We want also use much more Fourier-Mellin invariants, because we think that it would improve classification performance by supplying additional information. In this case, CDA based dimensionality reduction technique would be a foremost step to keep reasonable classification system’s complexity and processing time.\\nCONCLUSION \\nA reliable diagnosis of aesthetic flaws in high-quality optical devices is a crucial task to ensure products’ nominal specification and to enhance the production quality by studying the impact of the process on such defects. To ensure a reliable diagnosis, an automatic system is needed to detect defects and secondly dis-', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce91cfa9-9b30-4516-8296-d659be187ce0', embedding=None, metadata={'page_label': '171', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  ANN-Based Defects’ Diagnosis of Industrial Optical Devices\\ncriminate the “false” defects (correctable defects) from \\n“abiding” (permanent) ones. In this paper is described a complete framework, which allows detecting all de-fects present in a raw Nomarski image and extracting pertinent features for classification of these defects. Obtained proper performances for “dust” versus “other” defects classification task with MLP neural network has demonstrated the pertinence of proposed approach. In addition, data dimensionality reduction permits to use low complexity classifier (while keeping performance level) and therefore to save processing time.\\nREFERENCES\\nBouchareine, P. (1999). Métrologie des Surfaces. Techniques de l’Ingénieur, R1390.\\nChatterjee, S. (2003). Design Considerations and Fabri-\\ncation Techniques of Nomarski Reflection Microscope. Optical Engineering, 42,  2202-2212.\\nChoksuriwong, A., Laurent, H., & Emile, B. (2005). Comparison of invariant descriptors for object rec-ognition. IEEE International Conference on Image \\nProcessing (ICIP), 377-380.\\nDemartines, P. (1994). Analyse de Données par Réseaux \\nde Neurones Auto-Organisés. PhD Thesis - Institut \\nNational Polytechnique de Grenoble.\\nDemartines, P. & Hérault, J. (1993). Vector Quantiza-\\ntion and Projection Neural Network. Lecture Notes in \\nComputer Science, 686,  328-333.\\nDemartines, P. & Hérault, J. (1995). CCA : “Curvilinear Component Analysis”. Proceedings of 15th workshop \\nGRETSI.\\nDerrode, S. (1999). Représentation de Formes Planes \\nà Niveaux de Gris par Différentes Approximations de \\nFourier-Mellin Analytique en vue d’Indexation de Bases d’Images. PhD Thesis - Université de Rennes I.\\nEgmont-Petersen, M., de Ridder, D., & Handels, H. \\n(2002). Image Processing with Neural Networks - A Review. Pattern Recognition, 35,  2279-2301.\\nFlewitt, P. E. J. & Wild, R. K. (1994). Light Microscopy. In Physical Methods for materials characterisation .Ghorbel, F. (1994). A Complete Invariant Description for Gray Level Images by the Harmonic Analysis Ap-proach. Pattern Recognition, 15,  1043-1051.\\nLee, J. A., Lendasse, A., Donckers, N., & Verleysen, M. (2000). A Robust Nonlinear Projection Method. In European Symposium on Artificial Neural Networks - ESANN’2000.\\nLennon, M., Mercier, G., Mouchot, M. C., & Hubert-\\nMoy, L. (2001). Curvilinear Component Analysis for Nonlinear Dimensionality Reduction of Hyperspectral Images. Proceedings of SPIE, 4541,  157-168.\\nVerleysen, M. (2001). Learning high-dimensional data. In LFTNC’2001 - NATO Advanced Research \\nWorkshop on Limitations and Future Trends in Neural Computing. \\nV oiry, M., Houbre, F., Amarger, V ., & Madani, K. (2005). \\nToward Surface Imperfections Diagnosis Using Optical Microscopy Imaging in Industrial Environment. IAR & \\nACD Workshop 2005 Proceedings, 139-144.\\nV oiry, M., Madani, K., Amarger, V ., & Bernier, J. (2007). \\nImpact of Data Dimensionality Reduction on Neural Based Classification: Application to Industrial Defects Classification. Proceedings of the 3rd International \\nWorkshop on Artificial Neural Networks and Intelligent Information Processing - ANNIIP 2007 , 56-65.\\nV oiry, M., Madani, K., Amarger, V ., & Houbre, F. (2006). Toward Automatic Defects Clustering in Industrial Production Process Combining Optical Detection and Unsupervised Artificial Neural Network Techniques. Proceedings of the 2nd International Workshop on Artificial Neural Networks and Intelligent Information Processing - ANNIIP 2006 , 25-34.\\nZhang, G. P. (2000). Neural Networks for Classifica-tion: A Survey. IEEE Trans.on Systems, Man, and \\nCybernetics - Part C: Applications and Reviews, 30,  \\n451-462.\\nKEy TERMS\\nArtificial Neural Networks: A network of many \\nsimple processors (“units” or “neurons”) that imitates a biological neural network. The units are connected by unidirectional communication channels, which carry numeric data. Neural networks can be trained to find nonlinear relationships in data, and are used ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2be37a3-3654-43a8-8760-9cb6e6032dc6', embedding=None, metadata={'page_label': '172', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18ANN-Based Defects’ Diagnosis of Industrial Optical Devices\\nAin applications such as robotics, speech recognition, \\nsignal processing or medical diagnosis.\\nBackpropagation algorithm: Learning algorithm \\nof ANNs, based on minimising the error obtained from the comparison between the outputs that the network gives after the application of a set of network inputs and the outputs it should give (the desired outputs).\\nClassification: Affectation of a phenomenon to a \\npredefined class or category by studying its character -\\nistic features. In our work it consists in determining the nature of detected optical devices surface defects (for example “dust” or “other type of defects”). \\nData Dimensionality Reduction: Data dimension-\\nality reduction is the transformation of high-dimensional data into a meaningful representation of reduced dimen-sionality. The goal is to find the important relationships between parameters and reproduce those relationships in a lower dimensionality space. Ideally, the obtained representation has a dimensionality that corresponds to the intrinsic dimensionality of the data. Dimensional-ity reduction is important in many domains, since it facilitates classification, visualization, and compression of high-dimensional data. In our work it’s performed using Curvilinear Distance Analysis.\\nData Intrinsic Dimension: When data is described \\nby vectors (sets of characteristic values), data intrinsic dimension is the effective number of degrees of free-dom of the vectors’ set. Generally, this dimension is smaller than the data raw dimension because it may exist linear and/or non-linear relations between the different components of the vectors.Data Raw Dimension: When data is described \\nby vectors (sets of characteristic values), data raw dimension is simply the number of components of these vectors. \\nDetection: Identification of a phenomenon among \\nothers from a number of characteristic features or “symptoms”. In our work, it consists in identifying surface irregularities on optical devices.\\nMLP (Multi Layer Perceptron): This widely \\nused artificial neural network employs the perceptron \\nas simple processor. The model of the perceptron, proposed by Rosenblatt is as follows:\\nIn this diagram, the X represent the inputs and \\nY the output of the neuron. Each input is multiplied by the weight w, a threshold b is subtracted from the result and finally Y is processed by the application of an activation function f. The weights of the connection are adjusted during a learning phase using backpropa-gation algorithm. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd190d8e-53af-4ffb-99bd-261078332e07', embedding=None, metadata={'page_label': '173', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nArtificial Intelligence and Education\\nEduardo Sánchez\\nUniversity of Santiago de Compostela, Spain\\nManuel Lama\\nUniversity of Santiago de Compostela, Spain\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nGovernments and institutions are facing the new de-mands of a rapidly changing society. Among many significant trends, some facts should be considered (Silverstein, 2006): (1) the increment of number and type of students; and (2) the limitations imposed by educational costs and course schedules. About the for-mer, the need of a continuous update of knowledge and competences in an evolving work environment requires life-long learning solutions. An increasing number of young adults are returning to classrooms in order to finish their graduate degrees or attend postgraduate programs to achieve an specialization on a certain domain. About the later, due to the emergence of new types of students, budget constraints and schedule conflicts appear. Workers and immigrants, for instance, are relevant groups for which educational costs and job incompatible schedules could be the key factor to register into a course or to give up a program after investing time and effort on it. In order to solve the needs derived from this social context, new educational approaches should be proposed: (1) to improve and extend the online learning courses, which would reduce student costs and allows to cover the educational needs of a higher number of students, and (2) to automate learning processes, then reducing teacher costs and providing a more personalized educational experience anytime, anywhere.\\nAs a result of this context, in the last decade an \\nincreasing interest on applying computer technologies in the field of Education has been observed. On this regard, the paradigms of  the Artificial Intelligence (AI) field are attracting an special attention to solve the issues derived from the introduction of computers as supporting resources of different learning strategies. In this paper we review the state-of-art of the applica-tion of Artificial Intelligence techniques in the field of Education, focusing on (1)  the most popular educa-tional tools based on AI, and (2) the most relevant AI techniques applied on the development of intelligent educational systems.\\nEXAMPLES OF EDUCATIONAL TOOLS BASED ON AI\\nThe field of Artificial Intelligence can contribute with interesting solutions to the needs of the educational domain (Kennedy, 2002). In what follows, the type of systems that can be built based on AI techniques are outlined.\\nIntelligent Tutoring Systems\\nThe Intelligent Tutoring Systems are applications that provide personalized/adaptive learning without the intervention of human teachers (VanLehn, 2006). They are constituted by three main components: (1) knowledge of the educational contents, (2) knowledge of the student, and (3) knowledge of the learning pro-cedures and methodologies. These systems promise to radically transform our vision of online learning. As opposed to the hypertext-based e-learning applications, which provide the students with a certain number of opportunities to search for the correct answer before showing it, the intelligent tutoring systems perform like coaches not only after the introduction of the re-sponse, but also offering suggestions when the students doubt or are blocked during the process of solving the problem. In this way, the assistance guide the learning process rather than merely saying what is correct or what is wrong.\\nThere exist numerous examples of intelligent tutor-\\ning systems, some of them developed at universities as research projects while others created with business goals. Among the first ones, the Andes systems (Van-\\nLehn, Lynch, Schulze, Shapiro, Shelby, Taylor, Treacy, ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99c3615b-1660-413d-9760-6912d2ead38f', embedding=None, metadata={'page_label': '174', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18AI and Education\\nAWeinstein & Wintersgill, 2005), developed under the \\nguidance of Kurt VanLehn of the University of Pittsburg, is a popular example. The system is in charge of guid-ing the students while they try to solve different sets of problems and exercises. When the student ask for help in the middle of an activity, the system either provides hints in order to step further towards the solution or points out what was wrong in some earlier step. Andes \\nwas successfully evaluated during 5 years in the Naval Academy of the United States and can be downloaded for free. Another relevant system is Cognitive Tutor \\n(Koedinger, Anderson, Hadley & Mark, 1997), is a comprehensive secondary mathematics curricula and computer-based tutoring program developed by John R. Anderson, professor at the Carnegie Mellon University. The Cognitive Tutor is an example of how research prototypes can be evolved into commercial solutions, as it is nowadays used in 1,500 schools in the United States. On the business side, Read-On! is presented as a product that teaches reading comprehension skills for adults. It analyzes and diagnoses the specific defi -\\nciencies and problems of each student and then adapts the learning process based on that features (Read On, 2007). It includes an authoring tool that allows course designers to adapt course contents to different student profiles in a fast and flexible way. \\nAutomatic Evaluation Systems\\nAutomatic Evaluation Systems are mainly focused on evaluating the strengths and weaknesses of students in different learning activities through assessment tests (Conejo, Guzmán, Millan, Trella, Perez-de-la-Cruz. & Rios, 2004). In this way, these systems not only perform the automatic correction of the test, but also derive automatically useful information about the competences and skills obtained by the students during the educational process.\\nAmong the automatic evaluation systems, we could \\nhighlight ToL (Test On Line) (Tartaglia & Tresso, 2002), \\nwhich have been used by Physics students in the Poly-technic University of Milano. The system is composed of a database of tests, an algorithm for question selec-tion, and a mechanism for the automatic evaluation of tests, which can be additionally configured by the teachers. CELLA (Comprehensive English Language \\nLearning Assesment) (Cella, 2007) is another system that evaluates the student competence on using and understanding the English language. The application shows the progress carried out by the students and determines their proficiency and degree of competence on the use of foreign languages. As for commercial applications, Intellimetric  is a Web-based system that \\nlets students to submit their work online (Intellimetric, 2007). In a few seconds, the AI-supported grading engine automatically provides the score of the work. The company claims a reliability of 99%, meaning that 99 percent of the time the engine´s scores match those provided by human teachers. \\nComputer Supported Collaborative Learning\\nThe environments of computer supported collaborative learning are aimed at facilitating the learning process providing the students both the context and tools to interact and work in a collaborative way with their class-mates (Soller, Martinez, Jermann & Muehlenbrock, 2005). In intelligent-based systems, the collaboration is usually carried out with the help of software agents in charge of mediating and supporting student interaction to achieve the proposed learning objectives.\\nThe research prototypes are the suitable test-beds \\nto prove new ideas and concepts, to provide the best collaborative strategies. The DEGREE system, for \\ninstance, allows the characterization of group behav-iours as well as the individual behaviours of the people constituting them, on the basis of a set of attributes or tags. The mediator agent utilizes those attributes, which are introduced by students, in order to provide recommendations and suggestions to improve the in-teraction inside each group (Barros & Verdejo, 2000). In the business domain there exist multiple solutions although they do not offer intelligent mediation to facilitate the collaborative interactions. The DEBBIE \\nsystem (DePauw Electronic Blackboard for Interactive Education) is one of the most popular (Berque, John-son, Hutcheson, Jovanovic, Moore, Singer & Slattery, 2000). It was originally developed at the beginning of year 2000 at the University of Depauw, and managed later by the DyKnow company, which was specifically created to make profit with DEBBIE  (Schnitzler, 2004). \\nThe technology that currently offers DyKnow allows both teachers and students to instantaneously share information and ideas. The final goal is to support student tasks in the classroom by eliminating the need of performing simple tasks, as for instance backing up the teacher’s presentations. The students could therefore ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb278068-cf11-4197-b2ce-7f58d1cae0df', embedding=None, metadata={'page_label': '175', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x180  AI and Education\\nbe more focused on understanding as well as analyzing \\nthe concepts presented by the teacher.\\nGame-Based Learning\\nLearning based on serious games, a term coined to distinguish between learning-oriented games used in education and purely entertaining-oriented games, deal with the utilization of the motivational power and at-tractiveness of games in the educational domain in order to improve the satisfaction and performance of students when acquiring new knowledge and skills. This type of learning allows to carry out activities in complex educational environments that would be impossible to implement, because of budget, time, infrastructure and security limitations, with traditional resources (Michael & Chen, 2005; Corti, 2006).\\nNetAid’s is an institution that develop games to \\nteach concepts of global citizenship and to sensitize to fight against poverty. One of its first games, released in 2002, called NetAid World Class, consists on taking \\nthe identity of a real child living in India and to resolve the real problems that confront the poor children in this region (Stokes, 2005). In 2003 the game was used by 40.000 students in different Schools across the United States. In the business and entertainment arena, many games exist that can be resorted to reach educational goals. Among the most popular ones, Brain Training of \\nNintendo (Brain Training, 2007) challenges the user to improve her mental shape by doing memory, reasoning and mathematical exercises. The final goal is to reach an optimal cerebral age after some regular training.\\nAI TECHNIQUES IN EDUCATION\\nThe intelligent educational systems reviewed above are based on a diversity of artificial intelligence techniques (Brusilovsky & Peylo, 2003). The most frequently used in the field of education are: (1) personalization mechanisms based on student and group models, (2) intelligent agents and agent-based systems, and (3) ontologies and semantic web techniques.\\nPersonalization Mechanisms\\nThe personalization techniques, which are the basis of intelligent tutoring systems, involve the creation and use of student models. Broadly speaking, these models imply the construction of a qualitative representation of student behavior in terms of existing background knowledge about a domain (McCalla, 1992). These representations can be further used in intelligent tutor-ing systems, intelligent learning environments, and to develop autonomous intelligent agents that may collaborate with human students during the learning process. The introduction of machine learning tech-niques facilitates to update and extend the first versions of student models in order to adapt to the evolution of each student as well as the possible changes and modifications of contents and learning activities (Sison & Shimura, 1998). The most popular student model-ing techniques are (Beck, Stern, & Haugsjaa, 1996): overlay models and bayesian network models. The first method consists on considering the student model as a subset of the knowledge of an expert in the domain on which the learning is taking place. In fact, the degree of learning is measured in terms of the comparison between the knowledge acquired and represented in the student model with the background initially stored in the expert model. The second method deals with the representation of the learning process as a network of knowledge states. Once defined, the model should infer, from the tutor-student interaction, the probability of the student on being in a certain state.\\nIntelligent Agents and Agent-Based Systems\\nSoftware agents are considered software entities, such as software programs or robots, that present, with different degree, three main attributes: autonomy, cooperation and learning (Nwana, 1996). Autonomy refers to the principle that an agent can operate on their own (act-ing and deciding upon its own representation of the world). Cooperation refers to the ability to interact with other agents via some communication language. Finally, learning is essential to react or interact with the external environment. Teams of intelligent agents build up MultiAgent Systems (MAS). In this type of systems each agent has either incomplete information or limited capabilities for solving the problem at hand. Other important aspect concerns with the lack of cen-tralized global control; therefore, data is distributed all over the system and computation is asynchronous (Sycara, 1998). Many important tasks can be carried out by intelligent agents in the context of learning and ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9fbd296-d091-45cf-9f75-f346158dd5fc', embedding=None, metadata={'page_label': '176', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18AI and Education\\nAeducational systems (Jafari, 2002, Sánchez, Lama, \\nAmorim, Riera, Vila & Barro, 2003): the monitoring of inputs, outputs, and the activity outcomes produced by the students; the verification of deadlines during homework and exercise submission; automatic answer-ing of student questions; and the automatic grading of tests and surveys.\\nOntologies and Semantic Web Techniques\\nOntologies aim to capture and represent consensual knowledge in a generic way, and that they may be reused and shared across software applications (Gómez-Pérez, Fernández-López & Corcho, 2004). An ontology is composed of concepts or classes and their attributes, the relationships between concepts, the properties of these relationships, and the axioms and rules that ex-plicitly represents the knowledge of a certain domain. In the educational domain, several ontologies have been proposed: (1) to describe the learning contents of technical documents (Kabel, Wielinga, & de How, 1999), (2) to model the elements required for the design, analysis, and evaluation of the interaction between learners in computer supported cooperative learning (Inaba, Tamura, Ohkubo, Ikeda, Mizoguchi & Toyoda, 2001), (3) to specify the knowledge needed to define new collaborative learning scenarios (Barros, Verdejo, Read & Mizoguchi, 2002), (4) to formalize the semantics of learning objects that are based on metadata standards (Brase & Nejdl, 2004), and (5) to describe the semantics of learning design languages (Amorim, Lama, Sánchez, Riera & Vila, 2006).\\nFUTURE TRENDS\\nThe next generation of adaptive environments will in-tegrate pedagogical agents, enriched with data mining and machine learning techniques, capable of providing cognitive diagnosis of the learners that will help to determine the state of the learning process and then optimize the selection of personalized learning designs.  Moreover, improved models of learners, facilitators, tasks and problem-solving processes, combined with the use of Ontologies and reasoning engines, will facilitate the execution of learning activities on either online platforms or traditional classroom settings.Research in this field is very active and faces am -\\nbitious goals. In some decades it could be possible to dream about sci-fi environments in which the students would have brain interfaces to directly interact with an intelligent assistant (Koch, 2006), which would play the role of a tutor with a direct connection with learn-ing areas of the brain.\\nCONCLUSION\\nIn this paper we have reviewed the state-of-art of the application of Artificial Intelligence techniques in the field of Education. AI approaches seem promising to improve the quality of the learning process and then to satisfy the new requirements of a rapidly changing society. Current AI-based systems such as intelligent tutoring systems, computer supported collaborative learning and educational games have already proved the possibilities of applying AI techniques. Future applications will both facilitate personalized learning styles and help the tasks of teachers and students in traditional classroom settings.\\nREFERENCES\\nSilverstein, S. (2006) Colleges see the future in tech-nology, Los Angeles Times .\\nKennedy, K. (2002) Top 10 Smart technologies for Schools: Artificial Intelligence. Retrieved from: http://\\nwww.techlearning.com/db_area/archives/TL/2002/11/topten5.html\\nVanLehn, K. (2006) The Behavior of Tutoring Sys-\\ntems. International Journal of Artificial Intelligence \\nin Education, 16:227-265.\\nVanLehn, K., Lynch, C., Schulze, K., Shapiro, J.A., \\nShelby, R., Taylor L., Treacy D., Weinstein A. & Win-tersgill M. (2005) The Andes Physics Tutoring System: Lessons Learned. International Journal of Artificial \\nIntelligence in Education , 15:147-204.\\nKoedinger K., Anderson J.R., Hadley, W.H. & Mark M.A. (1997) Intelligent tutoring goes to school in the big city. International Journal of Artificial Intelligence \\nin Education,  8: 30-43.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='75593db3-8a78-49a3-871a-8535efee174c', embedding=None, metadata={'page_label': '177', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  AI and Education\\nRead On! (2007). Retrieved from: http://www.stot-\\ntlerhenke.com/products/index.htm\\nConejo, R., Guzmán, E., Millan, E., Trella, M., Perez-\\nde-la-Cruz, J.L. & Rios, A. (2004) SIETTE: A Web-Based Tool for Adaptive Testing. International Journal \\nof Artificial Intelligence in Educatio n, 14:29-61.\\nTartaglia A. & Tresso E. (2002) An Automatic Evalua-tion System for Technical Education at the University Level. IEEE Transactions on Education, 45(3):268-275.\\nCELLA – Comprehensive English Language Learn-\\ning Assessment (2007) Retrieved from: http://www.ets.org\\nIntellimetric (2007) Retrieved from: http://www.van-\\ntagelearning.com/intellimetric/\\nSoller, A., Martinez, A., Jermann, P., & Muehlenbrock, \\nM. (2005) From Mirroring to Guiding: A Review of State of the Art Technology for Supporting Collab-orative Learning. International Journal of Artificial \\nIntelligence in Education, 15:261-290.\\nBarros, B. & Verdejo, M.F. (2000) Analysing student \\ninteraction processes in order to improve collabora-tion: the DEGREE approach. International Journal of \\nArtificial Intelligence in Educatio n, 11:221-241.\\nBerque, D., Johnson, D., Hutcheson, A., Jovanovic, L., Moore, K., Singer, C. & Slattery, K. (2000) The design of an interface for student note annotation in a networked electronic classroom. Journal of Network \\nand Computer Applications , 23(2):77-91.\\nSchnitzler, P. (2004) Becker bets on education software startup. The Indianapolis Business Journal, 24(44). \\nRetrieved from: http://www.dyknowvision.com/news/\\narticles/BeckerBetsOnEdSoftware.html\\nMichael, D. & Chen, S. (2005) Serious Games: Games \\nThat Educate, Train, and Inform. Course Technology \\nPTR.\\nCorti, K. (2006) Gamesbased Learning: a serious busi-\\nness application. Report on PixelLearning. Retrieved \\nfrom: http://www.pixelearning.com/docs/serious-\\ngamesbusinessapplications.pdf\\nStokes, B. (2005) Videogames have changed: time to \\nconsider Serious Games? The Development Education \\nJournal, 11(2).Brain Training (2007) Retrieved from: http://\\nes.videogames.games.yahoo.com/especiales/brain-training\\nBrusilovsky, P. & Peylo, C. (2003) Adaptive and \\nIntelligent Web-based Educational Systems. Interna-\\ntional Journal of Artificial Intelligence in Education , \\n13:156–169.\\nMcCalla, G. (1992) The central importance of student \\nmodeling to intelligent tutoring. En: New Directions for \\nIntelligent Tutoring Systems, Springer Verlag.\\nSison, R. & Shimura, M. (1998) Student modeling and \\nmachine learning. International Journal of Artificial \\nIntelligence in Education , 9:128-158.\\nBeck, J., Stern, M. & Haugsjaa, E. (1996) Applications of AI in Education. Crossroads , 3(1):11-15.\\nNwana, H.S. (1996) Software Agents: An Overview. Knowledge Engineering Review . 11(2): 205-244.\\nSycara, K.P. (1998) Multiagent Systems. AI Magazine, \\n19(2): 79-92.\\nJafari, A. (2002) Conceptualizing intelligent agents for \\nteaching and learning. Educause Quaterly, 25(3):28-\\n34.\\nSánchez, E., Lama, M., Amorim, R., Riera, A., Vila, J & \\nBarro, S. (2003) A multi-tiered agent-based architecture for a cooperative learning environment. Proceedings \\nof IEEE Euromicro Conference on Parallel and Dis-tributed Processing (PDP 2003), 2003.\\nGómez-Pérez, A., Fernández-López, M. & Corcho, O. \\n(2004) Ontological Engineering , Springer Verlag.\\nKabel, S., Wielinga, B. & de How, R. (1999) Ontolo-gies for indexing Technical Manuals for Instruction. Proceedings of the AIED-Workshop on Ontologies for Intelligent Educational Systems, LeMans, France, 44-53.\\nInaba, A., Tamura, T., Ohkubo, R., Ikeda, M., Mizo-\\nguchi, R. & Toyoda, J. (2001) Design and Analysis of Learners Interaction based on Collaborative Learning Ontology. Proceedings of the Second European Confer-\\nence on Computer-Supported Collaborative Learning (Euro-CSCL’2001), 308-315.\\nBarros, B., Verdejo, M.F., Read, T. & Mizoguchi, R. \\n(2002) Applications of a Collaborative Learning Ontol-', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9838d80c-33ac-4fb3-9d02-1d075d640ac3', embedding=None, metadata={'page_label': '178', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18AI and Education\\nAogy. Proceedings of the Second Mexican International \\nConference on Artificial Intelligence (MICAI 2002), \\n301-310.\\nBrase, J. & Nejdl, W. (2004) Ontologies and Metadata \\nfor eLearning. Handbook on Ontologies, Springer-\\nVerlag.\\nAmorim, R., Lama, M., Sánchez, E., Riera, A. & Vila, \\nX.A. (2006) A Learning Design Ontology based on the IMS Specification. Journal of Educational Technology \\n& Society , 9(1):38-57.\\nKoch, C. (2006) Christof Koch forecast the future. New \\nScientist. Retrieved from: http://www.newscientist.com/channel/opinion/science-forecasts/dn10626-christof-koch-forecasts-the-future.html\\nKEy TERMS\\nAutomatic Evaluation Systems:  Applications \\nfocused on evaluating the strengths and weaknesses of students in different learning activities through as-sessment tests.\\nComputer Supported Collaborative Learning \\n(CSCL): A research topic on supporting collaborative learning methodologies with the help of computers and collaborative tools.Game-Based Learning: A new type of learning that \\ncombines educational content and computer games in order to improve the satisfaction and performance of students when acquiring new knowledge and skills.\\nIntelligent Tutoring Systems:  A computer program \\nthat provides personalized/adaptive instruction to stu-dents without the intervention of human beings.\\nOntologies: A set of concepts within a domain \\nthat capture and represent consensual knowledge in a generic way, and that they may be reused and shared across software applications.\\nSoftware Agents:  Software entities, such as \\nsoftware programs or robots, characterized by their autonomy, cooperation and learning capabilities.\\nStudent Models: Representation of student be-\\nhavior and degree of competence in terms of existing background knowledge about a domain.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4cb72609-63ee-4abb-97ed-cdac6f9cc13d', embedding=None, metadata={'page_label': '179', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nArtificial Intelligence and Rubble-Mound\\nBreakwater Stability\\nGregorio Iglesias Rodriguez\\nUniversity of Santiago de Compostela, Spain\\nAlberte Castro Ponte\\nUniversity of Santiago de Compostela, Spain\\nRodrigo Carballo Sanchez\\nUniversity of Santiago de Compostela, Spain\\nMiguel Ángel Losada Rodriguez\\nUniversity of Granada, Spain\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nBreakwaters are coastal structures constructed to shelter a harbour basin from waves. There are two main types: rubble-mound breakwaters, consisting of various layers of stones or concrete pieces of different sizes (weights), making up a porous mound; and vertical breakwaters, impermeable and monolythic, habitually composed of concrete caissons. This article deals with rubble-mound breakwaters. \\nA typical rubble-mound breakwater consists of an \\narmour layer, a filter layer and a core. For the breakwater to be stable, the armour layer units (stones or concrete pieces) must not be removed by wave action. Stability is basically achieved by weight. Certain types of con-crete pieces are capable of achieving a high degree of interlocking, which contributes to stability by impeding the removal of a single unit.\\nThe forces that an armour unit must withstand un-\\nder wave action depend on the hydrodynamics on the breakwater slope, which are extremely complex due to wave breaking and the porous nature of the struc-ture. A detailed description of the flow has not been achieved until now, and it is unclear whether it will be in the future in view of the turbulent phenomena involved. Therefore the instantaneous force exerted on an armour unit is not, at least for the time being, amenable to determination by means of a numerical model of the flow. For this reason, empirical formu -\\nlations are used in rubble-mound design, calibrated on the basis of laboratory tests of model structures. However, these formulations cannot take into account all the aspects affecting the stability, mainly because the inherent complexity of the problem does not lend itself to a simple treatment. Consequently the empirical formulations are used as a predesign tool, and physical model tests in a wave flume of the particular design in question under the pertinent sea climate conditions are de rigueur, except for minor structures. The physical model tests naturally integrate all the complexity of the problem. Their drawback lies in that they are expensive and time consuming.\\nIn this article, Artificial Neural Networks are trained \\nand tested with the results of stability tests carried out on a model breakwater. They are shown to reproduce very closely the behaviour of the physical model in the wave flume. Thus an ANN model, if trained and tested with sufficient data, may be used in lieu of the physical model tests. A virtual laboratory of this kind will save time and money with respect to the conven-tional procedure.\\nBACKGROUND\\nArtificial Neural Networks have been used in civil engineering applications for some time, especially in Hydrology (Ranjithan et al., 1993; Fernando and Jay-awardena, 1998; Govindaraju and Rao, 2000; Maier and Dandy, 2000; Dawson and Wilby, 2001; Cigizoglu, 2004); some Ocean Engineering issues have also been tackled (Mase et al., 1995; Tsai et al., 2002; Lee and Jeng, 2002; Medina et al., 2003; Kim and Park, 2005; Yagci et al., 2005). Rubble-mound breakwater stabil-', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61162591-6c56-44ba-85f7-fc1df9e0ede2', embedding=None, metadata={'page_label': '180', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18AI and Rubble-Mound Breakwater Stability\\nAity is studied in Mase et al.’s (1995) pioneering work, \\nfocusing on a particular stability formula. Medina et al. (2003) train and test an Artificial Neural Network with stability data from six laboratories. The inputs are the relative wave height, the Iribarren number and a variable representing the laboratory. Kim and Park (2005) compare different ANN models on an analysis revolving around one empirical stability formula, as did Mase et al.’s (1995). Yagci et al. (2005) apply different kinds of neural networks and fuzzy logic, characterising the waves by their height, period and steepness. \\nPHySICAL MODEL AND ANN MODEL\\nThe Artificial Neural Networks were trained and tested on the basis of laboratory tests carried out in a wave flume of the CITEEC Laboratory, University of La Coruña. The flume section is 4 m wide and 0.8 m high, with a length of 33 m (Figure 1). Waves are generated by means of a piston-type paddle, controlled by an Active Absorption System (AWACS) which ensures that the waves reflected by the model are absorbed at the paddle.\\nThe model represents a typical three-layer rubble-\\nmound breakwater in 15 m of water, crowned at +9.00 m, at a 1:30 scale. Its slopes are 1:1.50 and 1:1.25 on the seaward and leeward sides, respectively.  The armour layer consists in turn of two layers of stones with a weight W=69 g ±10%; those in the upper layer \\nare painted in blue, red and black following horizontal bands, while those in the lower layer are painted in white, in order to easily identify after a test the dam-aged areas, i.e., the areas where the upper layer has \\nbeen removed. The filter layer is made up of a gravel with a median size D\\n50 = 15.11 mm and a thickness of \\n4 cm. Finally, the core consists of a finer gravel, with D\\n50 = 6.95 mm, D15 = 5.45 mm, and D85 = 8.73 mm, \\nand a porosity n = 42%. The density of the stones and \\ngravel is γr = 2700 kg/m3.\\nWaves were measured at six different stations along \\nthe longitudinal, or x-axis, of the flume. With the origin of x located at the rest position of the wave paddle, the first wave gauge, S1, was located at x=7.98 m. A group of three sensors, S2, S3 and S4, was used to separate the incident and the reflected waves. The central wave gauge, S3, was placed at x=12.28 m, while the position of the others, S2 and S4, was varied according to the wave generation period of each test (Table 1). Another wave gauge, S5, was located 25 cm in front of the model breakwater toe, at x=13.47 m, and 16 cm to the right (as seen from the wave paddle) of the flume centreline, so as not to interfere with the video recording of the \\nFigure 1. Experimental set-up\\nTable 1. Relative water depth (kh), wave period (T), and separation between sensors S2, S3 and S4 in the stabil-ity tests\\nTest key kh T (s) S2-S3 (cm) S3-S4 (cm)\\nT10, T20 0.98 1.65 35 55\\nT11, T21 1.36 1.3 20 30\\nT12, T22 1.68 1.13 20 30\\nT13, T23 1.97 1.03 20 30', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3ab5df27-18f6-4381-a07d-3c3c1a7af9b2', embedding=None, metadata={'page_label': '181', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  AI and Rubble-Mound Breakwater Stability\\ntests. Finally, a wave gauge (S6) was placed to the lee \\nof the model breakwater, at x=18.09 m. \\nBoth regular and irregular waves were used in the \\nstability tests. This article is concerned with the eight regular wave tests, carried out with four different wave periods. The water depth in the flume was kept constant throughout the tests ( h=0.5 m). Each test consisted of a \\nnumber of wave runs with a constant value of the wave period T, related to the wavenumber k by\\n \\n[ ]1\\n2 2 tanh( )T gk kh−=,\\nwhere g is the gravitational acceleration. The wave \\nperiods and relative water depths ( kh) of the tests are \\nshown in Table 1. \\nEach wave run consisted of 200 waves. In the first \\nrun of each test, the generated waves had a model height H=6 cm (corresponding to a wave height in the \\nprototype H\\np=1.80 m); in the subsequent runs, the wave \\nheight was increased in steps of 1 cm (7 cm, 8 cm, 9 cm, etc.), so that the model breakwater was subject to ever more energetic waves. \\nFour damage levels (Losada et al., 1986) were \\nused to characterize the stability situation of the model breakwater after each wave run:\\n(0)  No damage. No armour units have been moved \\nfrom their positions.\\n(1)  Initiation of damage. Five or more armour units \\nhave been displaced.\\n(2) Iribarren damage. The displaced units of the \\narmour’s first (outer) layer have left uncovered \\nan area of the second layer large enough for a stone to be removed by waves.\\n(3)  Initiation of destruction. The first unit of the \\narmour’s second layer has been removed by wave action.\\nAs the wave height was increased through a test, \\nthe damage level also augmented from the initial ‘no damage’ to ‘initiation of damage’, ‘Iribarren damage’, and eventually ‘initiation of destruction’, at which point the test was terminated and the model rebuilt for the following test. The number of wave runs in a test varied from 10 to 14.\\nThe foregoing damage levels provide a good semi-\\nquantitative assessment of the breakwater stability condition. However, the following nondimensional damage parameter is more adequate for the Artificial Neural Network model: \\n50\\n(1 )nDSp b=−\\nwhere D50 is the median size of the armour stones, p \\nis the porosity of the armour layer, b is the width of \\nthe model breakwater, and n is the number of units \\ndisplaced after each wave run. In this case,  D50 = 2.95 \\ncm, p = 0.40, and b = 50 cm.\\nThe incident wave height was nondimensionalized \\nby means of the zero-damage wave height of the SPM (1984) formulation,\\n \\n1\\n33\\n01 cotr\\nD\\nw\\nrW K\\nH\\uf8eb \\uf8f6 \\uf8eb \\uf8f6\\uf8ec \\uf8f7 −\\uf8ec \\uf8f7\\uf8ec \\uf8f7 \\uf8ed \\uf8f8=\\uf8ec \\uf8f7\\n\\uf8ec \\uf8f7\\n\\uf8ec \\uf8f7\\uf8ed \\uf8f8\\nwhere KD=4 is the stability coefficient, γw=1000 kg/m3 \\nis the water density (freshwater used in the laboratory \\ntests), and a is the breakwater slope. With these val-\\nues, H0 = 9.1 cm. The nondimensional incident wave \\nheight is given by\\n0*HHH=\\nwhere H stands for the incident wave height.\\nMost of the previous applications of Artificial \\nNeural Networks in Civil Engineering use multilayer feedforward networks trained with the backpropagation algorithm (Freeman and Skapura, 1991; Johansson et al., 1992), which will also be employed in this study; their main advantage lies in their generalisation capa-bilities. Thus this kind of network may be used, for instance, to predict the armour damage that a model breakwater will sustain under certain conditions, even if these conditions were not exactly part of the data set with which the network was trained. However, the pa-rameters describing the conditions (e. gr., wave height and period) must be within the parameter ranges of the stability tests with which the ANN was trained.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9010eaf-ae19-4619-ac4b-b7ec3973a524', embedding=None, metadata={'page_label': '182', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18AI and Rubble-Mound Breakwater Stability\\nAIn this case, the results from the stability tests of the \\nmodel rubble-mound breakwater described above were \\nused to train and test the Artificial Neural Network. The eight stability tests comprised 96 wave runs. The input to the network was the nondimensional wave height (H*) and the relative water depth (kh) of a wave run, \\nand the output, the resulting nondimensional damage parameter (S). Data from 49 wave runs, corresponding to the four stability tests T20, T21, T22, and T23, were used for training the network; while data from 46 wave runs, pertaining to the remaining four tests (T10, T11, T12, and T13) were used for testing it. This distribution of data made sure that each of the four wave generation periods (Table 1) was present in both the training and the testing data sets.\\nFirst, an Artificial Neural Network with 10 sigmoid \\nneurons in the hidden layer and a linear output layer was trained and tested 10 times. The ANN was trained by means of the Bayesian Regularisation method (MacKay, 1992), known to be effective in avoiding overfitting. The average MSE values were 0.2880 considering all the data, 0.2224 for the training data set, and 0.3593 for the testing data set. The standard deviations of the MSE values were 5.9651x10\\n-10, 9.0962x10-10, and \\n7.7356x10-10, for the complete data set, the training \\nand the testing data sets, respectively.  Increasing the number of neural units in the hidden layer to 15 did not produce any significant improvement in the aver -\\nage MSE values (0.2879, 0.2222 and 0.3593 for all the data, the training data set and the testing data set, respectively), so the former Artificial Neural Network, with 10 neurons in the hidden layer, was retained. \\nThe following results correspond to a training and \\ntesting run of this ANN with a global MSE of 0.2513. The linear regression analysis indicates that the ANN data fit very well to the experimental data over the whole range of the nondimensional damage parameter S. In \\neffect, the correlation coefficient is 0.983, and the equa -\\ntion of the best linear fit, \\n0.938 0.00229y x= − , is very \\nclose to that of the diagonal line y = x (Figure 2).\\nThe results obtained with the training data set (sta-\\nbility tests T20, T21, T22 and T23) show an excellent agreement between the ANN model and the physical model (Figure 3). In three of the four tests (T20, T22 and T23) the ANN data mimic the measurements on the model breakwater almost to perfection. In test T21, the physical model experiences a brusque increase in the damage level at H* =1.65, which is slightly softened \\nby the ANN model. The MSE value is 0.1441.The testing data set comprised also four stability \\ntests (T10, T11, T12 and T13). The inherent difficulty of the problem is apparent in test T11 (Figure 4), in which the nondimensional damage parameter (S) does not increase in the wave run at H* =1.54, but sud-\\ndenly soars by about 100% in the next wave run, at H* =1.65. Such differences from one wave run to the \\nnext are practically impossible to capture by the ANN model, given that the inputs to the ANN model either vary only slightly, by less than 7% in this case (the nondimensional wave height, H*) or do not vary at all \\n(the relative water depth, kh). It should be remembered \\nthat, when computing the damage after a given wave run, the ANN does not have any information about the damage level before that wave run, unlike the physical model. Yet the ANN performs well, yielding an MSE value of 0.3678 with the testing data set. \\nFUTURE TRENDS\\nIn this study, results from stability tests carried out with regular waves were used. Irregular wave tests should also be analyzed by means of Artificial Intelligence, and it is the authors’ intention to do so in the future. Breakwater characteristics are another important aspect of the problem. The ANN cannot extrapolate beyond the ranges of wave and breakwater characteristics on which it was trained. The stability tests used for this Figure 2. Regression analysis. Complete data set.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8196ae02-54b9-4937-94f2-747fddf7422f', embedding=None, metadata={'page_label': '183', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  AI and Rubble-Mound Breakwater Stability\\nFigure 3. ANN ( \\x7f) and physical model results ( ο) for the stability tests T20, T21, T22 and T23 (training data \\nset)\\nFigure 4. ANN ( \\x7f) and physical model results (ο) for the stability tests T10, T11, T12 and T13 (testing data \\nset)\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9319d642-8390-4ab4-8788-3dea0f27f338', embedding=None, metadata={'page_label': '184', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18AI and Rubble-Mound Breakwater Stability\\nAstudy considered one model breakwater; further tests \\ninvolving physical models with other geometries and materials should be undertaken. Once the potential of Artificial Neural Networks to model the behaviour of a rubble-mound breakwater subject to wave action has been proven, a virtual laboratory could be constructed with the results from these tests. \\nCONCLUSION\\nThis article shows that Artificial Neural Networks are capable of modelling the behaviour of a model rubble-mound breakwater in the face of energetic waves. This is a very complex problem for a number of reasons. In the first place, the hydrodynamics of waves break-ing on a slope are not well known, so much so that a detailed characterization of the motions of the water particles is not possible for the time being, and may remain so in the future due to the chaotic nature of the processes involved. Second, in the case of a rubble-mound breakwater the problem is further compounded by the porous nature of the structure, which brings about a complex wave-structure interaction in which the flux of energy carried by the incident wave is distributed into the following processes: (i) wave reflection; (ii) wave breaking on the slope; (iii) wave transmission through the porous medium; and (iv) dissipation. The subtle interplay between all these processes means that it is not possible to study one of them without taking the others into account. Third, the porous medium itself is of a stochastic nature: no two rubble-mound break-waters can be said to be identical. This complexity has precluded up to now the development of a numerical model which can reliably analyse the forces acting on the armour layer units and hence the stability situation of the breakwater. As a consequence, physical model tests are a necessity whenever a major rubble-mound structure is envisaged.\\nNotwithstanding the difficulty of the problem, the \\nArtificial Neural Network used in this work has been shown to reproduce very closely the physical model results. Thus, an Artificial Neural Network can con-stitute, once properly trained and validated, a virtual laboratory. Testing a breakwater in this virtual labora-tory is much quicker and far less expensive that testing a physical model of the same structure in a laboratory wave flume.REFERENCES\\nCigizoglu, H.K., 2004. Estimation and forecasting of daily suspended sediment data by multilayer percep-trons. Advances in Water Resources 27,185-195.\\nDawson, C.W., Wilby, R.L., 2001. Hydrological model-\\ning using artificial neural networks. Progress in Physical Geography 25 (1), 80-108. 20\\nFernando, D.A.K., Jayawardena, A.W., 1998. Runoff \\nforecasting using RBF networks with OLS algorithm. Journal of Hydrologic Engineering 3(3), 203-209.\\nFreeman, J. A., Skapura, D. M., 1991. Neural Net-\\nworks. Algorithms, Applications, and Programming Techniques. Addison-Wesley.\\nGovindaraju, R.S., Rao, A.R., 2000. Artificial neural \\nnetworks in hydrology. Kluwer Academic Publishers, Dordrecth Boston, MA, p. 329.\\nHaykin, S. (1999). Neural Networks (2nd ed.). Engle-\\nwood Cliffs, NJ: Prentice Hall.\\nJohansson, E. M., Dowla, F. U., Goodman, D. M., 1992. \\nBackpropagation learning for multi-layer feed-forward neural networks using the conjugate gradient method. Int. J. of Neural Systems, 2(4), 291-301.\\nKim, D.H., Park, W.S., 2005. Neural network for design \\nand reliability analysis of rubble mound breakwaters. Ocean Engineering 32 (11-12), 1332-1349. 21\\nLee, T.L., Jeng, D.S., 2002. Application of artificial \\nneural networks in tideforecasting. Ocean Engineering 29 (9), 1003–1022.\\nLippmann, R. P., 1987. An Introduction to Computing \\nwith Neural Nets, IEEE, ASSP Magazine.\\nLosada, M. A., Desiré, J. M., Alejo, L. M., 1986. Sta-\\nbility of blocks as breakwater armor units. J. Struc. Engrg., ASCE, 112(11), 2392-2401.\\nMacKay, D. J. C., 1992, Bayesian interpolation, Neural \\nComputation, vol. 4, no. 3, pp. 415-447.\\nMaier, H.R., Dandy, G.C., 2000. Neural network for \\nthe prediction and forecasting of water resources vari-ables: a review of modeling issues and applications. Environmental Modeling and Software 15, 101–124.\\nMase, H., Sakamoto, M., Sakai, T., 1995. Neural \\nnetwork for stability analysis of rubble mound break-', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd968036-ed5b-416f-ab5c-3153d542efc1', embedding=None, metadata={'page_label': '185', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x180  AI and Rubble-Mound Breakwater Stability\\nwaters. Journal of Waterway, Port, Coastal and Ocean \\nEngineering, ASCE 121 (6), 294–299.\\nMedina, J. R., Garrido, J., Gómez-Martín, M.E., Vi-\\ndal, C., 2003. Armour damage analysis using Neural Networks. Proc. Coastal Structures ’03, Portland, Oregon (USA).\\nRanjithan, S., Eheart, J. W., Garrett, J.H., 1993. Neural \\nnetwork-based screening for groundwater reclamation under uncertainty. Water Resources Research 29 (3), 563-574.\\nSPM, 1984. Shore Protection Manual. Dept. of the \\nArmy, Coast. Engrg. Res. Ctr., Wtrwy. Experiment Station, Vicksburg, Miss. (USA).\\nTsai, C.P., Lin, C., Shen, J.N., 2002. Neural network \\nfor wave forecasting among multi-stations. Ocean Engineering 29 (13), 1683–1695.\\nYagci, O., Mercan, D.E., Cigizoglu, H.K., Kabdasli, \\nM.S., 2005. Artificial intelligence methods in break -\\nwater damage ratio estimation. Ocean Engineering 32 (17-18), 2088-2106.KEy TERMS\\nArmour Damage: Extraction of stones or concrete \\nunits from the armour layer by wave action. \\nArmour Layer: Outer layer of a rubble-mound \\nbreakwater, consisting of heavy stones or concrete blocks.\\nArtificial Neural Networks: Interconnected set \\nof many simple processing units, commonly called neurons, that use a mathematical model representing an input/output relation.\\nBackpropagation Algorithm: Supervised learn-\\ning technique used by ANNs that iteratively modifies the weights of the connections of the network so the error given by the network after the comparison of the outputs with the desired one decreases.\\nBreakwater: Coastal structure built for sheltering \\nan area from waves, usually for loading or unloading vessels.\\nReflection: The process by which the energy of the \\nincoming waves is returned seaward.\\nSignificant Wave Height: In wave record analysis, \\nthe average height of the highest one-third of a selected number of waves. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09c55e78-ed08-4287-b76a-750afff06ce3', embedding=None, metadata={'page_label': '186', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nAArtificial Intelligence for Information Retrieval\\nThomas Mandl\\nUniversity of Hildesheim, Germany\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nThis article describes the most prominent approaches to apply artificial intelligence technologies to infor-\\nmation retrieval  (IR). Information retrieval is a \\nkey technology for knowledge management. It deals with the search for information and the representation, storage and organization of knowledge. Information \\nretrieval is concerned with search processes in which a \\nuser needs to identify a subset of information which is relevant for his information need within a large amount of knowledge. The information seeker formulates a query trying to describe his information need. The query is compared to document representations which were extracted during an indexing  phase. The representations \\nof documents and queries are typically matched by a similarity function such as the Cosine. The most similar documents are presented to the users who can evaluate the relevance with respect to their problem (Belkin, 2000). The problem to properly represent documents and to match imprecise representations has soon led to the application of techniques developed within Artificial Intelligence to information retrieval. \\nBACKGROUND\\nIn the early days of computer science, information \\nretrieval (IR) and artificial intelligence (AI) developed \\nin parallel. In the 1980s, they started to cooperate and the term intelligent information retrieval was coined \\nfor AI applications in IR. In the 1990s, information \\nretrieval has seen a shift from set based Boolean \\nretrieval models to ranking systems like the vector \\nspace model and probabilistic  approaches. These \\napproximate reasoning systems opened the door for more intelligent value added components. The large amount of text documents available in professional databases and on the internet has led to a demand for intelligent methods in text retrieval and to considerable research in this area. The need for better preprocessing to extract more knowledge from data has become an important way to improve systems. Off the shelf ap-proaches promise worse results than systems adapted to users, domain and information needs. Today, most techniques developed in AI have been applied to re-trieval systems with more or less success. When data from users is available, systems use often machine learning to optimize their results.\\nArtificial Intelligence Methods in Information Retrieval \\nArtificial intelligence methods are employed throughout the standard information retrieval process and for \\nnovel value added services. The first section gives a brief overview of information retrieval. The subsequent sections are organized along the steps in the retrieval process and give examples for applications. \\nInformation Retrieval\\nInformation retrieval deals with the storage and \\nrepresentation  of knowledge and the retrieval of \\ninformation relevant for a specific user problem. The information seeker formulates a query trying to de-scribe his information need. The query is compared to document representations . The representations \\nof documents and queries are typically matched by a similarity function such as the Cosine or the Dice coef-ficient. The most similar documents are presented to the users who can evaluate the relevance with respect to their problem. \\nIndexing usually consists of the several phases. \\nAfter word segmentation, stopwords are removed. These common words like articles or prepositions contain little meaning by themselves and are ignored in the document representation. Second, word forms \\nare transformed into their basic form, the stem. During the stemming phase, e.g. houses would be transformed \\ninto house. For the document representation , different \\nword forms are usually not necessary. The importance of a word for a document can be different. Some words better describe the content of a document than others. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='18f970d2-a5e5-4a54-9520-8562f4cb8423', embedding=None, metadata={'page_label': '187', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Artificial Intelligence for Information Retrieval\\nThis weight is determined by the frequency of a stem \\nwithin the text of a document (Savoy, 2003). \\nIn multimedia retrieval, the context is essential \\nfor the selection of a form of query and document representation . Different media representation s may \\nbe matched against each other or transformations may become necessary (e.g. to match terms against pictures or spoken language utterances against documents in written text).\\nAs information retrieval needs to deal with \\nvague knowledge, exact processing methods are not appropriate. Vague retrieval models like the probabilistic model are more suitable. Within these models, terms are provided with weights corresponding to their importance for a document. These weights mirror different levels of relevance. \\nThe result of current information retrieval systems \\nare usually sorted lists of documents where the top results are more likely to be relevant according to the system. In some approaches, the user can judge the documents returned to him and tell the systems which ones are relevant for him. The system then resorts the result set. Documents which contain many of the words present in the relevant documents are ranked higher. This relevance feedback process is known to greatly improve the performance. Relevance feedback is also an interesting application for machine learning. Based on a human decisions, the optimization step can be modeled with several approaches, e.g. with rough sets (Singh & Dey 2005). In Web environments, a click is often interpreted as an implicit positive relevance judgment (Joachims & Radlinski, 2007). \\nAdvanced Representation Models\\nIn order to represent documents in natural language, the content of these documents needs to be analyzed. This is a hard task for computer systems. Robust semantic analysis for large text collections or even multime-dia objects has yet to be developed. Therefore, text documents are represented by natural language terms mostly without syntactic or semantic context. This is often referred to as the bag-of-words approach. These keywords or terms can only imperfectly represent an object because their context and relations to other terms are lost. \\nHowever, great progress has been made and systems \\nfor semantic analysis are getting competitive. Advanced syntactic and semantic parsing for robust processing of mass data has been derived from computational linguistics (Hartrumpf, 2006). \\nFor application and domain specific knowledge, \\nanother approach is taken to improve the representation  \\nof documents. The representation scheme is enriched \\nby exploiting knowledge about concepts of the domain (Lin & Demner-Fushman, 2006). \\nMatch Between Query and Document \\nOnce the representation has been derived, a crucial \\naspect of an information retrieval system is the \\nsimilarity calculation between query and document representation. Most systems use mathematical simi-larity functions such as the Cosine. The decision for a specific function is based on heuristics or empirical evaluations. Several approaches use machine learning for long term optimization of the matching between term and document. E.g. one approach applies genetic algorithm to adapt a weighting function to a collection (Almeida et al., 2007). \\nNeural networks have been applied widely in IR. \\nSeveral network architectures have been applied for retrieval tasks, most often the so-called spreading activa-tion networks are used. Spreading activation networks are simple Hopfield-style networks, however, they do not use the learning rule of Hopfield networks. They typically consist of two layers representing terms and documents. The weights of connections between the layers are bi-directional and initially set according to the results of the traditional indexing and weighting \\nalgorithms (Belkin, 2000). The neurons corresponding to the terms of the user’s query are activated in the term layer and activation spreads along the weights into the document layer and back. Activation represents relevance or interest and reaches potentially relevant terms and documents. The most highly activated docu-ments are presented to the user as result. A closer look at the models reveals that they very much resemble the traditional vector space model of Information \\nRetrieval (Mandl, 2000). It is not until after the second \\nstep that associative nature of the spreading activation process leads to results different from a vector space model. The spreading activation networks successfully tested with mass data do not take advantage of this associative property. In some systems the process is halted after only one step from the term layer into the document layer, whereas others make one more step ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b146f6b2-70f0-4bca-9d3a-9669f365c47a', embedding=None, metadata={'page_label': '188', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Artificial Intelligence for Information Retrieval\\nAback to the term layer to facilitate learning (Kwok & \\nGrunfeld, 1996).\\nQueries in information retrieval systems are \\nusually short and contain few words. Longer queries have a higher probability to achieve good results. As a consequence, systems try to add good terms to a query entered by a user. Several techniques have been applied. Either these terms are taken from top ranked documents or terms similar to the original ones are used. Another technique is to use terms from documents from the same category. For this task, classification algorithms from machine learning are used (Sebastiani, 2002). \\nLink analysis applies well known measures from \\nbibliometric analysis to the Web. The number links pointing to a Web page is used as an indicator for its quality (Borodin et al., 2005). PageRank assigns an authority value to each Web page which is primarily a function of its back links. Additionally, it assumes that links from pages with high authority should be weighed higher and should result in a higher authority for the receiving page. To account for the different values each page has to distribute, the algorithm is carried out iteratively until the result converges (Borodin et al., 2005). Machine Learning approaches complement link analysis. Decisions of humans about the quality of Web pages are used to determine design features of these pages which are good indicators of their quality. Machine learning models are applied to determine the quality of pages not judged yet (Mandl, 2006, Marti & Hearst, 2002). \\nLearning from users has been an important strategy \\nto improve systems. In addition to the content, artificial intelligence methods have been used to improve the user interface. \\nValue Added Components for User Interfaces\\nSeveral Researchers have implemented information \\nretrieval systems based on the Kohonen self organiz-\\ning map (SOM), a neural network model for unsuper-vised classification. They provide an associative user interface where neighborhood of documents expresses a semantic relation. Implementations for large collec-tions can be tested on the internet (Kohonen, 1998). The SOM consists of a usually two-dimensional grid of neurons, each associated with a weight vector. Input documents are classified according to the similarity between the input pattern and the weight vectors, and, the algorithm adapts the weights of the winning neuron and its neighbor. In that way, neighboring clusters have a high similarity. \\nThe information retrieval applications of SOMs \\nclassify documents and assign the dominant term as name for the cluster. For real world large scale col-lections, one two-dimensional grid is not sufficient. It would be either too big or each node would contain too many documents consequently. Neither would be helpful for users, therefore, a layered architecture is adopted. The highest layer consists of nodes which represent clusters of documents. The documents of these nodes are again analyzed by a SOM. For the \\nuser, the system consists of several two-dimensional maps of terms where similar terms are close to each other. After choosing one node, he may reach another two-dimensional SOM. \\nThe information retrieval paradigm for the SOM  is \\nbrowsing and navigating between layers of maps. The SOM seems to be a very natural visualization. However, the SOM approach has some serious drawbacks. \\n• The interface for interacting with several layers \\nof maps makes the system difficult to browse. \\n• Users of large text collections need primarily search mechanisms which the SOM itself does not offer. \\n• The similarity of the document collection is reduced to two dimensions omitting many po-tentially interesting aspects.\\n• The SOM unfolds its advantages for human-computer-interaction better for a small number of documents. A very encouraging application would be the clustering of the result set. The neurons would fit on one screen, the number of terms would be limited and therefore, the reduc-tion to two dimensions would not omit so many aspects. \\nUser Classification and Personalization\\nAdaptive information retrieval approaches intend to \\ntailor the results of a system to one user and his inter-ests and preferences. The most popular representation \\nscheme relies on the representation scheme used in \\ninformation retrieval where a document-term-matrix \\nstores the importance or weight of each term for each document. When a term appears in a document, this weight should be different form zero. User interest can ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b5ac388-4ef6-4c3c-afe4-25f7145a6479', embedding=None, metadata={'page_label': '189', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Artificial Intelligence for Information Retrieval\\nalso be stored like a document. Then the interest is a \\nvector of terms. These terms can be ones that a user has entered or selected in a user interface or which the system has extracted from documents for which the user has shown interest by viewing or downloading them (Agichtein et al., 2006). \\nAn example for such a system is UCAIR which \\ncan be installed as a browser plugin. UCAIR relies on a standard web search engine to obtain a search result and a primary ranking. This ranking is now \\nbeing modified by re-ranking the documents based \\non implicit feedback and a stored user interest profile (Shen et al., 2005).\\nMost systems use this method of storing the user \\ninterest in a term vector. However, this method has several drawbacks. The interest profile may not be stable and the user may have a variety of diverging interests for work and leisure which are mixed in one profile. \\nAdvanced individualization techniques personal-\\nize the underlying system functions. The results of empirical studies have shown that relevance feedback is an effective technique to improve retrieval quality. Learning methods for information retrieval need to \\nextend the range of relevance feedback effects beyond the modification of the query in order to achieve long-term adaptation to the subjective point of view of the user. The mere change of the query often results in improved quality; however, the information is lost after the current session. \\nSome systems change the document representation  \\naccording to the relevance feedback information. In a vector space metaphor, the relevant documents are moved toward the query representation . This approach \\nalso comprises some problems. Because only a fraction of the documents are affected by the modifications, the basic data from the indexing process is changed to a \\nsomewhat heterogeneous state. The original indexing \\nresult is not available anymore. \\nCertainly, this technique is inadequate for fusion \\napproaches where several retrieval methods are com-bined. In this case, several basic representation s would \\nneed to be changed according to the influence of the corresponding methods on the relevant documents. The indexes are usually heterogeneous, which is often considered an advantage of fusion approaches. A high computational overload would be the consequence.\\nThe MIMOR (Multiple Indexing and Method-Object \\nRelations) approach does not rely on changes to the document or the query representation  when processing relevance feedback information for personalization. Instead, it focuses on the central aspect of a retrieval function, the calculation of the similarity between docu-ment and query. Like other fusion methods, MIMOR accepts the result of individual retrieval systems like from a black box. These results are fused by a linear combination which is stored during many sessions. The weights for the systems experience a change through learning. They adapt according to relevance feedback information provided by users and create a long-term model for future use. That way, MIMOR learns which systems were successful in the past (Mandl & Womser-Hacker, 2004).\\nFUTURE TRENDS\\nInformation retrieval systems are applied in more and \\nmore complex and diverse environments. Searching e-mail, social computing collections and other specific domains pose new challenges which lead to innovative systems. These retrieval applications require thorough and user oriented evaluation. New evaluation measures and standardized test collections are necessary to achieve reliable evaluation results. \\nIn user adaptation, recommendation systems are an \\nimportant trend for future improvement. Recommenda-tion systems need to be seen in the context of social computing applications. System developers face the growth of user generated content which allows new reasoning methods. \\nNew application like question answering relying on \\nmore intelligent processing can be expected to gain more market share in the near future (Hartrumpf, 2006)\\nCONCLUSION\\nKnowledge management is of main importance for the information society. Documents written in natural language contain an important share of the knowl-edge available. Consequently, retrieval is crucial for the success of knowledge management systems. AI technologies have been widely applied in retrieval systems. Exploiting knowledge more efficiently is a major research field. In addition, user oriented value added systems require intelligent processing and ma-chine learning in many forms. \\nAn important future trend for AI methods in IR will \\nbe the context specific adaptation of retrieval methods. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53fe79ee-e74e-4a28-98ec-2a9369cbc6ed', embedding=None, metadata={'page_label': '190', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Artificial Intelligence for Information Retrieval\\nAMachine learning can be applied to find optimized \\nfunctions for collections or queries. \\nREFERENCES\\nAgichtein, E., Brill, E., Dumais, S., & Ragno, R. (2006). Learning user interaction models for predicting web search result preferences. Annual International ACM \\nConference on Research and Development in Informa-tion Retrieval (SIGIR) Seattle. ACM Press. 3-10.\\nde Almeida, H.M., Gonçalves, M.A, Cristo, M., & Calado, P. (2007). A combined component approach for finding collection-adapted ranking functions based on genetic programming. Annual International ACM \\nConference on Research and Development in Infor-mation Retrieval (SIGIR) Amsterdam. ACM Press. \\n399-406. \\nBelkin, R. (2000). Finding out about: a Cognitive Per-\\nspective on Search Engine Technology and the WWW. Cambridge et al.: Cambridge University Press. \\nBrusilovsky, P., Kobsa, A. & Nejdl, W. (eds.) (2007). \\nThe adaptive Web: Methods and strategies of Web personalization. Heidelberg: Springer.\\nBoughanem, M., & Soulé-Dupuy, C. (1998). Mercure \\nat trec6. In V oorhees, E. & Harman, D. (eds.). The \\nsixth text retrieval conf (TREC-6). NIST Special Publ 500-240. Gaithersburg, MY .\\nBorodin, A., Roberts, G., Rosenthal, J. & Tsaparas, P. \\n(2005). Link analysis ranking: algorithms, theory, and experiments. ACM Transactions on Internet Technology \\n(TOIT) 5(1), 231-297.\\nHartrumpf, S. (2006). Extending Knowledge and \\nDeepening Linguistic Processing for the Question Answering System InSicht. Accessing Multilingual Information Repositories, 6\\nth Workshop of the Cross-\\nLanguage Evalution Forum, CLEF. [LNCS 4022] Springer. 361-369 \\nIvory M. & Hearst, M. (2002). Statistical Profiles of \\nHighly-Rated Sites. ACM CHI Conference on Hu-\\nman Factors in Computing Systems . ACM Press. \\n367-374.\\nJoachims, T., & Radlinski, F. (2007). Search engines \\nthat learn from implicit feedback. IEEE Computer \\n40(8), 34-40. Kohonen T. (1998). Self-organization of very large document collections: state of the art. In Proceedings \\n8\\nth Intl Conf Artificial Neural Networks. Springer, 1. \\n65-74. \\nKwok, K., & Grunfeld, L. (1996). TREC-4 ad-hoc, \\nrouting retrieval and filtering experiments using PIRCS. In: Harman Donna (ed.). The fourth Text Retrieval Conference (TREC-4). NIST Special Publ 500-236. Gaithersburg, MY . \\nLin, J., & Demner-Fushman, D. (2006). The role of \\nknowledge in conceptual retrieval: a study in the domain of clinical medicine. In  Annual International ACM \\nConference on Research and Development in Informa-tion Retrieval (SIGIR) Seattle. ACM Press. 99-106\\nMandl, T. (2000). Tolerant Information Retrieval with Backpropagation Networks.  Neural Computing & Ap-\\nplications  9(4), 280-289.\\nMandl, T. (2006). Implementation and evaluation of a quality based search engine. In Proceedings of the 17\\nth \\nACM Conference on Hypertext and Hypermedia (HT ‘06) Odense, Denmark. ACM Press. 73-84. \\nMandl, T., & Womser-Hacker, C. (2004). A Framework \\nfor long-term Learning of Topical User Preferences in Information Retrieval. New Library World, 105(5/6) \\n184-195.\\nSavoy, J. (2003). Cross-language information retrieval: \\nexperiments based on CLEF 2000 corporaSebastiani, F. (2002). Machine Learning in Automated Text Catego-rization. ACM Computing Surveys 34(1), 1-47.\\nShen, X., Tan, B. & Zhai, C. (2005). Context-sensitive information retrieval using implicit feedback. Annual \\nInternational ACM Conference on Research and De-velopment in Information Retrieval (SIGIR), Seattle, \\nWA. ACM Press. 43-50. \\nSingh, S., & Dey Lipika (2005). A rough-fuzzy docu-\\nment grading system for customized text information retrieval. Information Processing & Management \\n41(2), 195-216. \\nZhang, D., Chen, X. & Lee, W. (2005). Text clas-\\nsification with kernels on the multinomial manifold. Annual International ACM Conference on Research and Development in Information Retrieval  (SIGIR) \\nSeattle. ACM Press. 266-273.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='31846ea6-f218-4246-9bb8-8d215131e138', embedding=None, metadata={'page_label': '191', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Artificial Intelligence for Information Retrieval\\nKEy TERMS\\nAdaptation: Adaptation is a process of modification \\nbased on input or observation. An information system \\nshould adapt itself to the specific needs of individual users in order to produce optimized results. \\nIndexing: Indexing means the assignment of terms \\n(words) which represent a document in an index. In-dexing can be carried out manually or automatically. Automatic indexing requires the elimination of stop-words and stemming. \\nInformation Retrieval:  Information retrieval is \\nconcerned with the representation and knowledge and subsequent search for relevant information within these knowledge sources. Information retrieval provides the technology behind search engines.\\nLink Analysis: The links between pages on the web \\nare a large knowledge source which is exploited by link analysis algorithms for many ends. Many algorithms similar to PageRank determine a quality or authority score based on the number of in-coming links of a page. Furthermore, link analysis is applied to identify thematically similar pages, web communities and other social structures. \\nRecommendation Systems: Actions or content is \\nsuggested to the user based on past experience collected from other users. Very often, documents are recom-mended based on similarity profiles between users. \\nTerm Expansion: Terms not present in the original \\nquery to an information retrieval system entered by the user are added automatically. The expanded query is then sent to the system again. \\nWeighting: Weighting determines the importance \\nof a term for a document. Weights are calculated using many different formulas which consider the frequency of each term in a document and in the collection as well as the length of the document and the average or maximum length of any document in the collection. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a885a284-9e16-4736-870e-3d4a9414a8aa', embedding=None, metadata={'page_label': '192', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nAArtificial Intelligence in Computer-Aided\\nDiagnosis\\nPaulo Eduardo Ambrósio\\nSanta Cruz State University, Brazil\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nProfessionals of the medical radiology area depend directly on the process of decision making in their daily activities. This process is mainly based on the analysis of a great amount of information obtained for the evaluation of radiographic images.\\nSome studies demonstrate the great capacity of \\nArtificial Neural Networks (ANN) in support systems for diagnosis, mainly in applications as pattern clas-sification.\\nThe objective of this article is to present the de-\\nvelopment of an ANN-based system, verifying its behavior as a feature extraction and dimensionality reduction tool, for recognition and characterization of patterns, for posterior classification in normal and abnormal patterns.\\nBACKGROUND\\nThe computer-aided diagnosis (CAD) is considered one of the main areas of research of the medical images and radiological diagnosis (Doi, 2005).\\nAccording to Giger (2002) “In the future, is probable \\nthat all the medical images have some form of executed CAD to benefit to the results and the patient cares”.\\nThe diagnosis of the radiologist is normally based \\non qualitative interpretation of the analyzed data, that can be influenced and be harmed by many factors, as low quality of the image, visual fatigue, distraction, overlapping of structures, amongst others (Azevedo-Marques, 2001). Moreover, the human beings possess limitations in its visual ability, which can harm the analysis of a medical image, mainly in the detection of determined presented patterns (Giger, 2002).\\nResearch demonstrates that when the analysis is \\ncarried out by two radiologists, the diagnosis sensitivity is significantly increases (Thurfjell et al., 1994). In this \\ndirection, the CAD can be used as a second specialist, when providing the computer reply as a second opinion (Doi, 2005).\\nMany works analyze the radiologist performance \\nfront the use of a CAD systems, of which we detach the research of Jiang et al.  (2001) and Fenton et al.  \\n(2007).\\nIn the development of CAD systems, techniques \\nfrom two computational areas are normally used: Computer Vision and Artificial Intelligence.\\nFrom the area of Computer Vision, techniques \\nof image processing for enhancement, segmentation and feature extraction are used (Azevedo-Marques, 2001).\\nThe enhancement objectives to improve an image \\nto make it more appropriate for a specific application (Gonzalez & Woods, 2001). In applications with digital medical images, the enhancement is important to facili-tate the visual analysis on the part of the specialist.\\nThe segmentation is the stage where the image is \\nsubdivided in parts or constituent objects (Gonzalez & Woods, 2001). The result of the segmentation is a set of objects that can be analyzed and quantified individually, representing determined characteristic of the original image.\\nThe final stage involved in image processing is the \\nfeature extraction, that it basically involves the quanti-fication of elements that compose segmented objects of the original image, such as size, contrast and form.\\nAfter concluded this first part, the quantified attri-\\nbutes are used for the classification of the structures identified in the image, normally using methods of Artificial Intelligence. According to Kononenko (2001), the use of Artificial Intelligence in the support to the diagnosis is efficient, for allowing a complex data analysis of simple and direct form.\\nMany methods and techniques of Artificial Intel-\\nligence can be applied in this stage, normally with the objective to identify and to separate the patterns in distinct groups (Theodorides & Koutroumbas, 2003), for example, normal and abnormal patterns. According to Kahn Jr (1994), among the main techniques, can be ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99665ded-cd91-4c73-8a08-9d03bad20a19', embedding=None, metadata={'page_label': '193', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  AI in Computer-Aided Diagnosis\\ncited: rule-based reasoning, artificial neural networks, \\nbayesian networks, case-based reasoning. To these, the statistical methods, the genetic algorithms and the decision trees can be added.\\nA problem that reaches most of the applications \\nof pattern recognition is the data dimensionality. The dimensionality is associated with the number of at-tributes that represent a pattern, that is, the dimension of the search space. When this space contains only the most relevant attributes, the classification process is faster and consumes little processing resources (Jain et al., 2000), and also allows for greater precision of the classifier.\\nIn the problems of medical image processing, the \\nimportance of the dimensionality reduction is accen-tuated; therefore normally the images to be processed are composed of a very great number of pixels, used as basic attributes in the classification.\\nThe feature extraction is a common boarding to ef-\\nfect the dimensionality reduction. Of general form, an extraction algorithm creates a new set of attributes from transformations or combinations of the original set.\\nSome methods are studied with the intention to \\npromote the feature extraction and, consequently, the dimensionality reduction, such as statistical methods, methods based on the signal theory, and artificial neural networks (Verikas & Bacauskiene, 2002).\\nAs example of the use of artificial neural networks \\nin the support to the medical diagnosis, we can cite the research of Papadopoulos et al. (2005) and André \\n& Rangayan (2006).\\nMAIN FOCUS OF THE ARTICLE\\nIn this paper, we also present a proposal of use of Artificial Intelligence in the stage of feature extrac -\\ntion, substituting the traditional techniques of image processing.\\nTraditionally, the feature extraction is carried out on \\nthe basis of statistical or spectral techniques, which re-sult in, for example, texture or geometric attributes.\\nAfter these attributes are obtained, techniques of \\nArtificial Intelligence are applied in the pattern clas -\\nsification.\\nOur proposal is the use of ANN also for feature \\nextraction.Feature Extraction with ANNs\\nThe feature extraction with the use of Artificial Neural Networks functions basically as a selection of charac-teristics that represent the original data set.\\nThis selection of characteristics is related to a pro-\\ncess in which a data set is transformed into a space of characteristics that, in theory, accurately describes the same information as the original space of the data. However, the transformation is projected in such a way that the data set is represented by a reduced effective characteristic, keeping most of the intrinsic informa-tion to the data, that is, the original data set suffers a significant dimensionality reduction (Haykin, 1999).\\nThe dimensionality reduction is extremely useful \\nin applications that involve digital image processing, which normally depend on a very high number of data points to be manipulated.\\nIn summary, the feature extraction with ANNs trans-\\nforms the original set of pixels into a map, of reduced dimensions, that represents the original image without a significant loss of information.\\nFor this function, self-organizing neural networks \\nare normally used, as for example, the Kohonen’s Self-Organizing Map (SOM).\\nThe self-organizing map searches ways to transform \\none determined pattern into a bi-dimensional map, following a certain topological order (Haykin, 1999). The elements that compose the map are distributed in an only layer, having formed a grid (Figure 1).\\nFigure 1. Illustrative representation of a Kohonen’ s \\nself-organizing map\\nInputs  Winner  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='765409c9-0ff2-4bfa-846a-8f76cdb228f8', embedding=None, metadata={'page_label': '194', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18AI in Computer-Aided Diagnosis\\nAAll the elements of the grid receive the input signal \\nof all variables, associated to its respective weights. \\nThe calculation of its value of exit is carried through by one determined function, on the basis of the weights of the connections, and it is used to identify the win-ning element.\\nMathematically, each element of the grid is repre-\\nsented by a vector composed of the weights of con-nection, with the same dimension of the input space, that is, the amount of elements that compose the vector corresponds to the amount of input variables of the problem (Haykin, 1999).\\nMethodology\\nAs application example, a self-organizing neural network for the feature extraction of images of chest radiographs was developed, objectifying the charac-terization of normal and abnormal patterns.\\nEach original image was divided in 12 parts, hav-\\ning as base the anatomical division normally used in the diagnosis of the radiologist. Each part is formed by approximately 250,000 pixels.\\nWith the use of the proposal self-organizing network, \\na reduction for only 240 representative elements was obtained, with satisfactory results in the final pattern classification.\\nA detailed description of the methodology can be \\nfound in (Ambrósio, 2007; Azevedo-Marques et al., \\n2007).\\nFUTURE TRENDS\\nThe developed study shows the possibilities of appli-cation of the self-organizing networks in the feature extraction and dimensionality reduction; however, other types of neural networks can also be used for this purpose. New studies need to be carried out to compare the results and adequacy of the methodology.\\nCONCLUSION \\nThe contribution of the Information Technology is undeniable as support tool to the medical decision making. The Artificial Intelligence presents itself as a great source of important techniques to be used in this direction.It can be evidenced that the technique of artificial \\nneural networks highlights its great versatility and robustness, providing sufficiently satisfactory results, when used and implemented well.\\nThe use of an automatic system of image analysis \\ncan assist the radiologist, when used as a tool of ‘second opinion’, or second reading, in the analysis of possible inexact cases.\\nIt is also observed that the use of the proposed \\nmethodology represents a significant profit in the im -\\nage processing of chest radiographs, for its peculiar characteristics.\\nREFERENCES\\nAmbrósio, P.E. (2007). Self-Organizing Neural Net-\\nworks in the Characterization of Interstitial Lung Diseases in Chest Radiographs.  Thesis. Ribeirão Preto: \\nSchool of Medicine of Ribeirao Preto, University of Sao Paulo. [In Portuguese]\\nAndré, T. C. S. S.; Rangayan, R. M. (2006). Classifi-\\ncation of breast masses in mammograms using neural networks with shape, edge-sharpness and texture fea-tures. Journal of Electronic Imaging . 15(1).\\nAzevedo-Marques, P.M. (2001). Diagnóstico Auxiliado por Computador na Radiologia. Radiologia Brasileira.  \\n34(5), 285-293. [In Portuguese]\\nAzevedo-Marques, P.M., Ambrósio, P.E., Pereira-\\nJunior, R.R., Valini, R.A. & Salomão, S.C. (2007). Characterization of Interstitial Lung Disease in Chest Radiographs using SOM Artificial Neural Network. International Journal of Computer Assisted Radiology and Surgery. 2 (suppl. 1), 368-370.\\nDoi, K. (2005). Current Status and Future Potential \\nof Computer-Aided Diagnosis in Medical Imaging. The British Journal of Radiology.  78(special issue), \\nS3-S19.\\nFenton, J. J.; Taplin, S. H.; Carney, P. A.; Abraham, \\nL.; Sickles, E. A.; D’Orsi, C.; Berns, E. A.; Cutter, G.; Hendrick, R. E.; Barlow, W. E.; Elmore, J. G. (2007). Influence of computer-aided detection on performance of screening mammography. New England Journal of \\nMedicine . 356 (14), 1399-1409.\\nGiger, M.L. (2002). Computer-Aided Diagnosis in Radiology. Academic Radiology.  9, 1-3.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e299efe9-5267-46fe-ad9d-6db3b07aa689', embedding=None, metadata={'page_label': '195', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x180  AI in Computer-Aided Diagnosis\\nGonzalez, R.C. & Woods, R.E. (2001). Digital Image \\nProcessing (2nd ed.). Reading, MA: Addison-Wes-\\nley.\\nHaykin, S. (1999). Neural Networks (2nd ed.). Engle-\\nwood Cliffs, NJ: Prentice Hall.Jain, A.K., Duin, R.P.W. & Mao, J. (2000). Statistical \\nPattern Recognition: A Review. IEEE Transactions \\non Pattern Analysis and Machine Intelligence.  22(1), \\n4-37.\\nJiang, Y .; Nishikawa, R. M.; Schimidt, R. A.; Toledano, \\nA. Y .; Doi, K. (2001) Potential of computer-aided diagnosis to reduce variability in radiologists’ interpre-tations of mammograms depicting microcalcification. Radiology . 220(3), 787-794.\\nKahn-Jr, C.E. (1994). Artificial Intelligence in Radi-ology: Decision Support Systems. RadioGraphics.  \\n14(4), 849-861.\\nKononenko, I. (2001). Machine Learning for Medical \\nDiagnosis: History, State of the Art and Perspective. Artificial Intelligence in Medicine , 23, 89-109.\\nPapadopoulos, A; Fotiadis, D. I.; Likas, A. (2005). Characterization of clustered microcalcifications in digitized mammograms using neural networks and support vector machines. Artificial Intelligence in \\nMedicine . 34(2), 141-150.\\nTheodoridis, S. & Koutroumbas, K. (2003). Pattern \\nRecognition  (2nd ed.). Amsterdam: Elsevier.\\nThurfjell, E.L., Lernevall, K.A. & Taube, A.A.S. (1994). Benefit of Independent Double Reading in a Population-Based Mammography Screening Program. Radiology. 191, 241-244.\\nVerikas, A. & Bacauskienes, M. (2003). Feature Selec-\\ntion with Neural Networks. Pattern Recognition Letters.  \\n23(11), 1323-1335.KEy TERMS\\nComputer-Aided Diagnosis:  Research area that en-\\nclose the development of computational techniques and procedures for aid to the health professionals in process of decision making for the medical diagnosis.\\nDimensionality Reduction:  Finding a reduced data \\nset, with the capacity of mapping a bigger set.\\nFeature Extraction:  Finding of representative \\nfeatures of a determined problem from samples with different characteristics.\\nMedical Images:  Images generated in special \\nequipment, used for aid to the medical diagnosis. Ex.: X-Ray images, Computer Tomography, Magnetic Resonance Images.\\nPattern Recognition: Research area that enclose \\nthe development of methods and automatized tech-niques for identification and classification of samples in specific groups, in accordance with representative characteristics.\\nRadiological Diagnosis: Medical diagnosis based \\nin analysis and interpretation of patterns observed in medical images.\\nSelf-Organizing Maps:  Category of algorithms \\nbased on artificial neural networks that searches, by means of self-organization, to create a map of char-acteristics that represents the involved samples in a determined problem.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='617d4819-9cfa-4a74-98e1-00e61e1fec66', embedding=None, metadata={'page_label': '196', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18\\nAArtificial Neural Networks and Cognitive\\nModelling\\nAmanda J.C. Sharkey\\nUniversity of Sheffield, UK\\nCopyright © 2009, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.INTRODUCTION\\nIn their heyday, artificial neural networks promised a radically new approach to cognitive modelling. The connectionist approach spawned a number of influential, and controversial, cognitive models.  In this article, we consider the main characteristics of the approach, look at the factors leading to its enthusiastic adoption, and discuss the extent to which it differs from earlier com-putational models.    Connectionist cognitive models have made a significant impact on the study of mind.  However connectionism is no longer in its prime.   Possible reasons for the diminution in its popularity will be identified, together with an attempt to identify its likely future.  \\nThe rise of connectionist models dates from the \\npublication in 1986 by Rumelhart and McClelland, of an edited work containing a collection of connec-tionist models of cognition, each trained by exposure to samples of the required tasks.  These volumes set the agenda for connectionist cognitive modellers and offered a methodology that subsequently became the standard.  Connectionist cognitive models have since been produced in domains including memory retrieval and category formation, and (in language) phoneme recognition, word recognition, speech perception, ac-quired dyslexia, language acquisition, and (in vision) edge detection, object and shape recognition.  More than twenty years later the impact of this work is still apparent.   \\nBACKGROUND\\nSeidenberg and McClelland’s (1989) model of word pronunciation is a well-known connectionist example. They used backpropagation to train a three-layer network to map an orthographic representation of words and non-words onto a distributed phonological representation, and an orthographic output represen-tation. The model is claimed to provide a good fit to experimental data from human subjects.  Humans can make rapid decisions about whether a string of letters is a word or not, (in a lexical decision task), and can readily pronounce both words and non-words. The time they take to do both is affected by a number of factors, including the frequency with which words occur in language, and the regularity of their spelling.  The trained artificial neural network outputs both a phonological and an orthographic representation of its input.  The phonological representation is taken as the equivalent to pronouncing the word or non-word.  The orthographic representation, and the extent to which it duplicates the original input, is taken to be the equivalent of the lexical decision task\\nThe past tense model (McClelland & Rumelhart, \\n1986) has also been very influential.  The model mirrors several aspects of human learning of verb endings.   It was trained on examples of the root form of the word as input, and of the past-tense form as output.   Each input and output was represented as a set of context-sensitive phonological features, coded and decoded by means of a fixed encoder/decoder network.  A goal of the model was to simulate the stage-like sequences of past tense learning shown by humans.  Young children first correctly learn the past tense of a few verbs, both regular (e.g. looked) and irregular (e.g. went, or came).  In stage 2 they often behave as though they have inferred a general rule for creating the past tense, (adding –ed to the verb stem).  But they often over-generalise this rule, and add –ed to irregular verbs (e.g  comed).  There is a gradual transition to the final stage in which they learn to produce the correct past tense form of both regular and exception words.  Thus their performance exhibits a U-shaped function for irregular verbs (initially cor-rect, then often wrong, then correct again).  \\nThe model was trained in stages on 506 English \\nverbs.  First, it was trained on 10 high frequency verbs (regular, and irregular). Then medium frequency verbs (mostly regular) were introduced and trained for a number of epochs.  A dip in performance on the ir-regular verbs occurred shortly after the introduction ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='807c51ed-1677-4d60-9dec-d6599b38b8ad', embedding=None, metadata={'page_label': '197', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Artificial Neural Networks and Cognitive Modelling\\nof the medium frequency verbs – a dip followed by \\na gradual improvement that resembled the U-shaped curve found in human performance. \\nTHE STRENGTHS AND LIMITATIONS OF CONNECTIONIST COGNITIVE MODELLING\\nThe models outlined above exhibit five typical features of connectionist models of cognition: (i) They provide an account that is related to and inspired by the opera-tions of the brain;  (ii) They can be used both to model mental processes, and to simulate the actual behaviour involved; (iii) They can provide a ‘good fit’ to the data from psychology experiments; (iv) The model, and its fit to the data, is achieved without explicit programming and (v) They often provide new accounts of the data. We discuss these features in turn. \\nFirst there is the idea that a connectionist cognitive \\nmodel is inspired by, and related to, the way in which brains work.  Connectionism is based on both the al-leged operation of the nervous system and on distrib-uted computation. Neuron-like units are connected by means of weighted links, in a manner that resembles the synaptic connections between neurons in the brain.  These weighted links capture the knowledge of the system; they may be arrived at either analytically or by “training” the system with repeated presentations of input-output training examples.  Much of the interest in connectionist models of cognition was that they offered a new account of the way in which knowledge was represented in the brain.  For instance, the behaviour of the past tense learning model can be described in terms of rule following – but its underlying mechanism does not contain any explicit rules.  Knowledge about the formation of the past tense is distributed across the weights in the network.  \\nInterest in brain-like computing was fuelled by a \\ngrowing dissatisfaction with the classical symbolic processing approach to modelling mind and its rela-tionship to the brain.  Even though theories of symbol manipulation could account for many aspects of hu-man cognition, there was concern about how such symbols might be learnt and represented in the brain.  Functionalism (Putnam, 1975) explicitly insisted that details about how intelligence and reasoning were actu-ally implemented were irrelevant.  Concern about the manipulation of meaningless, ungrounded symbols is exemplified by Searle’s Chinese Room thought-experi -\\nment (1980).   Connectionism, by contrast, offered an approach that was based on learning, made little use of symbols, and was related to the way in which the brain worked.  Arguably, one of the main contributions that connectionism has made to the study and under-standing of mind has been the development of a shared vocabulary between those interested in cognition, and those interested in studying the brain.\\nThe second and third features relate to the way in \\nwhich artificial neural nets can both provide a model of a cognitive process and simulate a task, and provide a good fit to the empirical data.  In Cognitive Psychol -\\nogy, the emphasis had been on building models that could account for the empirical results from human subjects, but which did not incorporate simulations of experimental tasks.  Alternatively, in Artificial Intel -\\nligence, models were developed that performed tasks in ways that resembled human behaviour, but which took little account of detailed psychological evidence.  However, as in the two models described here, con-nectionist models both simulated the performance of the human tasks, and were able to fit the data from psychological investigations.\\nThe fourth feature is that of achieving the model and \\nthe fit to the data without explicit handwiring.  It can be favourably contrasted to the symbolic programming methodology of Artificial Intelligence, where the model is programmed step by step, leaving room for ad hoc modifications and kludges.  The fifth characteristic is the possibility of providing a novel explanation of the data.  In their model of word pronunciation, Seidenberg and McClelland showed that their artificial neural network provided an integrated (single mechanism) account of data on both regular and exception words where pre-viously the old cognitive modelling conventions had forced an explanation in terms of a dual route.  Similarly, the past-tense model was formulated as a challenge to rule-based accounts: although children’s performance can be described in terms of rules, it was claimed that the model showed that the same behaviour could be accounted for by means of an underlying mechanism that does not use explicit rules.  \\nIn its glory days, connectionism’s claims about \\nnovel explanations of stimulated much debate.  There was also much discussion of the extent to which con-nectionism could provide an adequate account of higher mental processes.  Fodor and Pylyshyn (1988) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4062de33-c2bc-479b-8d72-6c6cd644ee09', embedding=None, metadata={'page_label': '198', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Artificial Neural Networks and Cognitive Modelling\\nAmounted an attack on the representational adequacy \\nof connectionism.  Connectionists retaliated, and in papers such as van Gelder’s (1990) the argument was made that not only could they provide an account of the structure sensitive processes underlying human language, but that connectionism did so in a novel manner: the eliminative connectionist position.  \\n Now the dust has subsided, connectionist models \\ndo not seem as radically different to other modelling approaches as was once supposed.  It was held that one of their strengths was their ability to model mental processes, simulate behaviour, and provide a good fit to data from psychology experiments without being \\nexplicitly programmed to do so.  However, there is now greater awareness that decisions about factors such as the architecture of the net, the form its representa-tions will take, and even the interpretation of its input and output, are tantamount to a form of indirect, or \\nextensional programming. \\nControlling the content, and presentation of the \\ntraining sample, is an important aspect of extensional programming. When Pinker and Prince (1988) criticised the past tense model, an important element of their criticisms was that the experimenters had unrealisti-cally tailored the environment to produce the required results, and that the results were an artifact of the train-ing data.  Although the results indicated a U-shaped curve in the rate of acquisition, as occurs with children, Pinker and Prince argued that this curve occurred only because the net was exposed to the verbs in an unreal-istically structured order. Further research has largely answered these criticisms, but it remains the case that selection of the input, and control of the way that it is presented to the net, affects what the net learns.  A similar argument can be made about the selection of input representations.  \\nIn summary: there has been debate about the novelty \\nof connectionism, and its ability to account for higher level cognitive processing.  There is however general acknowledgement that the approach made a lasting con-tribution by indicating how cognitive processes could be implemented at the level of neurons.  Despite this, the connectionist approach to cognitive modelling is no longer as popular as it once was.   Possible reasons are considered below: \\n• Difficult challenges: A possible reason for the \\ndiminished popularity of artificial neural nets is \\nthat as Elman (2005) suggests, “we have arrived at the point where the easy targets have been identified but the tougher problems remain”.  Difficult challenges to be met include the idea of scaling up models to account for wider ranges of phenomena, and building models that can account for more than one behaviour. \\n• Greater understanding: As a result of our greater \\nunderstanding of the operation and inherent limitations of artificial neural nets, some of their attraction has faded with their mystery.   They have become part of the arsenal of statistical methods for pattern recognition, and much recent research on artificial neural networks has focused more on questions about whether the best level of generalisation has been efficiently achieved, than on modelling cognition.  \\nAlso there is greater knowledge of the limitations of \\nartificial neural nets, such as the problem of the “cata -\\nstrophic interference” associated with backpropagation.  Backpropagation performs impressively when all of the training data are presented to the net on each training cycle, but its results are less impressive when such training is carried out sequentially and a net is fully trained on one set of items before being trained on a new set.  The newly learned information often interferes with, and overwrites, previously learned information.  For instance, McCloskey and Cohen (1989) used back-propagation to train a net on the arithmetic problem of + 1 addition (e.g. 1+1, 2+1, …, 9+1).  They found that when they proceeded to train the same net to add 2 to a given number, it “forgot” how to add 1.   Sequential training of this form results in catastrophic interfer-ence.  Sharkey and Sharkey (1995) demonstrated that it is possible to avoid the problem if the training set is sufficiently representative of the underlying function, or there are enough sequential training sets.  In terms of this example, if the function to be learned is both + 1 and + 2, then training sets that incorporate enough examples of each could lead to the net learning to add either 1 or 2 to a given number.  However, this is at the expense of being able to discriminate between those items that have been learned from those that have not.  \\nThis example is related to another limitation of ar-\\ntificial neural nets: their inability to extrapolate beyond their training set.  Although humans can readily grasp the idea of adding one to any given number, it is not so straightforward to train the net to extrapolate beyond the data on which it is trained.  It has been argued ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f955ddc-11b4-44fe-873b-c55b09cd7204', embedding=None, metadata={'page_label': '199', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\x18\\x18\\x18  Artificial Neural Networks and Cognitive Modelling\\n(Marcus, 1998) that this inability of artificial neural \\nnets trained using backpropagation to generalise beyond their training space provides a major limitation to the power of connectionist nets: an important one, since humans can readily generalise universal relationships to unfamiliar instances.  Clearly there are certain aspects of cognition, particularly those to do with higher level human abilities, such as their reasoning and planning abilities, that are more difficult to capture within con -\\nnectionist models.   \\n• Changing zeitgest: There is now an increased \\ninterest in more detailed modelling of brain func-\\ntion, and a concomitant dissatisfaction with the simplicity of cognitive models that often consisted of “a small number of neurons connected in three rows” (Hawkins, 2004).  Similarly, there is greater impatience with the emphasis in connectionism on the biologically implausible backpropaga-tion learning algorithm. At the same time, there is greater awareness of the role the body plays in cognition, and the relationships between the body, the brain, and the environment (e.g. Clark, 1999).  Traditional connectionist models do not fit easily with the new emphasis on embodied cognition (e.g. Pfeifer and Scheier, 1999). \\nFUTURE TRENDS\\nOne expected future trend will be to focus on the difficult challenges.  It is likely that investigations will explore how models of isolated processes, such as learning the past tense of verbs, might fit into more general accounts of language learning, and of cognition.  There are fur-ther questions to be addressed about the developmental origin of many aspects of cognition, and the origin and progression of developmental disorders.\\nConnectionist cognitive modelling is likely to \\nchange in response to the new zeitgest.  A likely scenario is that artificial neural nets will continue to be used for cognitive modeling, but not exclusively as they were before.  They will continue to form part of hybrid ap-proaches to cognition (Sun, 2003), in combination with symbolic methods. Similarly, artificial neural nets can be used in combination with evolutionary algorithms to form the basis of adaptive responses to the environ-ment.  Rather than training artificial neural nets, they can be adapted by means of evolutionary methods, and used as the basis for robotic controllers (e.g. Nolfi and Floreano, 2000).  Such changes will ensure a future for connectionist modeling, and stimulate a new set of questions about the emergence of cognition in response to an organism’s interaction with the environment.  \\nCONCLUSION\\nIn this article we have described two landmark con-nectionist cognitive models, and considered their char-acteristic features.  We outlined the debates over the novelty and sufficiency of connectionism for modelling cognition, and argued that in some respects the approach shares features with the modelling approaches that preceded it.  Reasons for a gradual waning of interest in connectionism were identified, and possible futures were discussed.  Connectionism has had a strong impact on cognitive modelling, and although its relationship to the brain is no longer seen as a strong one, it provided an indication of the way in which cognitive processes could be accounted for in the brain.  It is argued here that although the approach is no longer ubiquitous, it will continue to form an important component of future cognitive models, as they take account of the interactions between thought, brains and the environment.\\nREFERENCES\\nClark, A. (1997) Being there: Putting brain, body and \\nworld together again .  Cambridge, MA: MIT Press\\nElman, J.L.. (2005) Connectionist models of cogni-tive development: Where next?  Trends in Cognitive \\nSciences , 9, 111-11\\nFodor, J.A. & Pylyshyn, Z. (1988) Connectionism and cognitive architecture: A critical analysis.  Cognition , \\n28, 3-71.\\nHawkins, J. (2004) On intelligence .  New York: Henry \\nHolt and Company, Owl Books.Marcus, G.F. (1998) Rethinking eliminative connec-\\ntionism.  Cognitive Psychology, 37, 243-282.\\nMcClelland, J.L. & Rumelhart, D.E. & the PDP Re-search Group Parallel Distributed Processing Vol 2: \\nPsychological and Biological Models . Cambridge, \\nMA: MIT Press. (1986) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0103f399-9508-4197-8fff-bff21af7f3f0', embedding=None, metadata={'page_label': '200', 'file_name': 'AI.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/AI.pdf', 'file_type': 'application/pdf', 'file_size': 2477716, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\x18\\x18\\x18Artificial Neural Networks and Cognitive Modelling\\nAMcCloskey, M. & Cohen, N.J. (1989) Catastrophic \\ninterference in connectionist networks: The sequential learning problem.  The Psychology of Learning and \\nMotivation , V ol 24, 109-165.\\nNolfi, S. and Floreano, D. (2004) Evolutionary robotics: \\nThe biology, intelligence and technology of self-orga-nising machines.  Cambridge, MA: MIT Press\\nPinker, S. & Prince,A. (1988) On language and con-\\nnectionism: Analysis of a parallel distributed processing model of language acquisition. In Connections and \\nsymbols (S., Pinker and J. Mehler, Eds), Cambridge, MA: Bradford/MIT Press pp 73-194\\nPfeifer, R. and Scheier, C. (1999) Understanding intel-\\nligence . Cambridge, MA: MIT Press\\nPutnam, H. (1975) Philosophy and our mental life.  In \\nH. Putnam (Ed.) Mind, language and reality: Philo-\\nsophical papers (vol 2) Cambridge, UK: Cambridge University Press, pp 48-73\\nRumelhart, D.E. and McClelland, J.L. (1986) On \\nlearning the past tenses of English verbs.  In Parallel \\nDistributed Processing: Explorations in the Microstruc-ture of Cognition, Vol 2. Psychological and Biological Models.  (Rumelhart, D.E. and McClelland, J.L., eds) pp 216-271, MIT Press\\nSeidenberg, M.S. and McClelland, J.L. (1989) A distrib-\\nuted developmental model of visual word recognition and naming.  Psychological Review , 96, 523-568.\\nSearle, J.R. (1980) Minds, brains and programs.  Be-\\nhavioural and Brain Sciences,  3: 417-424.  Reprinted \\nin J. Haugeland. Ed. Mind design.  Montgomery, VT: \\nBradford Books, 1981. \\nSharkey, N.E. & Sharkey, A.J.C. (1995) An Analysis \\nof Catastrophic Interference, Connection Science , 7, \\n3/4, 313-341.\\nSharkey, A.J.C. and Sharkey, N. (2003) Cognitive \\nModelling: Psychology and Connectionism.  In M.A. Arbib (Ed) The Handbook of Brain Theory and Neural \\nNetworks, A Bradford Book: MIT Press\\nSun, R. (2003) Hybrid Connectionist/Symbolic Sys-\\ntems.  In M.A. Arbib (Ed) The Handbook of Brain \\nTheory and Neural Networks, A Bradford Book: MIT \\nPress pp 543-547van Gelder, T. (1990) Compositionality: A Connection-ist variation on a classical theme.  Cognitive Science, \\n14, pp 355-364.\\nKEy TERMS\\nChinese Room: In Searle’s thought experiment, \\nhe asks us to imagine a man sitting in a room with a number of rule books.  A set of symbols is passed into the room.   The man processes the symbols according to the rule books, and passes a new set of symbols out of the room.  The symbols posted into the room correspond to a Chinese question, and the symbols he passes out are the answer to the question, in Chinese.  However, the man following the rules has no knowl-edge of Chinese.   The example suggests a computer program could similarly follow rules in order to answer a question without any understanding.  \\nClassical Symbol Processing: The classical view \\nof cognition was that it was analogous to symbolic computation in digital computers.  Information is repre-sented as strings of symbols, and cognitive processing  involves the manipulation of these strings by means of a set of rules.  Under this view, the details of how such computation is implemented are not considered important. \\nConnectionism: Connectionism is the term used to \\ndescribe the application of artificial neural networks to the study of mind. In connectionist accounts, knowledge is represented in the strength of connections between a set of artificial neurons.  \\nEliminative Connectionism: The eliminative \\nconnectionist is concerned to provide an account of cognition that eschews symbols, and operates at the subsymbolic level.  For instance, the concept of “dog” could be captured in a distributed representation as a number of input features (e.g. four-footed, furry, barks etc) and would then exist in the net in the form of the weighted links between its neuron like units.\\nGeneralisation:  Artificial neural networks, once \\ntrained, are able to generalise beyond the items on which they were trained and to produce a similar output in response to inputs that are similar to those encountered in training\\nImplementational Connectionism: In this less \\nextreme version of connectionism, the goal is to find ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6302b279-fcd4-4f00-8f37-51e0c01592b1', embedding=None, metadata={'page_label': '1', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Univ ersity of Windsor Univ ersity of Windsor \\nScholarship at UWindsor Scholarship at UWindsor \\nElectr onic Theses and Disser tations Theses, Disser tations, and Major P apers \\n2023 \\nCompar ative Study of Gener ative Models for T ext-t o-Image Compar ative Study of Gener ative Models for T ext-t o-Image \\nGener ation Gener ation \\nNazia Siddiqui \\nUniv ersity of Windsor \\nFollow this and additional works at: https:/ /scholar .uwindsor .ca/etd \\n Part of the Artificial Intelligence and Robotics Commons \\nRecommended Citation Recommended Citation \\nSiddiqui, Nazia, \"Compar ative Study of Gener ative Models for T ext-t o-Image Gener ation \" (2023). Electr onic \\nTheses and Disser tations . 8954. \\nhttps:/ /scholar .uwindsor .ca/etd/8954 \\nThis online database contains the full-text of PhD disser tations and Masters’ theses of Univ ersity of Windsor \\nstudents fr om 1954 for ward. These documents ar e made a vailable for personal study and r esear ch purposes only , \\nin accor dance with the Canadian Cop yright Act and the Cr eativ e Commons license—CC B Y-NC-ND (A ttribution, \\nNon-Commer cial, No Deriv ative Works). Under this license, works must alwa ys be attributed t o the cop yright holder \\n(original author), cannot be used for any commer cial purposes, and ma y not be alter ed. Any other use would \\nrequir e the permission of the cop yright holder . Students ma y inquir e about withdr awing their disser tation and/or \\nthesis fr om this database. F or additional inquiries, please contact the r eposit ory administr ator via email \\n(scholarship@uwindsor .ca) or b y telephone at 519-253-3000ext. 3208. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5b40d77-af2a-4ea1-8345-f8bc635619a5', embedding=None, metadata={'page_label': '2', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Comparative Study of Generative Models\\nfor Text-to-Image Generation\\nBy\\nNazia Siddiqui\\nA Thesis\\nSubmitted to the Faculty of Graduate Studies\\nthrough the School of Computer Science\\nin Partial Fulfillment of the Requirements for\\nthe Degree of Master of Science\\nat the University of Windsor\\nWindsor, Ontario, Canada\\n2023\\n©2023 Nazia Siddiqui', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='205a206b-537d-43eb-b4f2-ce6fe6cb217c', embedding=None, metadata={'page_label': '3', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Comparative Study of Generative Models for Text-to-Image Generation\\nby\\nNazia Siddiqui\\nAPPROVED BY:\\nM. Khalid\\nDepartment of Electrical and Computer Engineering\\nB. Boufama\\nSchool of Computer Science\\nI. Ahmad\\nSchool of Computer Science\\nJanuary 19, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15831428-68d3-43b3-80ab-cbd934c20116', embedding=None, metadata={'page_label': '4', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DECLARATION OF ORIGINALITY\\nI hereby certify that I am the sole author of this thesis and that no part of this\\nthesis has been published or submitted for publication.\\nI certify that, to the best of my knowledge, my thesis does not infringe upon\\nanyone’s copyright nor violate any proprietary rights and that any ideas, techniques,\\nquotations, or any other material from the work of other people included in my\\nthesis, published or otherwise, are fully acknowledged in accordance with the standard\\nreferencing practices. Furthermore, to the extent that I have included copyrighted\\nmaterial that surpasses the bounds of fair dealing within the meaning of the Canada\\nCopyright Act, I certify that I have obtained a written permission from the copyright\\nowner(s) to include such material(s) in my thesis and have included copies of such\\ncopyright clearances to my appendix.\\nI declare that this is a true copy of my thesis, including any final revisions, as\\napproved by my thesis committee and the Graduate Studies office, and that this thesis\\nhas not been submitted for a higher degree to any other University or Institution.\\nIII', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='63f4c581-9d51-4bb9-ba9f-bfb0f82ea29d', embedding=None, metadata={'page_label': '5', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ABSTRACT\\nThe development of deep learning algorithms has tremendously helped computer\\nvision applications, image processing methods, Artificial Intelligence, and Natural\\nLanguage Processing. One such application is image synthesis, which is the cre-\\nation of new images from text. Recent techniques for text-to-image synthesis offer\\nan intriguing yet straight forward conversion capability from text to image and have\\nbecome a popular research topic. Synthesis of images from text descriptors has prac-\\ntical and creative applications in computer-aided design, multimodal learning, digital\\nart creation, etc. Non-Fungible Tokens (NFTs) are a form of digital art that is being\\nused as tokens for trading across the globe. Text-to-image generators let anyone with\\nenough creativity can develop digital art, which can be used as NFTs. They can\\nalso be beneficial for the development of synthetic datasets. Generative Adversarial\\nNetworks (GANs) is a generative model that can generate new data using a training\\nset. Diffusion Models are another type of generative model which can create desired\\ndata samples from the noise by adding random noise to the data and then learning to\\nreverse the diffusion process. This thesis compares both models to determine which is\\nbetter at producing images that match the given description. We have implemented\\nthe Vector-Quantized GAN (VQGAN) - Connecting Text and Images (CLIP) model.\\nIt combines the VQGAN and CLIP machine learning techniques to create images\\nfrom text input. The diffusion model that we have implemented is Guided Language\\nto Image Diffusion for Generation and Editing (GLIDE). For both models, we use\\ntext input from the MS-COCO data set. This thesis is an attempt to assess and\\ncompare the images generated using text for both models using metrics like Inception\\nScore (IS) and Fr´ echet Inception Distance (FID). The semantic object accuracy score\\n(SOA) is another metric that considers the caption used during the image generation\\nprocess. We compute and compare the results for each label in the MS COCO data\\nset. We highlight the potential causes of why the models may not be able to generate\\nimages through analysis of the results obtained. Our experimental results indicate\\nthat the GLIDE model outperforms the VQGAN - CLIP for our task of generating\\nIV', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70c9f776-68a8-42f1-82ff-7b92a2687f83', embedding=None, metadata={'page_label': '6', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='images from text.\\nV', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da302e78-9fcc-4dac-948a-8af743befe93', embedding=None, metadata={'page_label': '7', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='AKNOWLEDGEMENTS\\nI would like to extend my sincere gratitude towards my supervisor Dr. Imran Ah-\\nmad for his patience, support, and professional guidance during my graduate studies.\\nWith his input, I was able to look at my research with a different perspective and a\\nmore critical eye. I want to take this opportunity to express my gratitude to my thesis\\ncommittee members for their beneficial advice and suggestions for my thesis. I want\\nto thank my internal reader, Dr.Boufama and my external reader, Dr. Mohammed\\nKhalid, for their time, effort, and valuable feedback. Dr.Robin Grass for being on my\\ncommittee at short notice and always being easy to access and willing to help.\\nI am greatly indebted to my parents for their support and words of inspiration\\nthroughout my education. I would also like to thank my sister, Farheen Siddiqui\\nfor always encouraging and supporting me. I would also like to mention my friends\\nAsim, Karan, Puneet, Suraj and Liya for making university life an unforgettable\\nexperience for me. I humbly extend my thanks to the School of Computer Science\\nand all concerned people who helped me in this regard.\\nVI', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a86912d-aba3-4e22-9c48-a611fda7d2ba', embedding=None, metadata={'page_label': '8', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE OF CONTENTS\\nDECLARATION OF ORIGINALITY III\\nABSTRACT IV\\nAKNOWLEDGEMENTS VI\\nLIST OF TABLES IX\\nLIST OF FIGURES X\\nLIST OF ABBREVIATIONS XII\\n1 Introduction 1\\n1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Text-to-image Generation and its Applications . . . . . . . . . . . . 2\\n1.3 Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.4 Thesis Objective and Contribution . . . . . . . . . . . . . . . . . . . 4\\n1.5 Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2 Related Work 6\\n2.1 Overview of different Generative Models . . . . . . . . . . . . . . . . 6\\n2.1.1 Generative Adversarial Networks (GAN) . . . . . . . . . . . . 6\\n2.1.2 Variational Autoencoders (VAE) . . . . . . . . . . . . . . . . 7\\n2.1.3 Flow-based Model . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.1.4 Diffusion Model . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Overview of GAN based Approach for Text to Image Synthesis . . . . 10\\n2.3 Overview of Diffusion Model Approach for Text to Image Synthesis . 16\\n2.4 Comparison of Generative Models . . . . . . . . . . . . . . . . . . . . 18\\n2.5 Data sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n3 Methodology 21\\n3.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n3.2 Proposed Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n3.2.1 VQGAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n3.2.2 CLIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n3.2.3 VQGAN - CLIP . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n3.2.4 GLIDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n3.3 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n3.4 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n3.4.1 Inception Score (IS) . . . . . . . . . . . . . . . . . . . . . . . 29\\n3.4.2 Frechet inception distance (FID) . . . . . . . . . . . . . . . . 30\\n3.4.3 Semantic Object Accuracy (SOA) . . . . . . . . . . . . . . . . 31\\nVII', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0f82a7b-cab2-40e4-a0ff-4a813a7f5693', embedding=None, metadata={'page_label': '9', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 Experiments and Results 33\\n4.1 Implementation and Tools . . . . . . . . . . . . . . . . . . . . . . . . 33\\n4.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n4.2.1 Inception Score Results . . . . . . . . . . . . . . . . . . . . . . 34\\n4.2.2 FID Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n4.2.3 SOA Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n4.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n5 Conclusion and Future Work 49\\n5.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\nREFERENCES 51\\nVITA AUCTORIS 56\\nVIII', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='783ddf60-5d5c-4b47-b3cd-fade4c79da3b', embedding=None, metadata={'page_label': '10', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='LIST OF TABLES\\n4.2.1 Overall IS Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n4.2.2 SOA Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nIX', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9ed35b1-7261-46e4-9107-6b382d11678a', embedding=None, metadata={'page_label': '11', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='LIST OF FIGURES\\n2.2.1 GAN Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.2.1 Process Flowchart . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n3.2.2 VQGAN Architecture [10] . . . . . . . . . . . . . . . . . . . . . . . . 23\\n3.2.3 VQGAN Architecture [10] . . . . . . . . . . . . . . . . . . . . . . . . 24\\n3.2.4 VQGAN - CLIP Architecture [5] . . . . . . . . . . . . . . . . . . . . 26\\n4.2.1 Inception Scores for VQGAN - CLIP and GLIDE . . . . . . . . . . . 34\\n4.2.2 Set of books sitting next to a small black clock. . . . . . . . . . . . . 35\\n4.2.3 A bird sitting on the ground. . . . . . . . . . . . . . . . . . . . . . . 35\\n4.2.4 A cat sleeping in a red handbag. . . . . . . . . . . . . . . . . . . . . 36\\n4.2.5 A dog is sitting between two large potted plants. . . . . . . . . . . . 36\\n4.2.6 FID Scores for VQGAN - CLIP and GLIDE . . . . . . . . . . . . . . 37\\n4.2.7 A brown teddy bear sitting next to bottles of person care items. . . 38\\n4.2.8 A variety of donuts and pastries in a box. . . . . . . . . . . . . . . . 39\\n4.2.9 A large hairy sheep standing on a lush green field. . . . . . . . . . . 39\\n4.2.10 A table topped with a couple of sandwiches and a bowl of soup. . . . 39\\n4.2.11 A desk with a computer monitor and a keyboard. . . . . . . . . . . . 40\\n4.2.12 Trucks driving down a steep narrow dirt road. . . . . . . . . . . . . . 41\\n4.2.13 A big airplane flying in the big blue sky. . . . . . . . . . . . . . . . . 41\\n4.2.14 Two people are using a hair drier on a small dog. . . . . . . . . . . . 41\\n4.2.15 A yellow school bus parked in a parking lot. . . . . . . . . . . . . . . 42\\n4.2.16 Some suitcases and a hat laying on a carpeted floor. . . . . . . . . . 42\\n4.2.17 A bowl of veggie soup with a spoon in it. . . . . . . . . . . . . . . . 42\\n4.2.18 A room with a bed that has white and red pillows. . . . . . . . . . . 43\\n4.2.19 A apple with a bite in it on a table. . . . . . . . . . . . . . . . . . . 43\\n4.2.20 Two park benches that are overlooking a valley. . . . . . . . . . . . . 44\\n4.2.21 SOA Scores for VQGAN - CLIP and GLIDE . . . . . . . . . . . . . 44\\nX', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2711982a-8e51-4754-9da8-d57358e929c2', embedding=None, metadata={'page_label': '12', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2.22 Generated Images: Pizza . . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.2.23 Generated Images: Cake . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.2.24 Generated Images: Bananas . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.2.25 Generated Images: Stopsign . . . . . . . . . . . . . . . . . . . . . . . 46\\n4.3.1 A boy swinging a baseball bat at a ball. . . . . . . . . . . . . . . . . 48\\n4.3.2 Generated Image: A person eating pizza and salad at a table. . . . . 48\\nXI', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a2b856d-dd58-40de-a9b1-99e79eea3b1b', embedding=None, metadata={'page_label': '13', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='LIST OF ABBREVIATIONS\\nGAN Generative Adversarial Network\\nDRAW Deep Recurrent Attentive Writer\\nFID Frechet Inception Distance\\nIS Inception Score\\nNFT Non-Fungible Tokens\\nVAE Variational Autoencoder\\nMS COCO Microsoft Common Objects in Context\\nSOA Semantic Object Accuracy\\nMNIST Modified National Institute of Standards and Technology\\nDC-GAN Deep Convolutional Generative Adversarial Network\\nCLIP Contrastive Language- Image Pretraining\\nCLIP-GLaSS CLIP-guided Generative Latent Space Search\\nDDPM Denoising Diffusion Probabilistic Models\\nDDIM Denoising Diffusion Implicit Models\\nGLIDE Guided Language to Image Diffusion for Generation and Editing\\nCUB-200 Caltech-UCSD Birds-200-2011\\nVQGAN Vector Quantized Generative Adversarial Network\\nSOA-I Semantic Object Accuracy - Image\\nSOA-C Semantic Object Accuracy - Class\\nXII', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11547726-378c-44b1-b41d-cec6b05081fb', embedding=None, metadata={'page_label': '14', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 1\\nIntroduction\\n1.1 Overview\\nGenerative models have recently gained attention for their use in producing fake\\nimages. The emergence of artificial intelligence (AI)-generated fake images, referred\\nto as “deep fakes” presents several challenges such as developing synthetic images\\nthat look realistic, images with multiple objects, and reliable evaluation metrices\\nthat align with human judgment [13]. However, new technologies appear promising\\nfor image generation. Developing a system that can create images representing a\\ngiven textual description inspired by how humans perceive is a significant step towards\\ncomputer intelligence [13]. Prior to the development of generative models, the process\\nof creating an image from text relied on image querying. Which involved selecting the\\nbest collection of images from an image database to illustrate text description. Recent\\nadvances in artificial intelligence and computer vision have facilitated the creation of\\nimages based on text descriptions. The goal of text-to-image synthesis is to generate\\nimages from textual descriptions. Research on text-to-image creation has sparked\\nmuch interest due to its applications in art, marketing, business, and education,\\namong others. Several frameworks and improvements have been proposed to produce\\nmore visually realistic images. The representation of an image as the numeric values of\\nits pixels is termed as image data distribution. If there is an image with the dimensions\\nmxnpixels. The image may be interpreted as a vector with mxndimensions, which\\nis high dimensional data distribution. Image data distribution of high dimension\\nmakes image creation a complex problem. The generated images should be visually\\n1', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f15319d-f2e2-4a48-ad7e-ff9d4329920d', embedding=None, metadata={'page_label': '15', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. INTRODUCTION\\nrealistic and semantically accurate for adequate text-to-image synthesis. Semantic\\naccuracy refers to the agreement between the content of the image and the text\\ndescription. Generative modelling is when we provide the model with a description of\\nwhat we want to generate, and the model returns an image. The model automatically\\nlearns from the input data and replicates it with variety and accuracy. Suppose we\\nhave a description using which we try to generate an image. The generated image\\nwill resemble but not be identical to the input sample image because the model uses\\ninput images to learn the image’s representation. That is why the representation is\\nunique each time and varies for different models.\\nThere are different types of generative models like Autoencoders [16, 21], Gener-\\native Adversarial Network (GAN) [16], and Diffusion models [35] for text-to-image\\ngeneration that several authors have introduced over the years. These models have\\nbeen compared based on visual realism, diversity, and semantic alignment to under-\\nstand which model works better at generating an image related to the text used for\\nits generation [9].\\n1.2 Text-to-image Generation and its Applica-\\ntions\\nAfter being trained on image data, experiments have shown that producing fake but\\nphotorealistic images is feasible. Ramesh et al. [32] showed text-to-image generation\\nbased on an autoregressive transformer in terms of zero-shot learning. Zero-shot\\nlearning implies creating samples for the text input without being trained on the same\\ninput. Scaling can result in more accurate generalization compared to earlier domain-\\nspecific techniques and the approach achieves the best frechet inception distance\\n[44] and the highest inception score [1] when qualitatively comparing samples from\\nproposed model to those from prior work [32]. This approach is used by the popular\\ntext-to-image generation tool DALL.E [9], which became available for public use by\\nOpenAI shortly after the diffusion model was introduced. The diffusion model has\\n2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de3162d8-17eb-4d77-bdd2-8090b7acf7c8', embedding=None, metadata={'page_label': '16', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. INTRODUCTION\\nbeen proven to generate high-quality images and give desirable characteristics like\\ndistribution coverage, a stationary training aim of creating images from text, and\\nsimple scalability [9]. With these improvements, they achieve a new state-of-the-art,\\noutperforming GANs on a variety of metrics and data sets [9]. DALL.E was released\\nin January 2021, followed by DALL-E 2, which was released later in November 2022.\\nMeanwhile, diffusion models gained popularity in the vision community. OpenAI chose\\nthis method as the basis for DALL-E 2 because it uses simple image-denoising nets\\nto reduce a convex regression loss instead of a minmax.\\nDALL.E and other generative AI picture tools are the latest innovation that ven-\\nture capitalists have been eager to try out. The use of non-fungible tokens (NFTs),\\na kind of digital asset that can be in the form of digital art, has skyrocketed. NFT\\nartworks are already fetching millions of dollars [4]. With enough imagination, peo-\\nple can create digital art using text-to-image generators, which can be utilized to\\ncreate NFTs. Creating realistic graphics from natural language can enable people\\nto produce rich and varied visual content. There has been impressive progress in\\nthe first few years since Mansimov et al. [24] started with modern machine learning\\napproaches for text-to-image synthesis [32]. The approach has numerous commercial,\\neducational, and artistic applications. One of the commercial applications is the ex-\\npensive production of video games and animated films, in which many production\\nartists are employed to perform very mundane tasks. Text-to-image models can cre-\\nate and colourize characters automatically by giving their descriptions. The tool can\\nbe used to generated images for books and for teaching which makes visual learning\\neasier and more accessible. The authors of short story books can use the tool for\\ncreating images related to the stories. The artists can look up for inspiration for\\ntheir art and use images that are original. Developers can generate images and use\\nthem for their websites or applications without having to spend money for images\\nor any copyright issues. The image generation can also be used for creating medical\\nimage data set which can be used for training for tasks like detection of diseases.\\nSome other uses can also be to add variation to already existing data sets, generate\\nrelevant images from chatbot interactions, and fulfill the scarcity of image data sets.\\n3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2703af48-b02f-4642-bc7f-f511e559aa10', embedding=None, metadata={'page_label': '17', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. INTRODUCTION\\nThe application possibilities for text-to-image generators in the future are limitless.\\n1.3 Generative Models\\nIn terms of a probabilistic model, a generative model specifies how a data set can be\\nproduced. We can generate fresh data by sampling from this model. For example,\\nconsider a data set that includes pictures of cars. We want to create a model that,\\nafter learning the fundamental principles governing a car’s appearance, can create\\na brand-new image of a car that has never existed but still appears realistic. This\\nrequires a data set containing many sample images of the object that is to be created\\n[12]. Each of the samples in the training data is referred to as an observation. Each\\nobservation is made up of a variety of features. For an image generating issue, the\\nfeatures are typically the various pixel values. The model trained on the observations\\nis able to produce new collections of features that appear to have been produced by\\nusing the same set of rules as the original data. The resulting photos will consist of a\\nnew collection of pixels that have been rearranged so that the item is identifiable as the\\nsame object, but is not identical to the original observation. Given the vast number\\nof possible pixel assignments and the small number of feasible image layouts, this is a\\nchallenging task for image synthesis. Additionally, a generative model should produce\\nprobabilistic results rather than predetermined outcomes [12]. If a model just does a\\nfixed calculation, such as averaging the value of each pixel from the data set, it is not\\ngenerative because it constantly produces the same output [12]. The model should\\nbe able to learn an approximation of the input distribution and then sample from it\\nto produce new, separate observations that appear similar to the initial training set\\n[12].\\n1.4 Thesis Objective and Contribution\\nSynthesizing images from text descriptions has lately gained attention as a research\\ntopic due to the development of generative models. There are versatile methods for\\n4', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cec44228-aa65-4dfa-86c8-797a20acdc88', embedding=None, metadata={'page_label': '18', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. INTRODUCTION\\nconditional image generation that have made major advancements in recent years in\\nterms of visual realism, diversity, and semantic alignment [13]. However, there are\\nstill several issues that the area must address through additional research, such as\\nmaking it possible to generate high-resolution photos with several objects and creating\\neffective evaluation metrics that are correlated with human judgment. One of these\\nchallenges is being able to tell how closely the image we make matches the text we use\\nto generate it. The performance of a model depends on the specific task we are using\\nit for. The assessment of generative models is crucial to understand where further\\nresearch is required. In this thesis, we are comparing to draw conclusion upon how the\\ntwo types of models, GANs and diffusion models, perform when we generate images\\nover the same textual data from the MS COCO [22] data set and compare them\\nusing different available metrices. This thesis complements other previous research\\nby using a new metric, i.e., semantic object accuracy (SOA) [18], which to the best\\nof our knowledge has not been employed to compare GANs and the diffusion model.\\nThe evaluation will also reveal how the models behave for different types of objects.\\nWe discuss and analyze the reasons why a generative model is able and not able to\\ngenerate images for a certain object. The aim of this research is to see how the models\\nperform and which one is preferable when it comes to generating images from text.\\n1.5 Thesis Organization\\nRest of the thesis is organized as follows:\\nChapter 2 describes the literature on evolving text-to-image generation algorithms.\\nChapter 3 describes our methodology in detail. Chapter 4 discusses the experiments\\nwe have conducted and their outcomes. Finally, Chapter 5 discusses some future\\ndirections and concludes our work.\\n5', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a510fda-d991-4273-95ec-def139ef9258', embedding=None, metadata={'page_label': '19', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 2\\nRelated Work\\nIn this section, we briefly describe several previous research that this thesis is built\\nupon:\\n2.1 Overview of different Generative Models\\nThere are four major categories to classify generative models. Which include GANs,\\nvariational autoencoders (VAE’s), flow-based models, and diffusion models. We will\\nbe discussing all of them in this section.\\n2.1.1 Generative Adversarial Networks (GAN)\\nGenerative Adversarial Networks (GANs) have a generator neural network that cre-\\nates what should be a realistic image from noise or from some useful conditioning\\nvariable, such as a class label or text encoding. The success is determined by the dis-\\ncriminator, which classifies the images as either a real image or a false image produced\\nby the generator. GANs have shown that it is possible to create fake but highly real-\\nistic images. To create images from text using GANs, the simplest way to train is to\\ntreat text, image pairs as joint observations and train the discriminator to distinguish\\nbetween real and fake pairs [26]. If the model is trying to generate the sentence “A\\nyellow car”, here the conditioning information is the yellow colour of the car. Early\\nin training, the discriminator disregards the conditioning information and dismisses\\nsamples from generator because they don’t seem realistic. The generator must get\\nbetter at aligning the images it creates, like the “car” in this example. As soon as\\n6', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b001a1d-8cf6-4278-b9a3-e6a00d9dcb9b', embedding=None, metadata={'page_label': '20', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nit becomes adept at creating convincing images, it must also learn to match those\\nimages to the conditioning data [34]. There are different ways each model handles\\nthe learning of this conditioning information. A common machine learning task is to\\nlearn a density model, i.e., generating an estimate of a distribution based on observed\\ndata. For the density model of any object ‘X’, it would accept some input and say yes\\nthat’s ‘X’ or no that’s not ‘X’. GANs have been used to learn density models. Zhang\\net al. [45] employed a collection of images with accompanying text labels as their\\ntraining data, and the objective is to create a conditional density model by instead\\nlooking if there is a object ‘X’ given the condition ‘Y’. Which allows us to define the\\nfeatures that is the conditional information that we desire in the resulting image.\\nFor GAN to understand the text sentence, the model basically encodes the text\\ndescription into an embedding which represents the text sentence and is used by the\\ndiscriminator to identify ‘real image with right text’ , ‘real image and wrong text’ and\\n‘fake image and right text’. The weights are adjusted accordingly until the generator\\nis able to trick the discriminator into believing that the generated fake image is real.\\nAgain, the way the text is encoded is varied for each model and is significant for\\nperformance of the model.\\nGiven the advancement and research on GANs, much efficient and improved mod-\\nels have been introduced that are far more effective at creating realistic-looking images\\nfrom text. Reed et al. [34] introduced Stack GAN, demonstrating that their model\\ncould generate a photorealistic image from any text sentence. The model consists of\\ntwo stages. In the first stage, it takes a sentence as input and outputs an image with\\nprimitive shapes and basic colours creating a low-resolution image. The stage 2 of\\nGAN takes that low-resolution image and the original sentence as input and generates\\na much higher resolution version of that image by filling in all the details.\\n2.1.2 Variational Autoencoders (VAE)\\nA variational autoencoder (VAE) offers a probabilistic way to describe an observation\\nin latent space. They take the input and encode it, often compressing it to a latent\\nspace of lower dimensionality [29]. The main goal of autoencoders is to efficiently\\n7', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5487a1ce-e72c-41de-aa3d-9784e0920997', embedding=None, metadata={'page_label': '21', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nrepresent the data. Their task is to identify a low-dimensional representation of a\\nhigh-dimensional input that enables reconstruction of the original input with little\\ncontent loss. If the image is 784 pixels in size (28 x 28 pixels) in grayscale, the\\nautoencoder would discover a way to map the 784-dimensional space onto a 2D space\\nso that the compressed picture would only need to describe the X and Y coordinates\\nof a location on the map. The autoencoder would next attempt to recreate the\\noriginal 784 pixels using only the X-Y coordinates as input. Making a low-dimensional\\nrepresentation that enables the autoencoder to recreate the original input is its task.\\nThis makes sure that the latent space only contains the input features that are most\\nimportant for reconstructing the input and is free of noise and unimportant features.\\nIt must have two components: the encoder, which takes the input image and reduces it\\nto a low-dim representation, and the decoder, which performs the opposite operation\\nby creating the original-sized image from the latent representation [29].\\n2.1.3 Flow-based Model\\nThe class of models known as “flow-based models”, explicitly learn the data dis-\\ntribution. Flow-based models learn specific encoders and decoders: Similar to the\\nencoding stage in autoencoders, they apply a transformation “f”, parametrized by a\\nneural network, to the data. However, the decoder is not a brand-new neural network\\nthat must independently learn the decoding procedure and it is the exact opposite\\nof the function. With neural networks, it takes quite a few methods to obtain this\\ninvertibility of “f” [42].\\n2.1.4 Diffusion Model\\nThe Diffusion model learns to establish a Markov chain of diffusion steps to gradually\\nintroduce random noise to data, and then they learn to reverse the diffusion process\\nto create desired data samples from the noise [42]. In contrast to VAE or flow models,\\ndiffusion models are trained using a predefined process, and the latent variable has\\na high degree of dimension [42]. A Markov chain is a set of variables where each\\n8', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a1d5047-aff5-42a4-8587-1d4d7c4b22e5', embedding=None, metadata={'page_label': '22', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nvariable’s state solely depends on the previous event. Using a Markov chain, the\\ndata is contaminated with random noise. Taking the image, we sequentially add a\\nparticular amount of noise to it during the forward diffusion phase. At each step the\\nmodel generates the new image in the series by gradually increasing the noise and we\\nrepeat this a specific number of times until the image becomes noise. The diffusion\\nprocess is reversed using a neural network. To create the image from diffusion step\\ntto step t−1, the backward diffusion method uses the same network and weights\\nat each stage. Instead of letting the network anticipate the image, one might decide\\nto forecast the noise that has to be removed from the image at each step to further\\nsimplify the issue. In any event, the neural network’s design must be chosen in a\\nway that maintains the dimensionality of the data. There isn’t much room for error\\nwhen returning from noise to the original image by repeatedly denoising. Similar\\nto GANs, the generation process passes through each of these checkpoints, adding\\nmore and more information to the image that was once just noise but compared to\\nGANs, diffusion models are a more gradual, iterative, and controlled approach. The\\nfollowing iteration, which was introduced by Nichol et al. [26], successfully incor-\\nporated textual information into the generation procedure, enabling us to generate\\nimages using diffusion models from text. The data set consisting of images and their\\ncaptions is used. The images became noisier and noisier after the forward diffusion\\nprocess. Similar to the prior research from OpenAI, the diffusion model is trained\\nto reverse this process using a UNet-based architecture. For the backward diffusion,\\nthe model takes the text caption into account as well. The authors took the text,\\ntransformed it using a transformer, and then used the resulting token embedding as\\na class-conditioning in the diffusion model. Each layer of the model’s attention layer\\nalso pays attention to every text token that the transformer generates as it encodes\\nthe text. To increase the text’s persuasiveness in terms of image formation from\\ntext, the authors, Nicole et al. [26] experimented with Contrastive Language-Image\\nPre-Training (CLIP)-guided diffusion. The concept behind this is to utilize a second\\nmodel to improve how well the generated image matches the text. The additional\\nmodel in this case is CLIP, a program from OpenAI that has been trained to estimate\\n9', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c510caa5-94b2-4d0c-848e-de7036318695', embedding=None, metadata={'page_label': '23', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nthe similarity of an image and a text. Before generating an image with CLIP-guided\\ndiffusion, the model is initially used to denoise the image based on text. They also\\nadd a gradient of CLIP’s image-sentence similarity to the image, this shifts the initial\\ndenoised image in the direction that CLIP predicts will result in a strong image text\\nmatch and is called as classifier-guided diffusion. The classifier-free advice employed\\nby the authors is another approach and it worked better for them. As the name\\nalready implies, no additional model is required. It is an unique approach employed\\nat each level of diffusion to accentuate the text even more. The image is created\\ntwice by the model, once with access to the text and once without. Then, using the\\ndifference between the diffusion step with text and without text. This difference is\\nused to determine the way to proceed in order to transition from no-text to text. The\\noutput of the model without text information is, therefore, strongly projected in the\\ndirection of text information if we take the text-less generation and add this differ-\\nence scaled by a large amount. Although Guided Language to Image Diffusion for\\nGeneration and Editing (GLIDE) has approximately four times less parameters than\\nDALL-E and was trained using the same data as DALL-E, it excels in photorealism\\n[26]. The majority of participants in the human evaluation trials clearly favoured\\nGLIDE’s generations to DALL- E’s fuzzier and messier outputs [26].\\n2.2 Overview of GAN based Approach for Text to\\nImage Synthesis\\nBefore GANs, Mansimov et al. [24] used a recurrent neural network to create images\\nfrom text captions. It focused on creating the image in multiple steps, similar to\\nDRAW by Gregor et al. [16]. Reed et al. [34] later demonstrated improved image\\ngeneration using a generative adversarial network rather than a recurrent variational\\nauto-encoder [45]. Progress was made over the next few years using various methods,\\nwhich included modifying the generative model architecture. The work done since\\nMansimov et al. [24] has resulted in appreciable improvements in visual quality.\\n10', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbb01755-031a-417c-88f0-41cdcffacb5f', embedding=None, metadata={'page_label': '24', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nHowever, severe defects such as object deformation, incorrect item placement, or an\\nunnatural blending of foreground and background elements can still appear in some\\nexamples [32].\\nConstructing image pixels from text human written descriptions is a complex task.\\nIn order to address the challenge, we need to understand how to learn text feature\\nrepresentations that will have visual details and to use these features to generate\\nimages that are real enough to trick a person. Image generation using generative\\nadversarial networks (GANs) was introduced by Goodfellow et al. [15] . The GANs\\nconsist of a generator and a discriminator that play the minmax game [15]. This\\nminimax game helps them by challenging each other and, thus, training them to be\\nable to generate better images. While at first, the samples are not good and are\\nrejected with confidence by the discriminator, the generator trains itself to be better\\nover time [15].\\nFig. 2.2.1: GAN Architecture\\nThe Minimax Game [15]:\\nmin\\nGmax\\nDV(D, G) =Ex∼pdata (x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))] (1)\\nThe generator and the discriminator are competing against each other during the\\ntraining process. While the generator is attempting to deceive, the discriminator\\nis attempting not to be deceived. When combining both aspects together, a min-\\nmax game is placed between the generator(G) and the discriminator(D) with value\\n11', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1aded3f8-beb1-4f75-a2ce-aa35bd142fd4', embedding=None, metadata={'page_label': '25', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nfunction V. While the discriminator is trained, it classifies both the real and fake data\\nfrom the generator.\\nxis data representing an image. pz(z) is the input noise variable, and G(z) is\\ngeneration of data with random noise zas input. D(x) is trained to identify gen-\\nerated and real instance. It represents the probability that xcame from the data\\nrather than generator. The model penalizes itself for misclassifying a real instance as\\nfake, or a fake instance (created by the generator) as real, by maximizing equation\\n1. The likelihood that the generator correctly classifies the real image is denoted\\nbylog(D(x)). Maximizing log(1−D(G(z)) would aid in correctly labelling the gen-\\nerated fake image. The generator’s loss is then calculated from the discriminator’s\\nclassification, and it gets rewarded if it successfully fools the discriminator and pe-\\nnalized otherwise. Generated instances serve as negative training examples for the\\ndiscriminator, which learns to distinguish between fake and real data and penalizes\\nthe generator for producing unconvincing results.\\nIn terms of creating synthetic images from real-world images, generative adver-\\nsarial networks (GANs) have demonstrated promising outcomes [15]. GAN must first\\nbe trained to produce high-resolution, photorealistic images from text descriptions,\\nwhich is an extremely challenging task. In modern GAN models, simply adding more\\nupsampling layers to generate high-resolution (e.g., 256 ×256) pictures typically leads\\nto training instability and provides meaningless outputs [45].\\nGregor et al. [16] presented a Deep Recurrent Attentive Writer (DRAW) neural\\nnetwork design for generating images. The encoder network compresses the real\\nimages shown during training, while the decoder network reconstructs images after\\nreceiving the codes. These two recurrent neural networks are the foundation of the\\nDRAW architecture. The encoder and decoder are both recurrent networks, which\\nallows them to exchange a series of code samples. Additionally, the encoder has\\naccess to the decoder’s prior outputs, allowing it to modify the codes it transmits in\\naccordance with the decoder’s past behavior. Instead of emitting this distribution all\\nat once, the outputs of the decoder are sequentially added to the distribution that will\\nfinally create the data. The input region that the encoder observes and the output\\n12', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='189fb1f1-39e5-42ae-beb8-1d9dcdb47dd3', embedding=None, metadata={'page_label': '26', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nregion that the decoder modifies are both constrained by a dynamically updated\\nattention mechanism. MNIST [7] database is a massive collection of handwritten\\ndigits. The experiments demonstrated the models ability to outperform MNIST image\\ngeneration and generate realistic images.\\nMansimov et al. [24] introduced the AlignDRAW model, which uses a soft atten-\\ntion method to create a generative model of images from captions. The alignDRAW\\nmodel used a deterministic laplacian pyramid adversarial network [8] to refine the\\nimages produced by the model in a post-processing stage. This approach was built\\non an extension of the Deep Recurrent Attention Writer (DRAW) [16], which repeat-\\nedly creates patches while paying attention to the pertinent description words [24].\\nThe model, which combines an alignment model over words with a recurrent vari-\\national autoencoder, was successful in producing pictures that match a given input\\ncaption. The model benefited from various factors due to the intensive use of atten-\\ntion processes. In other words, by using the visual attention mechanism, the authors\\nwere able to break down the challenge of creating images into a series of steps rather\\nthan a single forward pass, and by using attention over words, they were able to gain\\ninsight whenever the model was unable to produce a pertinent image. Furthermore,\\nthe algorithm produced images with captions that went outside the training set, such\\nas unexpected circumstances that are extremely unlikely to occur in real life.\\nReed, Akata, Yan, et al. [34] proposed a text-to-image generation approach that\\naims to scale up the model to higher resolution images and add more types of text.\\nThe model was able to combine a variety of logical visual interpretations of a given\\ntext caption [34]. The study demonstrated how style and content could be separated,\\nas well as the posture and backgrounds could be transferred from images to written\\ndescriptions [34]. The text characteristics are stored via a hybrid character-level con-\\nvolutional recurrent neural network and then trained into a deep convolutional gen-\\nerative adversarial network (DC-GAN). The discriminator is taught to differentiate\\nbetween authentic and fraudulent image-text combinations. The text characteristics\\nare stored via a hybrid character-level convolutional recurrent neural network and\\nthen trained into a DC-GAN. Feed-forward inference is carried out by the generator\\n13', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc4043a0-f2c7-490a-93c5-9dd7f2f106ee', embedding=None, metadata={'page_label': '27', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nnetwork and discriminator network based on the text feature. The model is unique\\nin that it is entirely a GAN rather than employing GAN just for post-processing, and\\ncan frequently produce aesthetically convincing 64 ×64 pictures conditioned on text.\\nPrior to encoding the text query using a text encoder, it samples from the noise in\\nthe generator. A fully connected layer is utilized to compress the description em-\\nbedding to a minimal dimension, which is then followed by leaky-ReLU before being\\nconcatenated to the noise vector z. Then, inference happens as it normally would\\nin a deconvoluted network. Pass it forward to the generator to create the synthetic\\nimage x. Using query text and a noise sample as inputs, the generator performs feed-\\nforward inference to generate images. Multiple layers of stride-2 convolution with\\nspatial batch normalization and leaky ReLU are used in the discriminator. To reduce\\nthe description’s dimensionality following correction by embedding it in another fully\\nconnected layer. Lastly to compute the final score from discriminator, use a 1 ×1\\nconvolution followed by rectification and a 4 ×4 convolution. With the help of the\\nfindings from the MS-COCO data set, it was shown how generalized the method was\\nfor creating images with various objects and changing backgrounds.\\nFollowing research suggested the use of several stacked generators to enable text-\\nto-image models to synthesize better quality images [13]. In StackGAN [45], the\\nauthors introduced a brand-new stacked generative adversarial network to generate\\nphotorealistic images from text descriptions. It greatly enhances the state of the\\nart by breaking down the challenging task of producing high-resolution pictures into\\nsmaller, more manageable subproblems [45]. The first stage of StackGAN [45] uses\\na text conditioning vector and a random noise vector to build a coarse 64 ×64 pixel\\npicture. A second generator takes this initial image and the text embedding and\\ngenerates a 256 ×256 pixel image. A discriminator is taught to tell the difference\\nbetween image-text pairings that match and which do not at both phases [13]. The\\nauthors present easy-to-use but powerful stacked generative adversarial networks to\\ngenerate high-resolution images with photorealistic features. It divides the process of\\ngenerating images from text into two steps [45]. Stage-I GAN creates a low-resolution\\nimage by first sketching the basic shape and colours of the item based on the text\\n14', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6f76f460-1258-497f-ae2d-d4e0cc63a556', embedding=None, metadata={'page_label': '28', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\ndescription provided and then laying out the backdrop using a random noise vec-\\ntor. Stage-II GAN creates a high-resolution, photo-realistic image by fixing flaws in\\nthe low-resolution image from Stage-I and finishing the object’s features by reading\\nthe text description once more. Zhang et al. [45] further enhanced the architec-\\nture (StackGAN++) with an end-to-end framework in which three generators and\\ndiscriminators are collaboratively trained to simultaneously approximate the multi-\\nscale, conditional, and unconditional image distributions [13]. Instead of using the\\nfixed text embedding produced by a pre-trained text encoder, the authors of At-\\ntnGAN [43] used Skip-Thought vectors [21] and StackGAN [45]. The authors also\\ndemonstrated the inferiority of conventional text representations like Word2Vec which\\nis a algorithm that uses a neural network model to find connections between words\\nin a large text corpus. The latent variable is sampled at random from a Gaussian\\ndistribution, where the text embedding determines the mean and covariance matrix.\\nMany of the text-to-image methods that came after this one utilized this method.\\nOpenAI launched a revolutionary deep neural network that learns visual ideas via\\nnatural language supervision. Two encoders make up CLIP (Contrastive Language-\\nImage Pretraining), one for texts and one for images. It is sufficient for each image\\nto verify whether the text description “a photo” or “photo of X” is more likely to be\\nmatched with the image of an item X in an image collection [14]. The authors Galatolo\\net al. [14] introduced a CLIP-based framework called CLIP-guided Generative Latent\\nSpace Search (CLIP-GLaSS), which is used to produce the best images matching a\\ntarget caption. After being explored by a genetic algorithm, this ideal image (or text)\\nis generated by a generative network. Models like StyleGAN2 and generator GPT2\\nhave been used in preliminary experiments of the proposed CLIP-guided generative\\nlatent space search (CLIP-GLaSS) for the text-to-image tasks. The results show that\\nthe suggested framework has a lot of potential in terms of the quality and usefulness\\nof the picture or text that comes out, which calls for more comparative research.\\n15', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93e86e8b-d10a-48a2-af2e-18bda223cc66', embedding=None, metadata={'page_label': '29', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\n2.3 Overview of Diffusion Model Approach for Text\\nto Image Synthesis\\nDiffusion models are a new category of cutting-edge generative models that produce\\na variety of high-resolution pictures. There are currently a variety of models based on\\ndiffusion. The initial denoising diffusion technique was developed by Sohl-Dickstein\\net al [39]. They create a technique that simultaneously provides adaptability and\\nmanageability. The basic idea, which comes from non-equilibrium statistical physics,\\nis to use an iterative forward diffusion process to systematically and gradually get rid\\nof structure in a data distribution. Then, they discovered a reverse diffusion process\\nthat recovers data structure, resulting in a highly flexible and tractable generative\\ndata model. This method permits the quick learning, sampling, and evaluation of\\nprobabilities in models with hundreds of layers or time steps, as well as the computa-\\ntion of conditional and posterior probabilities under the learned model. The result is\\nan algorithm that can learn to fit any data distribution, is easy to train, test, and cre-\\nate samples from, and makes it easy to change conditional and posterior distributions\\n[39].\\nIn diffusion models, a distribution is sampled by reversing a slow noise process.\\nEach timestep tcorresponds to a different level of noise, x0is signal and xtis a\\nsignal mixed with noise, with the signal-to-noise ratio determined by the timestep t\\n[10]. Sohl-Dickstein et al. [39] suggested diffusion probabilistic models that learn to\\nreverse a laborious, multi-step noise process to suit a data distribution [27]. Ho et al.\\n[19] developed a novel explicit relationship between diffusion models and denoising\\nscore matching, which resulted in a reduced, weighted variational bound objective for\\ndiffusion models. The diffusion model creates a Markov chain of latent variables x1,...\\nxTby gradually adding Gaussian noise q(x0) to a sample from the data distribution\\nx0[26].\\nq(xt|xt−1) :=N(xt;√αtxt−1,(1−αt)I) (2)\\n16', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6878de56-e538-41a8-a154-3a52ec6ebab6', embedding=None, metadata={'page_label': '30', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nThe posterior q(xt−1|xt) is clearly defined if the magnitude 1– α1..αTof the noise\\nsupplied at each step is large enough [26]. xtis well approximated by N. These\\nqualities propose learning p(xt−1|xt) to approximate posterior by the below given\\nequation 3 [26].\\npθ(xt−1|xt) :=N(µθ(xt),Σθ(xt)) (3)\\nIt is possible to use it to generate samples of the form x0∼pθ(x0) by beginning\\nwith Gaussian noise of the form xt∼ N(0, I) and then progressively lowering the noise\\nin the form of a series of steps xt−1, xt−2, ..., x 0[26]. To create samples xt∼q(xt|x0)\\nby adding Gaussian noise to x0in order to compute this surrogate goal, and then\\ntrain a model to anticipate the additional noise using a common mean-squared error\\nloss.\\nDiffusion models seem to have good inductive biases for image data. The best re-\\nsults came from training on a weighted variational bound that was made based on a\\nnew connection between diffusion probabilistic models and denoising score matching\\nwith Langevin dynamics. The models naturally allowed a progressive lossy decom-\\npression scheme that can be thought of as a generalization of autoregressive decoding.\\nAccording to Ho et al. [19], the basic mean-squared error goal performs better in real-\\nity than the actual variational lower bound determined by interpreting the denoising\\ndiffusion model as a VAE [9]. Denoising Diffusion Probabilistic Models (DDPM)\\nare an attractive choice for generative modeling since they combine log-likelihoods,\\nhigh-quality samples, and reasonably fast sampling with a well-grounded, stationary\\ntraining objective [27]. The authors, Nichol et al. [27], found that DDPMs can match\\nthe sample quality of GANs while achieving much better mode coverage as measured\\nby recall. Denoising diffusion implicit models (DDIMs), introduced by Song et al.\\n[40], are an implicit generative model trained with denoising auto-encoding and score\\nmatching objectives. It can generate high-quality samples much more efficiently than\\nexisting DDPMs. The non-markovian forward process shown here seems to suggest\\ncontinuous forward processes that are not gaussian, which can not be done in the\\noriginal diffusion framework.\\n17', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54eed3a8-7064-4070-951a-bdccabef0678', embedding=None, metadata={'page_label': '31', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\nNichol et al. [26] introduced Guided Language to Image Diffusion for Generation\\nand Editing (GLIDE) with classifier-free guidance that is capable of generalizing to\\na wide variety of text prompts. The model can generate realistic shadows and reflec-\\ntions, as well as high-quality textures. It is also capable of producing illustrations in\\nvarious styles, such as the style of an artist or painting. GLIDE outperforms classifier-\\nfree guidance when it comes to matching the prompt. The model achieves competi-\\ntive FID on MS-COCO without ever explicitly training on this dataset. GLIDE was\\ntrained with roughly the same training computation as DALL-E but with a much\\nsmaller model. Diffusion models have been improved in various recent papers. Ope-\\nnAI, Nvidia, and Google have successfully trained large-scale models, which have\\nreceived considerable interest. GLIDE [26], DALLE-2 [31], and Imagen [36] are com-\\nplete open-source tools that are examples of designs that are based on diffusion models\\n[20].\\n2.4 Comparison of Generative Models\\nMost of the previous research has mostly concentrated on analyzing generative models\\nof single or few object scenarios or facial features. Comparatively less work has been\\ndone to evaluate how well the generative model can generate scenes with a higher\\nlevel of complexity [34]. The development of automatic metrics that are consistent\\nwith human judgment and that enable rankings of models that are accurate as well as\\nmeaningful is a problem that must be met [9]. Frolov et al. [13] presented a review of\\nthe generative adversarial methods with emphasis on text-to-image synthesis. These\\nnew areas of research includes the creation of improved data sets and evaluation\\nmetrices to the possibility of advancements in architectural design and model training.\\nIn addition to this, they investigated the most frequently assessment methods in\\norder to evaluate image quality and the image-text alignment. The use of automated\\nmeasures such as the Inception score [1], Frechet inception distance [3], and SOA [18]\\nhas made the process of evaluating models much more straightforward.\\nCrowson et al. [5] employed human subjects who were asked to assign a score\\n18', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0482eef-be5c-480b-b778-3f1e444d1be4', embedding=None, metadata={'page_label': '32', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\non a scale ranging from one (low) to five (high) to indicate how well text and image\\ncombinations are aligned. They were told to rate higher-quality pictures that didn’t\\nmatch the prompt lower than lower-quality pictures that did align with the text\\ncaption. By calculating the average score per text caption used for each model,\\nthey determined that, the participants believe that the generations that utilized their\\nmodel are more aligned with the text captions.\\nUsing SOA introduced by Hinz et al. [18], evaluation of several state-of-the-art\\napproaches demonstrated that no method used at the moment can provide accurate\\nbackground elements for the 80 classes in the MS COCO data set. Some models are\\nadequate for common objects but they all fail for unusual objects or those without an\\neasily recognizable appearance. Using SOA to evaluate text-to-image models offers\\nmore precise information about how well they perform for different object classes or\\nimage captions and is aligned with human evaluation.\\n2.5 Data sets\\nDeep learning algorithms require large, high-quality data sets to be successful. Oxford-\\n102 Flowers [28] and CUB-200 Birds [41] are two of the most popular text-to-image\\ndata sets. They are relatively simple to use and contain approximately 10,000 im-\\nages with five human-generated captions per image. Both the CUB-200 Birds and\\nthe Oxford-102 Flowers data sets are also referred to as “single item data sets” be-\\ncause they contain only one item per image [18]. CelebA-HQ [23] is another data\\nset with a single object. The captions or code to replicate for the CelebA-HQ data\\nset are not open-sourced yet [18]. MS COCO, on the other hand, contains 123,000\\nimages, where each image may contain multiple objects, and there are associated\\nsingle-sentence captions [13]. Over time, generative models have progressed from\\ngenerating single-object picture data sets like the CUB-200 data sets to multi-object\\nimage data sets like the MS COCO. The more advanced models are utilizing data\\nfrom the internet which gives them an upper hand on previously trained models. The\\ntype of output expected from the generative model depends on the data we used to\\n19', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='366e8147-0355-4994-9011-08ef8f8e9f8e', embedding=None, metadata={'page_label': '33', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. RELATED WORK\\ntrain it. A model trained on the data set for CUB-200 birds won’t be able to generate\\nimages of faces. It’s not necessary for the input caption used for the generation to\\nmatch with the training data set. Although the model is expected to manage the\\nvariety of text input and generate images, it can not generate an object that it has\\nnot been trained on.\\n20', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='65987e4b-dae4-4167-9e84-723fa7edf3ed', embedding=None, metadata={'page_label': '34', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 3\\nMethodology\\n3.1 Motivation\\nWithin the last five years or so, the area of text-to-image generation has evolved\\nconsiderably and quickly in terms of the complexity of the data sets employed, the\\nresolution of the generated images, and their quality. This technology marks a sig-\\nnificant shift since it eliminates the need for technical work and manpower in the\\ncreation of images. Instead, they look for original thinking and curatorial judgement.\\nThe long-term implications are hard to predict, but these algorithms point to a new,\\ndemocratic way of expressing ideas that may lead to a huge increase in the number\\nof pictures made by humans, just like the camera and digital camera did [11].\\nFor the majority of image generating tasks, GANs have been the most advanced\\ntechnique. AlignDRAW, a variational recurrent autoencoder built upon DRAW pro-\\nposed by Gregor et al. [16], is not entirely based on the generative adversarial network\\napproach, but it uses GAN to only sharpen the generated image. We now have various\\nversions of GAN available with much improvement in performance for image genera-\\ntion and the quality of the generated image. GANs are sometimes challenging to train\\nbecause they collapse in the absence of precisely chosen hyperparameters [9]. Later,\\nthe diffusion models were introduced, which turned out to be better than GANs at\\ncreating images from text [13, 9]. However, changes were made to the GAN model,\\nwhich led to a variation of GAN outperforming diffusion models by making better-\\nlooking images than earlier, less flexible methods [5]. This field has also developed\\nquantitative evaluation metrics that are used to evaluate the quality of text-to-image\\n21', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eaa6b17e-02ec-40e2-83fc-3ac26a5cee34', embedding=None, metadata={'page_label': '35', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\nsynthesis models. Frechet inception distance [3] and Inception Score [1] are examples\\nof metrices used to evaluate generative models. While some papers have not em-\\nployed all possible metrics and others haven’t employed any quantitative metrics at\\nall, there are other metrics like Semantic Object Accuracy (SOA) that evaluate the\\nmodel while also taking the text used to generate it into account. In this thesis, we\\nhave set up a way to evaluate these two different generative models. This evaluation\\ntakes into account the quality of the image and how well it fits with the meaning of\\nthe text.\\n3.2 Proposed Method\\nAfter reviewing the preceding chapters about generative models, in this section we\\nnow seek to investigate the modification of the GAN model which is VQGAN - CLIP\\nand the diffusion model GLIDE that we are going to use in our comparative analysis.\\nWe will also shine some light on text encoders, and the data set used. The methodol-\\nogy for the evaluation comprises of three steps: we start with the implementation of\\nboth the aforementioned models, then progress onto the generation of image data sets\\nutilising captions from the MS COCO data set. We then leverage these generated\\nimages to assess the model’s capacity to produce high-quality images and analyze\\nthese images against the captions from the aforementioned data set.\\nFig. 3.2.1: Process Flowchart\\nWe use the generated samples for the evaluation of both the models using metrics\\nlike Inception Score, FID and SOA which will be discussed further in this chapter.\\n22', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e014f71e-d00a-4175-bee4-d0c9148f4e4b', embedding=None, metadata={'page_label': '36', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\n3.2.1 VQGAN\\nVQGAN is a GAN modification that produces images with higher visual quality\\nthan diffusion models [5]. It consists of two machine learning models combined and\\nwas introduced by Esser et al. [10]. Fig. 3.2.3 shows the graph integrating GAN,\\nVQ-VAE, and transformers to construct the VQ-GAN model. The approach aims\\nto understand both the long-range dependencies between the phrases in a sentence\\nand the visual elements of an image. They have employed transformers to teach\\ndependencies, and a GAN to learn the visual components. The transformers scale\\nquadratically and calculate the pairwise inner product between each pair of tokens\\nbecause of the attention mechanism [25]. It uses this technique to discover long-term\\nrelationships between tokens or visual components.\\nFig. 3.2.2: VQGAN Architecture [10]\\nTo encode the feature map of the visual portions of the images, the feature map\\nof the image data is first directly supplied to a GAN. Then, this image data is “vector\\nquantized,” a type of signal processing that organizes vector groups into clusters that\\nmay be accessed by a representative vector designating the centroid and is referred to\\nas a “codeword” (Algorithm. 3.2.1). The vector quantized data is encoded and stored\\nas a codebook, or dictionary of codes, as depicted in Fig. 3.2.2. The image data is\\n23', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e3771271-e31e-40de-9ba9-889b3ca3dca0', embedding=None, metadata={'page_label': '37', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\nfirst represented by the codebook, which serves as an intermediary representation,\\nand is then entered as a sequence into a transformer [25]. After that, the transformer\\nis trained to simulate the composition of these encoded sequences as high-resolution\\nimages. The generator follows an encoder-decoder architecture. This setup is similar\\nto autoencoders, where the goal is to have a decoder properly reconstruct the input\\n[25]. If the reconstruction is perfect, then the encoder has found a good way to\\nrepresent the data.\\nFig. 3.2.3: VQGAN Architecture [10]\\n3.2.2 CLIP\\nThe CLIP model was developed to evaluate how well a caption fits with an image\\nwhen compared to the other captions in the collection. Since CLIP is capable of\\nzero-shot learning, it can function successfully even with untested data [30]. CLIP\\nis particularly good at determining whether a picture and a brief bit of text go to-\\ngether or not. It is not limited to the classes in the data set but knows mostly all\\nenglish terms, allowing it to formulate ImageNet classes into prompts that include\\nmore language than simply the classes. It can model a concept based on its meaning,\\nexpanding its capabilities far beyond a limited set of classes and demonstrating ex-\\ncellent zero-shot performance on previously unseen data sets [30]. As a result, CLIP\\nnever loses or forgets additional components of the image that were not caught in\\nclass, such as the wall or sky in the background, because it is never constrained dur-\\n24', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47151db6-f749-4b2e-9e78-852c392791a4', embedding=None, metadata={'page_label': '38', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\ning training to compress an image to a single notion or word [17]. The model takes\\na set of images and a collection of textual descriptions that go with them. Both the\\ntext and the images are encoded using either a ResNet or a Transformer [17]. The\\nencoder then computes an image vector for each image in the batch, with the first\\nimage corresponding to vector I1, and the subsequent images to vector In. The same\\nis true for the textual description, with the first textual sequence being encoded by\\none vector T1and the subsequent descriptions by Tn. Between each of these image\\nand text vectors, n squared similarities are calculated. As a result, since I1fits with\\nT1,I2fits with T2, and so on, CLIP should maximize the diagonal elements. The\\noff-diagonal elements should be reduced in a contrastive manner since we believe it is\\nhighly improbable that an image I1would fit with any random description other than\\nits own. CLIP’s can predict similarities between images and textual descriptions, as\\nwell as contrastive training. When these image and text encoders are transformed, a\\nlot of computations can run in parallel [17]. CLIP has been trained to forecast high\\nsimilarity for suitable image-text pairs and low similarity for random ones, and it is\\nready to be applied to a variety of tasks, including image recognition.\\n3.2.3 VQGAN - CLIP\\nWhen combined, VQGAN - CLIP (Fig.3.2.4) develops the model that can be used to\\nproduce images from text. CLIP can assess the quality of generated images compared\\nagainst a user input caption, and the output scores can be used as weights to guide\\nthe learning of the VQGAN to match the subject matter more accurately through\\nrecursive iteration [25].\\n25', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0954be6c-95c0-4e24-85b7-463487f069b3', embedding=None, metadata={'page_label': '39', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\nFig. 3.2.4: VQGAN - CLIP Architecture [5]\\nAlgorithm 3.2.1 Algorithm of the VQGAN - CLIP Model [5]\\nInput: image i, text t, number of steps S\\nInitialize: encoder( e), decoder( d), generator( N), discriminator( D), Vector Quantiza-\\ntion Module( V Q)\\ni→N(e,d) →g(i)\\nVQ(g(i)) →codebook\\ncodebook →transformer (to learn interaction between visual features)\\ncodebook →decoder →reconstructed (i)\\nupdate reconstruction loss\\nupdate VQ loss\\nreconstructed (i)→D→Real or Fake\\nUpdate D loss\\nreconstructed (i)→CLIP\\nt→CLIP (similarity score) →average loss\\nupdate N(e,d) with average loss\\nRepeat for multiple epochs ( S) until the models converge\\nThe VQGAN initially creates a random noise image that is vector quantized and\\nencoded in a codebook to create these images. The codebook is then used as input\\nto a transformer that produces the new image from the encoded signals. The output\\nis then used to assess the image’s accuracy to the input prompt using CLIP, and the\\nscoring is then sent back to the VQGAN to update the image generation model to\\nreflect the prompt more closely.\\n26', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='020037f9-5137-4906-a423-87431e0b7973', embedding=None, metadata={'page_label': '40', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\n3.2.4 GLIDE\\nGLIDE is trained on a 3.5 billion parameter diffusion model that uses a text encoder\\nto condition on natural language descriptions [26]. Using human and automated\\nevaluations, the authors found that using classifier-free guidance yields higher quality\\nimages [26]. The model can render a wide variety of text prompts, but it can have\\ndifficulty producing realistic images for complex prompts [26]. Nichol et al. [26]\\ntrained a 3.5 billion parameter text-conditional diffusion model at 64 ×64 resolution\\nfor primary experiments, and a text-conditional upsampling diffusion model with 1.5\\nbillion parameters and increased resolution to 256 ×256. According to Dhariwal\\nand Nichol [9], samples from class-conditional diffusion models can frequently be\\nenhanced with classifier guidance. The CLIP guidance model has been trained to\\nuse classifier guidance on GLIDE [26]. For fast sampling, the base model employs\\n100 diffusion steps, followed by another 27 for upsampling. GLIDE leverages its\\nown implementation of the diffusion model to perform its image generation. Forward\\ndiffusion and reverse diffusion are the two stages of its operation. During the backward\\ndiffusion process, the model learns to reverse the effect of the added noise on the\\nimages and guide the generated image towards its original form [26].\\nAlgorithm 3.2.2 Algorithm of the GLIDE Model [26]\\nInput: noised image n(i), text t, number of steps S\\nOutput: image i\\ninitial image = n(i)\\nobjects( o), attributes( a)←transformer (t)\\nfor each-step in range(1, S):\\npredictions (pred)←DiffusionStep (i)\\ndiff←CLIP (pred, o, a )\\nn(i)←update (i, diff )\\nreturn i\\nThe model starts with a noise image and a text description as input. GLIDE offers\\nsome level of control over the result of the picture production process by parsing the\\ntext input prompts. This is accomplished by training the transformer model on a\\nsizable data set made up of pictures and the descriptions that go with them [38].\\nThe text is first converted into a string of tokens, which are then used as conditions.\\n27', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33028994-0c8d-4ecd-bbd9-2c0f485dd3e4', embedding=None, metadata={'page_label': '41', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\nThe transformer model is then fed these tokens. There are then two uses for the\\ntransformer’s output. The class embedding for the diffusion model is first replaced\\nwith the final token embedding obtained after conversion. Embedding is translating\\nsomething from high dimensional space to low dimensional space. The last layer\\nof the token embeddings, which is a series of feature vectors, is then independently\\nprojected to each attention layer’s dimensions in the model and concatenated to each\\nattention layer’s context [38]. This information is then used to guide the generation of\\nthe image, by iteratively updating the pixels in the noise image based on the desired\\nproperties specified in the text description (Algorithm 3.2.2). At each iteration, the\\nmodel uses a set of steps to process the image and make predictions about the desired\\nproperties. The model then compares these predictions to the desired properties\\nspecified in the text description, and adjusts the pixels in the image accordingly.\\nThis process is repeated until the generated image is deemed photorealistic enough,\\nor until a maximum number of iterations has been reached.\\nThis enables the model to create an image from novel combinations of related\\ntext tokens in a distinctive and photorealistic way by using its learned understanding\\nof the input words and their associated images. This text-encoding transformer has\\nover 1.2 billion parameters and employs 24 building blocks, each with a size of 2048\\n[38]. The up-sampler diffusion model, which includes 384 base channels and a text\\nencoder with a size of 1024 and has around 1.5 billion parameters.\\n3.3 Dataset\\nWe are using the MS COCO data set for our evaluation. We choose all image captions\\nfrom the MS COCO validation set that specifically refer to one of the 80 major object\\ncategories (such as “human,” “dog,” “car,” etc.) [18]. The data set consists of more\\nthan 80,000 images. The data set is created by compiling typical complicated scenes\\nof everyday items. There are at least five captions for each image. We are selecting\\none caption for each image. It is crucial for our experiment that the chosen caption\\nincludes the object class in the sentence.\\n28', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e3a3ad4e-66c9-4280-899f-f1a4d5a4685c', embedding=None, metadata={'page_label': '42', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\n3.4 Evaluation Metrics\\nIt is crucial to have access to automatic evaluation criteria that fairly compare per-\\nformances and measure improvement. Evaluating generated images is particularly\\ndifficult because there are so many characteristics that resemble a good image, such as\\nvisual realism and diversity [13]. A good text-to-image model, however, encompasses\\nmore than just producing realistic visuals. The semantic alignment of generated im-\\nages and text descriptions is a crucial additional consideration. The training data\\ndistribution should be accurately portrayed in the images created from text descrip-\\ntions. We are using the matrices , Inception Score and Fr´ echet Inception Distance.\\nBarratt et al. [1] demonstrated Inception score provides beneficial guidance when\\nevaluating and comparing models. Heusel et al. [17] introduced the FID score which\\nmore accurately represents how similar generated images are to real images. We will\\ndiscuss about another metric which we have used i.e., the semantic object accuracy\\n(SOA). Unlike the majority of the existing evaluation measures, this metric focuses\\non specific elements and components within an image and also takes the caption into\\naccount when rating a picture. Often explicitly or indirectly, image captions describe\\nthe objects that can be observed in an image, such as an “A person using a cell\\nphone” is the description of the photo should show a person and a cell phone to-\\ngether [18]. The evaluation metrices that we have used for our comparative analysis\\nand experimentation will be discussed.\\n3.4.1 Inception Score (IS)\\nAn algorithm for measuring the effectiveness of image generative models is called the\\nInception Score. This metric is shown to correlate well with human scoring in terms\\nof the realism of generated images. It uses an Inception v3 network that has already\\nbeen trained on ImageNet and figures out statistics about the network’s results when\\napplied to images that were made by a computer [12].\\nIS(G) =exp(Ex∼pgDKL(p(y|x)||p(y)) (4)\\n29', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f100eb3e-9b93-41e7-9582-1207f272c1d4', embedding=None, metadata={'page_label': '43', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\nThe images generated should contain clear objects, i.e., the images should be sharp\\nrather than blurry, or p(y|x) should have low value. In other words, the Inception\\nNetwork should be highly confident that there is a single object in the image [1]. The\\ngenerative algorithm should output a high diversity of images from all the different\\nclasses in ImageNet, or p(y) should have high value. The KL-divergence is statistical\\ndistance that quantifies the differences between two probability distributions. If a\\ngenerative model has both of the aforementioned characteristics, we would expect a\\nlarge KL-divergence between the distributions of p(y) and p(y|x), which would lead\\nto a large IS [1].\\n3.4.2 Frechet inception distance (FID)\\nFID is a statistic that measures the distance between the feature vectors of real and\\nfake images generated by the model. Heusel et al. [17] first presented it in 2017. A\\nhigher quality and closer resemblance to real images are indicated by a lower FID\\nscore for the generator. The foundation of FID is an image’s feature vector. If FID\\nis your performance metric, attempt to keep it as low as possible, but having a very\\nlow score can also mean that the images are very similar and cause a lack of diversity\\nin the data set we are trying to generate. Fr´ echet inception distance (FID) is a\\nmeasurement used to evaluate the quality of the generated images.\\nd2=||µ1–µ2||2+Tr(C1+C2–2∗sqrt(C1∗C2)) (5)\\nThe score is denoted by the symbol d2, indicating that it is a distance in square\\nunits. The terms µ1andµ2designate the feature-wise means of the actual and arti-\\nficial images, respectively. These vectors have 2,048 elements each, each representing\\nthe mean feature that was seen across the images [3]. The covariance matrices, C1\\nandC2, represent the real and produced feature vectors, respectively. The expression\\n||µ1−µ2||2denotes the total squared difference between the two mean vectors. The\\nlinear algebra operation Trstands for the sum of the elements in the square matrix’s\\nmajor diagonal [3]. An Inception v3 model that has already been trained is loaded\\n30', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='814029e3-79de-4a5e-93b2-756204f7856f', embedding=None, metadata={'page_label': '44', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\nbefore calculating the FID score. The output is taken as the activations from the\\nfinal pooling layer, a global spatial pooling layer, after the output layer of the model\\nhas been removed [3]. Each image is projected to have 2,048 activation characteris-\\ntics since this output layer has 2,048 activations. This is referred to as the image’s\\ncoding vector or feature vector. Then, in order to demonstrate how genuine images\\nare represented, a 2,048 feature vector is projected for a selection of real photos from\\nthe issue domain. Then, feature vectors for newly created photos can be computed.\\nTwo collections totaling 2,048 feature vectors for both real and created photos is the\\nresult [3].\\n3.4.3 Semantic Object Accuracy (SOA)\\nMost evaluation metrics do not take the image caption into account and do not\\nevaluate individual areas or objects within an image. To address this, Hinz et al.\\n[18] introduced a novel evaluation metric based on a pre-trained object detection\\nnetwork. This metric is called Semantic Object Accuracy (SOA), which measures\\ndirectly whether objects mentioned in the caption are recognizable in an image as\\nwell as whether the image contains them. Similar techniques have been employed in\\nseveral earlier publications to assess the quality of the generated images [18]. But\\nsince the contents of the caption aren’t taken into account, any detection with a high\\nlevel of confidence is still good, even if the object being detected doesn’t make sense\\nin the context of the caption [18]. Images are generated for each of the 80 objects\\nin the MS COCO data set by analyzing the captions in the validation set. Captions\\nare filtered for keywords that are related to the available labels for objects. It uses\\nall captions that imply the existence of each object in the data set to produce three\\nimages for each object [18]. The YOLOv3 network [33], pre-trained on the MS COCO\\ndata set, is then applied to each of the generated photos to see if it can identify the\\nspecified object [18]. They presented the recall as a class average (SOA-C) (equation\\n6), which represents the average number of images per class in which the YOLOv3\\ndetects the given object (equation 7) and image average (SOA-I), which represents\\nthe average number of images in which the required object was detected [18].\\n31', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5184c3da-b9f6-4837-994d-fb2d0a049377', embedding=None, metadata={'page_label': '45', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. METHODOLOGY\\nSOA−C =1\\n|C|X\\nc∈C1\\n|Ic|X\\nie∈IeYOLOv3 ( ic) (6)\\nSOA−I =1P\\nc∈C|Ic|X\\nc∈CX\\nic∈IcYOLOv3 ( ic) (7)\\nic∈Icare images that are supposed to contain an object of class c.\\nYOLOv3( ic):\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f31 if YOLOv3 detected an object from class c\\n0 otherwise\\nIn order to determine whether generated photos contain objects that are specifi-\\ncally referenced in the image caption, we employ a pre-trained object detector. An\\nobject detector for the provided data set is all that is required for this. There are\\nmany of good pre-trained object detectors accessible for the MS COCO data set. To\\ncalculate the semantic object accuracy, we obtain all captions that expressly refer to\\neach of the 80 foreground objects that are given a label in the MS COCO data set\\n[18]. For each caption, we generate images using both the generative models. Use\\nthe object detector to determine whether the generated images has the object that\\nit ought to contain. This object detector is a pre-trained YOLOv3. Calculate the\\nfrequency with which a particular object was found in the images generated for each\\nlabel. The SOA fixes issues like considering the image caption, analyzes images using\\nan object detector that has already been trained on the same domain. Additionally,\\nit takes into account the foreground objects of the images and avoids overfitting the\\nmodel during training [18].\\n32', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce77b943-c769-45a9-80da-7bd7ddd5a902', embedding=None, metadata={'page_label': '46', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 4\\nExperiments and Results\\n4.1 Implementation and Tools\\nIn this section, we will go through the tools that we utilized to carry out the research\\nstudy. Here are the development tools, followed by the implementation environment.\\nDevelopment Tools:\\n•Platforms: Google Collaboratory- NVIDIA Tesla K80, Jupyter Notebooks, Ana-\\nconda\\n•Programming Language: Python 3.7\\n•Libraries: Sklearn, OpenCV, Scipy, Numpy, Pandas, Keras, pytorch\\nImplementation Environment:\\n•Operation system: 64-bit Windows 10\\n•System type: x64 based processor\\n•CPU: Intel Core i7-8565U CPU\\n•RAM: 16 GB\\n•GPU: NVIDIA GeForce RTX 2080\\n33', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='38d89d57-e0a6-438c-a786-944d5e32ec92', embedding=None, metadata={'page_label': '47', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\n4.2 Results and Discussion\\n4.2.1 Inception Score Results\\nWe run experiments using the samples created from the two models, which are the\\nVQGAN - CLIP and the GLIDE. For our analysis, 90 images are generated for each\\nclass from the MS COCO data set. We are creating three images from each caption.\\nWe are using a total of 30 captions for each category. We also combine all the images\\nand then compute the overall Inception score.\\nFig. 4.2.1: Inception Scores for VQGAN - CLIP and GLIDE\\nAlthough there has been significant development in recent years in producing im-\\nages from captions, the process of creating images of complicated scenes with several\\nitems that may interact with one another is still highly challenging. We utilize the\\nmodel’s output, to calculate the IS. The overall entropy over the classification outputs\\nfor all images should be large, indicating that the generated images contain objects\\nof different classes, while the values for each of the generated images should be small,\\nindicating that the network is confident that there is one specific object in the image.\\nAccording to our results, the classes “bird”, “book”, “person” and “racket” have the\\nhighest inception score for the GLIDE model relative to other class objects. While\\nthe object that the images of particular class is supposed to contain is not visible\\nin the sample images generated, the score seems good for the data set generated for\\nthe class “person” because the inception model can detect other objects in the im-\\n34', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5fb48b43-1a4a-4101-b3d3-775f2b63a872', embedding=None, metadata={'page_label': '48', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nages. For the class “book,” the models are generating images that depict the objects\\nstated in the text, align with the caption, and offer variety to the data set. The\\nVQGAN - CLIP model scored lower than GLIDE because the items were easier to\\nrecognize and identify using the inception model. For the “bird” and “racket” classes,\\nthe model generates more distinct objects, and the generated image focuses on one\\nobject, which leads to a higher IS for GLIDE. Because the images created are blurry,\\nVQGAN - CLIP receives relatively lower scores for the same classes. The images pro-\\nduced by GLIDE are unquestionably more visually appealing. For VQGAN - CLIP,\\n“handbags”, “potted plants”, “elephant”, and “bananas” are the highest. The cap-\\ntion type also plays a significant role in the image generation process. For example,\\na “bird” with “ground” as the background is relatively easier to create than a “cat”\\nin a “handbag”.\\nOriginal Image VQGAN - CLIP GLIDE\\nFig. 4.2.2: Set of books sitting next to a small black clock.\\nFig. 4.2.3: A bird sitting on the ground.\\n35', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8c39aac9-7f29-415e-a0f0-60f793a4c62c', embedding=None, metadata={'page_label': '49', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nFig. 4.2.4: A cat sleeping in a red handbag.\\nFig. 4.2.5: A dog is sitting between two large potted plants.\\nThe lowest scores for VQGAN - CLIP are, for classes, “keyboard”, “skateboard”,\\n“microwave”, and “baseballbat”. We observe that for all these objects the caption\\nwe are using has more than one uncommon objects. For example, for “mircowave”,\\nthe captions, “A microwave on top of a fridge in a kitchen.” or “Microwave on top of\\na small refrigerator with shelves above”. The model tries to generate one object at\\na time, resulting in generation of distorted images. These distorted images will have\\nlow inception scores as the models could not generate a coherent representation from\\nthese features and instead distributed them throughout the image. We can only see\\nthe texture and pattern generated for most of them, and not distinct objects. The\\nlack of image text pairs while training of models with both objects together can be\\none of the reason that the model is unable to generate the image.\\nFor the GLIDE model, lowest-scoring object classes are “elephant”, “zebra”, “stop\\nsign”, and “toilet”. Although most of images generated contain objects the score is\\n36', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7ecb5912-c555-4135-ab06-43f7eafc1346', embedding=None, metadata={'page_label': '50', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nrelatively lower than other classes. The score for these classes is also low because\\nmost of them are single object images and the object detection model used by IS only\\ndetects one object and scores accordingly.\\nModel Name IS Score\\nVQGAN - CLIP 12.86\\nGLIDE 19.26\\nTable 4.2.1: Overall IS Scores\\nThe above Table. 4.2.1 gives the overall IS Score for both models. The GLIDE\\nmodel has a higher overall IS for the images, which tells us it is more effective in\\nproducing images. The IS can be substantially greater for simpler data sets. Addi-\\ntionally, we see that the score rises as the size of the data set increases.\\n4.2.2 FID Results\\nThe FID represents the distance between two images. It is used to calculate the\\ndistance between the true image and the generated images. The FID still has the\\nissue that the image statistics are derived using a network that was previously trained\\non ImageNet [6], which is an extensive image database but may not be a representative\\ndata set, for more complicated data sets.\\nFig. 4.2.6: FID Scores for VQGAN - CLIP and GLIDE\\n37', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24006157-bbf5-4809-b4c7-fee5b8527b74', embedding=None, metadata={'page_label': '51', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nFurther discussion will be on the visuals for both high and low peaks in the graph\\npresented above. Beginning with the items with the highest distance between the\\noriginal image and the generated image for the VQGAN - CLIP model, the bottle\\nand teddy bear image generated by the VQGAN - CLIP model contains bottles but\\nwas unable to recreate the teddy bear as in the original image. The image distribution\\ncreated by generator will be random every time. The generator focuses on generation\\nof image using caption given to it and tries to recreate from what it has learned\\nduring training. The models are trained on various images other than the original\\nimage and that is why the generated image has high FID score. We can observe that\\nVQGAN - CLIP is recreating the donuts , but they look different than the original\\nimage for the same reason. Next is “sheep”, where the VQGAN - CLIP model can\\nonly generate the fur texture of the sheep , but the sheep is not visible in the picture.\\nThe reason being that model focused on the words “hairy sheep” from the caption\\nand the image distribution created only contains that portion of the caption. While\\nthe GLIDE was able to reconstruct the sheep but not with the same texture of fur as\\nthe original image. For “sandwich”, both the models the did not take into account\\nthe second part of the sentence, i.e., “a bowl of soup,” for the generation of the image.\\nFor the FID scores, we are generating single zero shot images for captions but if we\\ntry again the image distribution may contain the “bowl of soup” which is why for the\\nIS and SOA score we are using the same caption to generate 3 images and take the\\npossibility of model generating the other objects into consideration.\\nGround Truth VQGAN - CLIP Generated GLIDE Generated\\nFig. 4.2.7: A brown teddy bear sitting next to bottles of person care items.\\n38', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a4f1d907-ca35-4428-95fe-174ad5dcaa70', embedding=None, metadata={'page_label': '52', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nFig. 4.2.8: A variety of donuts and pastries in a box.\\nFig. 4.2.9: A large hairy sheep standing on a lush green field.\\nFig. 4.2.10: A table topped with a couple of sandwiches and a bowl of soup.\\nEven though the images created by GLIDE appear better, they are very different\\nfrom the original image, giving us variety in the data set. The below-given samples\\nare low FID scores for the VQGAN model. We notice that the “monitor” and “hair\\ndryer” in the samples are created. The GLIDE model can create a similar background\\n39', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4ac159f-de9d-4739-8f12-1cc8fbd0d221', embedding=None, metadata={'page_label': '53', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nfor “trucks” and “airplanes” but not the object mentioned. As we learned earlier\\nabout the training process of generative models they try to create the objects first\\nand then work on conditioning, here the model focuses on creation of background as\\nit is mentioned in the caption and then tries to add the other objects mentioned. The\\nmodel may implicitly place more weight on certain parts of the text description if they\\nare more important for generating the desired image. It appears that VQGAN - CLIP\\nonly creates a screen-like structure for the “monitor” class and nothing else. On the\\nother hand, GLIDE recreates a proper desk and monitor and also creates additional\\nobjects which belong to the desk as the model has learned that from the training data\\nset for either monitor or keyboard or desk and if all of them are together. Both models\\ncannot create the “hair dryer” and “dog” mentioned in the caption. The models are\\ntrying to recreate the objects which is tricking the CLIP to believe similarity to\\ncaption but the image is not visually correct. The image generated for the “truck”\\nby VQGAN - CLIP also does not contain the object mentioned but is similar to the\\noverall image, while the GLIDE created a normal road rather than a steep dirt road.\\nGround Truth VQGAN - CLIP Generated GLIDE Generated\\nFig. 4.2.11: A desk with a computer monitor and a keyboard.\\n40', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70430095-1e0d-471a-bbbc-b06bb5fb2d8c', embedding=None, metadata={'page_label': '54', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nFig. 4.2.12: Trucks driving down a steep narrow dirt road.\\nFig. 4.2.13: A big airplane flying in the big blue sky.\\nFig. 4.2.14: Two people are using a hair drier on a small dog.\\nThe sample images in Figures 4.2.15 , 4.2.16, 4.2.17 have low FID scores for\\nVQGAN - CLIP, and the bus is recreated very similarly to the original image. The\\nother class “bowl” is easy to recreate as it contains only one object, and the ‘spoon’\\nmentioned in the caption is not visible in either of the images created.\\n41', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b32a8203-105c-4cdc-9434-e7ada0c3d1ca', embedding=None, metadata={'page_label': '55', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nGround Truth VQGAN - CLIP Generated GLIDE Generated\\nFig. 4.2.15: A yellow school bus parked in a parking lot.\\nFig. 4.2.16: Some suitcases and a hat laying on a carpeted floor.\\nFig. 4.2.17: A bowl of veggie soup with a spoon in it.\\nThese below given samples of bed (Fig.4.2.18) and apple (Fig. 4.2.19) are highly\\nsimilar and hence have low scores. It is also because the images contain single ob-\\njects that are easier to recreate and the objects are not interacting. For “bench”\\n42', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f846a965-6e2d-4b22-a023-8be22d1e82a7', embedding=None, metadata={'page_label': '56', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\n(Fig.4.2.20) the scores are relatively lower than other objects. Overall, the GLIDE\\nmodel outperforms the VQGAN - CLIP model at scene recreation.\\nThere will be differences between the original image and the generated images,\\neven if the model is performing well. This is because the generated images are typi-\\ncally based on a noise image and a text description, rather than being an exact copy\\nof the original image. As such, the generated images may contain some variations or\\ndeviations from the original image. Comparing the original image to the generated\\nimages can provide valuable insights into the performance of the image generation\\nmodel, but it is important to consider the limitations of the generated images as well.\\nGround Truth VQGAN - CLIP Generated GLIDE Generated\\nFig. 4.2.18: A room with a bed that has white and red pillows.\\nFig. 4.2.19: A apple with a bite in it on a table.\\n43', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e8daa16d-9d03-4ab2-8ce6-a760e5cdf2a7', embedding=None, metadata={'page_label': '57', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nFig. 4.2.20: Two park benches that are overlooking a valley.\\n4.2.3 SOA Scores\\nFig. 4.2.21: SOA Scores for VQGAN - CLIP and GLIDE\\nWe can observe that, for only a few classes, the models are able to recreate the\\nobjects mentioned in the captions. Most of the sentences used for the generation\\ncontain more than one object, and the objects might sometimes be interacting with\\neach other, which makes it even more challenging for the model. The VQGAN -\\nCLIP is able to generate items like “pizza”, “hotdog”, “cake”, and “banana”, which\\nare common objects. If we had a caption in which the objects interact with other\\nobjects the models may not be able to create images this well as observed for other\\nclasses. For GLIDE, “pizza” , “cake” , “stop sign” and “horse” are on top. The\\nimages in GLIDE also provide more variety than the VQGAN - CLIP model. The\\n44', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d0a2c174-bc97-4339-9042-221131efc7b4', embedding=None, metadata={'page_label': '58', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nimages generated by the VQGAN model appear very similar the images generated by\\nGLIDE appear different for the three cases.\\nVQGAN - CLIP Generated GLIDE Generated\\nFig. 4.2.22: Generated Images: Pizza\\nFig. 4.2.23: Generated Images: Cake\\nFig. 4.2.24: Generated Images: Bananas\\n45', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc447ac5-f223-4024-8e33-c6c5124b2434', embedding=None, metadata={'page_label': '59', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nFig. 4.2.25: Generated Images: Stopsign\\nModel Name SOA-C ↑SOA-I ↑\\nGLIDE 25.83 32.29\\nVQGAN - CLIP 23.73 29.67\\nTable 4.2.2: SOA Scores\\nBoth models perform better on common items but have trouble producing un-\\ncommon ones. There is a difference between the SOA-I and SOA-C values. Since the\\nSOA-I is based on the average of images, it is skewed by things that are frequently\\nseen in captions and images, which is why the SOA-I values are higher [18]. GLIDE\\nperforms better for both cases proving that there is higher the occurrence of the ob-\\nject in the total images generated. Nevertheless, it is more challenging to construct\\ncomplicated statements, which are statements with multiple objects in the sentence,\\nusing common things than simple phrases with unusual objects.\\n4.3 Analysis\\nThere are several reasons why a generative model might not be able to generate a\\ncertain object:\\n•Insufficient training data: Generative models require a large amount of training\\ndata in order to learn to generate realistic images. If the training data does\\n46', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d2ba7c1c-fd1e-401a-8615-b6defc5705f3', embedding=None, metadata={'page_label': '60', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\nnot contain examples of the object that the model is being asked to generate,\\nit may not be able to generate that object.\\n•Unbalanced training data: If the training data is unbalanced (e.g., it contains\\na disproportionate number of examples of certain types of objects), the model\\nmay be biased towards generating those types of objects and may have difficulty\\ngenerating other types of objects.\\n•Limited model capacity: Generative models have a limited capacity and may\\nnot be able to generate all possible objects if they are too complex or varied.\\nFor example, a model trained on a dataset of small, simple images may not be\\nable to generate large, detailed images.\\n•Overfitting: If the model overfits to the training data (i.e., it performs well\\non the training data but poorly on new, unseen data), it may not be able to\\ngenerate objects that are not present in the training data.\\nFor both models, we can deduce that there is lack of training data when the object\\nto be generated is small in size for example “ball” or “baseball bat”. These items are\\noften referred as a small part of a scenery in background. It is tough for the model to\\nunderstand and generate with respect to the scenery. More emphasis is required on\\ntraining models with images which have more than two objects and a background. It\\nis also important that correct captions are given while training otherwise the model\\nmight learn to associate a cat as some other object. The model performs good if\\nthe focus is on one or two objects like “apple” or “pizza” which take up most of the\\nspace in the image generated. We can observe that for most of objects with high SOA\\nscores the models are overfitted. The models will only generate the object for which\\nit has most training examples, ignoring other parts of the sentence. For example, if\\nthe caption is “A person eating pizza and salad at a table.”. The model will recognize\\n“pizza” immediately and generate an image just containing a pizza (Fig. 4.3.2), which\\ngives us a high SOA but is not semantically accurate image.\\nBoth the models focus on the caption used for the generation of the image and\\nuse it to guide the image generation process. It is important that the caption used\\n47', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68f8ac2e-1bc1-46a6-917c-2837e769b547', embedding=None, metadata={'page_label': '61', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. EXPERIMENTS AND RESULTS\\ncontains enough details about the image to be generated. The sentence should have\\ncorrect spellings of objects for the model to understand as it was trained with correct\\nexamples. A very complex sentence with more than two objects makes the models\\ngenerate images and merge them without focusing on interaction between the objects.\\nThe resulting image is distorted which will give a low IS, large FID Score and low\\nSOA Score (Fig. 4.3.1). If the model is trained with enough examples for complex\\nsentences the models may be able to generate better samples.\\nFig. 4.3.1: A boy swinging a baseball\\nbat at a ball.\\nFig. 4.3.2: Generated Image: A person\\neating pizza and salad at a table.\\n48', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8c3aebc3-c871-4c41-9778-6a6ff0e600e7', embedding=None, metadata={'page_label': '62', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 5\\nConclusion and Future Work\\nAlthough there has been significant progress, more can still be done to improve auto-\\nmated analytics and standardize research. There is still more work to be done before\\nthe models can produce images of higher quality that more closely resemble the se-\\nmantics of the input text. While the models are pretty good at recreating art work\\nand imaginative statements it’s still a challenge to be able to recreate multiple objects\\nin a picture from text to image. Even for data sets like CUB or Flowers data set it\\nis relatively easier to recreate as, for most part, they only contain single objects and\\nmost of the image created is occupied by them. We can also observe that the objects\\nwhich have a clear pattern are also easier to detect by the object detector used and\\ncan take the texture for object making SOA unreliable for some instances. Although,\\nboth models are designed to generate high-quality images from text descriptions, and\\nwhich one is better depends on the specific task. For our task of generating images\\nfrom the MS COCO data set the GLIDE model performs better than the VQGAN\\n- CLIP. As a result of our experiments, we were able to determine that the diffusion\\nmodel GLIDE performed much better for IS and for SOA. While VQGAN - CLIP\\ndid perform slightly better for FID, the samples generated by the GLIDE model are\\nsemantically accurate and have good IS and SOA scores. The peaks and lows in\\nthe results for FID (Fig. 4.2.6) tells us which class objects have more common pixel\\nrepresentation with original data set. This information can be used to determine for\\nwhich classes the models need more training and attention. The samples generated\\nby GLIDE are very different from the original image giving a high FID score. Given\\nall of this evidence, we can claim that the GLIDE model performs better than the\\n49', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b67c324a-ae8d-432a-a440-ac404293189b', embedding=None, metadata={'page_label': '63', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5. CONCLUSION AND FUTURE WORK\\nVQGAN - CLIP model. The images generated by the GLIDE model offers variety by\\ngenerating unique each time for the same text input. Although there is not a measure\\nto consider how much variety a generative model can offer, we can observe that the\\nimages generated by the GLIDE offers more variety when generating a new data set.\\nBoth the models are over fitted and biased towards some object classes due to dis-\\nproportionate number of images in the training data as. For instance, the generated\\nimages for pizza the models ignore the other items in the caption and instead focus\\non generating just pizza pattern in a different way. There is also currently a lack of\\nclarity on the extent to which generative models memorize the training data.\\n5.1 Future Work\\nFor future work, the researchers can generate images utilizing new approaches such\\nas DALL-E 2 and Imagen for comparison. In order to obtain an even more accurate\\nassessment, the size of the image data set for each class can be increased. The\\nevaluation of the models can benefit from the introduction of new matrices that\\nquantify qualities like variation in generated images. More research can be done to\\nunderstand how the model memorizes training data. There can be more variation\\nin the data set used. The captions can be taken directly from the people instead of\\nusing pre-defined captions. Generative model assessment is critical, and we need this\\ninformation in order to comprehend the direction in which more work is required.\\n50', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='509c3010-60fa-440a-9d3d-a68be5f77a76', embedding=None, metadata={'page_label': '64', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='REFERENCES\\n[1] Barratt, S. and Sharma, R. (2018). A note on the inception score. arXiv preprint\\narXiv:1801.01973 .\\n[2] Borji, A. (2019). Pros and cons of gan evaluation measures. Computer Vision\\nand Image Understanding , 179:41–65.\\n[3] Brownlee, J. (2019). How to implement the frechet inception distance (fid) for eval-\\nuating gans. https://machinelearningmastery .com/how-to-implement-the-\\nfrechet-inception-distance-fid-from-scratch/ . Last accessed 19 December\\n2022.\\n[4] Chow, A. (2021). Nfts are shaking up the art world—but they could change so\\nmuch more. time.com/5947720/nft-art/ .Last accessed 21 December 2022.\\n[5] Crowson, K., Biderman, S., Kornis, D., Stander, D., Hallahan, E., Castricato, L.,\\nand Raff, E. (2022). Vqgan-clip: Open domain image generation and editing with\\nnatural language guidance. In European Conference on Computer Vision , pages\\n88–105. Springer.\\n[6] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet:\\nA large-scale hierarchical image database. In 2009 IEEE Conference on Computer\\nVision and Pattern Recognition , pages 248–255.\\n[7] Deng, L. (2012). The mnist database of handwritten digit images for machine\\nlearning research. IEEE Signal Processing Magazine , 29(6):141–142.\\n51', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eaece228-9bf2-4c75-b118-f8be2f4a6e9e', embedding=None, metadata={'page_label': '65', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='REFERENCES\\n[8] Denton, E. L., Chintala, S., Fergus, R., et al. (2015). Deep generative image\\nmodels using a laplacian pyramid of adversarial networks. Advances in neural\\ninformation processing systems , 28.\\n[9] Dhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis.\\nAdvances in Neural Information Processing Systems , 34:8780–8794.\\n[10] Esser, P., Rombach, R., and Ommer, B. (2021). Taming transformers for high-\\nresolution image synthesis. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 12873–12883.\\n[11] Fong, J. (2022). The text-to-image revolution. https://www .vox.com/23150422/\\ntext-to-image-ai-deep-learning . Last accessed 29 December 2022.\\n[12] Foster, D. (2019). Generative deep learning: teaching machines to paint, write,\\ncompose, and play . O’Reilly Media.\\n[13] Frolov, S., Hinz, T., Raue, F., Hees, J., and Dengel, A. (2021). Adversarial\\ntext-to-image synthesis: A review. Neural Networks , 144:187–209.\\n[14] Galatolo, F. A., Cimino, M. G., and Vaglini, G. (2021). Generating images from\\ncaption and vice versa via clip-guided generative latent space search. arXiv preprint\\narXiv:2102.01645 .\\n[15] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\\nS., Courville, A., and Bengio, Y. (2020). Generative adversarial networks. Com-\\nmunications of the ACM , 63(11):139–144.\\n[16] Gregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. (2015).\\nDraw: A recurrent neural network for image generation. In International conference\\non machine learning , pages 1462–1471. PMLR.\\n[17] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Klambauer, G., and\\nHochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a\\nnash equilibrium. arXiv preprint arXiv:1706.08500 , 12(1).\\n52', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d2c5eec4-b757-4907-bda8-c695eea620ef', embedding=None, metadata={'page_label': '66', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='REFERENCES\\n[18] Hinz, T., Heinrich, S., and Wermter, S. (2020). Semantic object accuracy for\\ngenerative text-to-image synthesis. IEEE transactions on pattern analysis and\\nmachine intelligence .\\n[19] Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models.\\nAdvances in Neural Information Processing Systems , 33:6840–6851.\\n[20] Karagiannakos, S. and Adaloglou, N. (2022). How diffusion models work: the\\nmath from scratch. Howdiffusionmodelswork:Themathfromscratch ,AISummer/ .\\nLast accessed 28 December 2022.\\n[21] Kingma, D. and Welling, M. (2013). Auto-encoding variational bayes. ICLR ,\\npages 1312–6114.\\n[22] Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar,\\nP., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In\\nEuropean conference on computer vision , pages 740–755. Springer.\\n[23] Liu, Z., Luo, P., Wang, X., and Tang, X. (2014). Deep learning face attributes\\nin the wild. pages 3730–3738.\\n[24] Mansimov, E., Parisotto, E., Ba, J., and Salakhutdinov, R. (2015). Generating\\nimages from captions with attention.\\n[25] Miranda, L. J. (2021). The illustrated vqgan. https://\\nljvmiranda921 .github .io/notebook/2021/08/08/clip-vqgan/ . Last accessed\\n30 December 2022.\\n[26] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\\nSutskever, I., and Chen, M. (2021). Glide: Towards photorealistic image generation\\nand editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 .\\n[27] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilis-\\ntic models. In International Conference on Machine Learning , pages 8162–8171.\\nPMLR.\\n53', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f9ed07e-c60c-4480-b519-3774464301e3', embedding=None, metadata={'page_label': '67', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='REFERENCES\\n[28] Nilsback, M.-E. and Zisserman, A. (2008). Automated flower classification over\\na large number of classes. In 2008 Sixth Indian Conference on Computer Vision,\\nGraphics & Image Processing , pages 722–729. IEEE.\\n[29] Oleszak, M. (2022). Autoencoders: From vanilla to variational. https:\\n//towardsdatascience .com/autoencoders-from-vanilla-to-variational-\\n6f5bb5537e4a . Last accessed 31 December 2022.\\n[30] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\\nG., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual\\nmodels from natural language supervision. In International Conference on Machine\\nLearning , pages 8748–8763. PMLR.\\n[31] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hi-\\nerarchical text-conditional image generation with clip latents. arXiv preprint\\narXiv:2204.06125 .\\n[32] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,\\nand Sutskever, I. (2021). Zero-shot text-to-image generation. In International\\nConference on Machine Learning , pages 8821–8831. PMLR.\\n[33] Redmon, J. and Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv\\npreprint arXiv:1804.02767 .\\n[34] Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., and Lee, H. (2016).\\nGenerative adversarial text to image synthesis. In International conference on\\nmachine learning , pages 1060–1069. PMLR.\\n[35] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backprop-\\nagation and approximate inference in deep generative models. In International\\nconference on machine learning , pages 1278–1286. PMLR.\\n[36] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,\\nS. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. (2022). Photorealistic\\n54', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='457b4753-d698-4c15-909f-98066a9037ce', embedding=None, metadata={'page_label': '68', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='REFERENCES\\ntext-to-image diffusion models with deep language understanding. arXiv preprint\\narXiv:2205.11487 .\\n[37] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen,\\nX. (2016). Improved techniques for training gans. Advances in neural information\\nprocessing systems , 29.\\n[38] Skelton, J. (2022). Generating and editing photorealistic images from text-\\nprompts using openai’s glide. https://blog .paperspace .com/glide-image-\\ngeneration/#architecture . Last accessed 31 December 2022.\\n[39] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015).\\nDeep unsupervised learning using nonequilibrium thermodynamics. In Interna-\\ntional Conference on Machine Learning , pages 2256–2265. PMLR.\\n[40] Song, J., Meng, C., and Ermon, S. (2020). Denoising diffusion implicit models.\\narXiv preprint arXiv:2010.02502 .\\n[41] Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. J. (2011). The\\ncaltech-ucsd birds-200-2011 dataset.\\n[42] Weng, L. (2021). What are diffusion models? https://lilianweng .github .io/\\nposts/2021-07-11-diffusion-models . Last accessed 29 December 2022.\\n[43] Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., and He, X.\\n(2018). Attngan: Fine-grained text to image generation with attentional generative\\nadversarial networks. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition , pages 1316–1324.\\n[44] Yu, Y., Zhang, W., and Deng, Y. (2021). Frechet inception distance (fid) for\\nevaluating gans.\\n[45] Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., and Metaxas, D. N.\\n(2017). Stackgan: Text to photo-realistic image synthesis with stacked genera-\\ntive adversarial networks. In Proceedings of the IEEE international conference on\\ncomputer vision , pages 5907–5915.\\n55', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7405d307-57fa-49e5-839c-e3e5ac2b482e', embedding=None, metadata={'page_label': '69', 'file_name': 'Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_path': '/home/akanksha/Desktop/LlamaIndex/LlamaIndex_LLM/data/Comparative Study of Generative Models for Text-to-Image Generati.pdf', 'file_type': 'application/pdf', 'file_size': 6056729, 'creation_date': '2024-06-28', 'last_modified_date': '2024-06-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='VITA AUCTORIS\\nNAME: Nazia Siddiqui\\nPLACE OF BIRTH: Pratapgarh, India\\nYEAR OF BIRTH: 1998\\nEDUCATION: Andhra Education Society School, New Delhi, India\\nHigh School, CBSE, 2013-2015\\nJamia Hamdard, New Delhi, India,\\nBachelor of Technology, Computer Science, 2015-2019\\nUniversity of Windsor,Windsor ON, Canada\\nM.Sc in Computer Science, 2021-2022\\n56', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 269/269 [00:00<00:00, 444.56it/s]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index\u001b[38;5;241m=\u001b[39m\u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     transformations,\n\u001b[1;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:308\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    301\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    302\u001b[0m ):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:280\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:233\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m--> 233\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:141\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[1;32m    133\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/utils.py:138\u001b[0m, in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m--> 138\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[1;32m    143\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:332\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    324\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    325\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    329\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[1;32m    330\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 332\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    334\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    335\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    336\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[1;32m    337\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[1;32m    338\u001b[0m         },\n\u001b[1;32m    339\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:429\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03mBy default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03mCan be overridden for batch queries.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/tenacity/__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/tenacity/__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:180\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[0;32m--> 180\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1250\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1238\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1247\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1248\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1249\u001b[0m     )\n\u001b[0;32m-> 1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1014\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1014\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: SyncAPIClient._request at line 1015 (7 times), SyncAPIClient._retry_request at line 1063 (7 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1014\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llamaindex/lib/python3.11/site-packages/openai/_base_client.py:1030\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1029\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1033\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1034\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1038\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "index=VectorStoreIndex.from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
